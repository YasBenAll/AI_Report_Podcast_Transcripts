Video title: Dit is de populairste AI bij jongeren ｜ ✨ Poki
Youtube video code: JX1JTVEC_Dw
Last modified time: 2024-07-12 07:12:07

------------------ 

[0.00 --> 2.32]  Interessant hè, deze podcastserie?
[2.70 --> 4.00]  Weet je wat ook interessant is?
[4.42 --> 7.42]  Data Expo, het nummer 1 data-event van het jaar.
[7.78 --> 9.96]  11 en 12 september, Jaarburg Utrecht.
[10.42 --> 13.42]  Met ruim 120 sprekers en 100 exposanten...
[13.42 --> 18.58]  ontdek je alle mogelijkheden van AI, analytics en cloudoplossingen voor jouw organisatie.
[19.00 --> 22.78]  Je maakt er nieuwe contacten, vindt informatie en ontdekt technologie...
[22.78 --> 24.62]  die nodig is om succesvol te zijn.
[25.26 --> 28.90]  Klaim jouw gratis ticket op data-expo.nl
[28.90 --> 33.10]  Even een kort bericht, want jouw tijd is je meest waardevolle bezit.
[33.76 --> 37.44]  Teamleader Focus helpt jou als ondernemer om het meeste uit je tijd te halen.
[37.92 --> 42.50]  Het brengt CRM, facturen, offertes en projectmanagement samen op één plek.
[43.06 --> 45.34]  Zodat jij meer tijd hebt om te doen waar je goed in bent.
[45.86 --> 47.54]  Tijd om te ondernemen. Teamleader.
[47.98 --> 49.88]  Kijk op focusopjebedrijf.nl
[52.34 --> 56.60]  Welkom bij POKI, dé Nederlandse podcast over kunstmatige intelligentie...
[56.60 --> 59.20]  waar we uitzoeken hoe AI ons bestaan beïnvloedt.
[59.20 --> 60.96]  Op de werkvloer en verder buiten ook.
[61.46 --> 65.56]  Maar ook in hoeverre het ons leven makkelijker en of moeilijker kan maken.
[66.18 --> 68.22]  Tegenover mij, Wietse Hagen en Alexander Klubbing.
[68.64 --> 69.54]  Ik ben Milou Brandt.
[70.00 --> 71.60]  En in deze aflevering hebben we het over...
[71.60 --> 75.02]  Ja, ben je alleen en heb je geen zin in echte mensen?
[75.02 --> 79.12]  Nou, dan wordt het steeds makkelijker en normaler om te kletsen met AI.
[79.30 --> 81.06]  En je kan zelf ook een eigen karakter bouwen.
[81.80 --> 83.48]  Dat is namelijk met Character AI.
[83.64 --> 88.02]  Dat is een app waar je allerlei karakters die jammer wat graag te woord willen staan...
[88.02 --> 89.98]  kan aanspreken en kan maken.
[90.64 --> 92.44]  Alexander, jij hebt je erin verdiept. Ben je hoekt?
[92.62 --> 93.62]  Ik heb het geprobeerd.
[93.84 --> 95.90]  Ik dacht dat ik teleurgesteld zou gaan zijn.
[96.36 --> 99.88]  Maar ik heb echt met heel veel plezier op de bank gezeten...
[99.88 --> 101.42]  om met mijn virtuele vrienden te praten.
[101.42 --> 103.32]  Het spijt me bijna dit te zeggen.
[103.54 --> 105.10]  Maar ik kan niet wachten om dit met jullie te delen.
[105.18 --> 106.98]  Oké, nou wij ook niet. Dat en meer straks.
[107.08 --> 109.14]  Maar we beginnen natuurlijk met het nieuws van deze week.
[109.58 --> 110.30]  Dit is Poki.
[121.56 --> 123.50]  Ja, Wietse, jij bent er natuurlijk ook gewoon, hè?
[124.36 --> 127.20]  Ik zit hier gewoon netjes op mijn beurt te wachten.
[128.10 --> 130.38]  Ik heb de feedback gekregen dat ik teveel praat.
[131.42 --> 132.26]  Niet waar, toch?
[133.20 --> 133.80]  Nee, nee, nee.
[135.72 --> 138.02]  Ja, schaap je keel maar vast.
[138.74 --> 139.18]  Maar hoe gaat het?
[139.18 --> 139.88]  Ja, ik ben er klaar voor.
[139.96 --> 140.86]  Ja, jij bent er klaar voor.
[140.90 --> 142.24]  Ja, nee, het gaat goed. Het gaat goed.
[142.56 --> 146.62]  Het was een beetje een rustig weekje qua nieuws, vond ik.
[146.70 --> 148.34]  Maar ik denk dat er toch wel weer heel veel...
[148.34 --> 149.50]  Er was toch wel weer heel veel.
[149.58 --> 150.78]  Een beetje dubbelzinnig wat ik nu zeg.
[151.04 --> 152.60]  Ik dacht dat er niet zoveel gebeurd was.
[153.02 --> 155.10]  En toen ging ik er indijken en toen was er toch wel weer wat gebeurd.
[155.16 --> 156.22]  Dat is wel grappig.
[156.34 --> 158.00]  Was jij andere dingen aan het doen overgelopen week?
[158.90 --> 159.88]  Nou, het is wel druk, ja.
[160.02 --> 160.98]  Ik ben al wat veel bezig.
[160.98 --> 163.44]  Ja, daar ligt het dan denk ik aan.
[163.94 --> 166.52]  Nou, ik heb inderdaad best wel veel nieuws tegengekomen.
[166.66 --> 168.60]  Het was weer een pittige selectie die ik heb moeten maken.
[169.24 --> 174.24]  Wat er bijvoorbeeld was deze week, heel veel nieuws op het gebied van gezondheid en AI.
[174.70 --> 177.20]  Er is natuurlijk heel veel toepassingen voor kunstmatige intelligentie.
[177.30 --> 179.92]  Maar het zou ons ook gezonder moeten kunnen maken.
[180.10 --> 184.24]  En er waren deze week wel drie verschillende aankondigen, in ieder geval die ik heb gezien.
[184.24 --> 187.94]  Van een soort persoonlijke health coach.
[187.94 --> 193.46]  Dus iemand die jou persoonlijk advies kan gaan geven over welke gewoonten je zou moeten aanleren.
[193.46 --> 195.04]  Of je beter zou moeten slapen.
[195.10 --> 195.80]  Hoe je dat dan doet.
[195.92 --> 196.96]  Wat je eigenlijk moet eten.
[197.34 --> 198.32]  Hoe je kunt ontspannen.
[198.78 --> 201.26]  Echt persoonlijk advies op maat.
[201.96 --> 204.16]  Daar zijn drie verschillende initiatieven voor.
[204.34 --> 207.18]  Een soort concurrentiestrijd bijna al die er nu gaande is.
[207.22 --> 208.06]  Ik neem jullie even mee.
[208.06 --> 213.24]  Ten eerste is er het nieuwe project van OpenAI in samenwerking met Thrive Global.
[213.44 --> 217.44]  Dat is een Amerikaans bedrijf dat technologie voor gedragsverandering levert.
[217.54 --> 218.30]  In ziekenhuizen en zo.
[219.34 --> 221.52]  En zij zijn aan het bouwen aan een app.
[221.68 --> 222.82]  Dat een soort coach moet zijn.
[222.92 --> 225.88]  Die dus hele goede professionele gezondheidsadviezen geeft.
[226.08 --> 228.80]  En dan denk je meteen, o jee gaat hij dan medische adviezen geven.
[229.26 --> 230.28]  Maar nee, dat gaat hij niet doen.
[230.40 --> 234.54]  Het is meer lifestyle advies.
[234.54 --> 238.38]  Dus gericht op gedragsverandering, betere gewoonten.
[238.54 --> 239.90]  Niet elke dag naar de McDonald's.
[240.30 --> 240.46]  Ja, precies.
[240.54 --> 241.90]  Dus niet ik heb mijn been gebroken.
[242.12 --> 247.12]  Maar hoe leef ik gezonder door dingen aan te passen.
[247.22 --> 249.80]  Ja, en na welke tijd kan ik nog wel eten.
[249.94 --> 251.74]  En wanneer kan ik beter stoppen met eten.
[251.84 --> 252.78]  In de avond, dat soort dingen.
[253.20 --> 256.72]  Dus dat zijn eigenlijk gewoonten die heel moeilijk te veranderen zijn.
[256.90 --> 259.14]  En heel veel mensen hebben best wel veel slechte gewoontes.
[259.32 --> 261.60]  We hebben allemaal wel slechte gewoontes, denk ik.
[263.04 --> 264.52]  Je zit naar Wietz te kijken.
[264.54 --> 265.64]  Je gaat mooi naar McDonald's.
[266.00 --> 268.98]  Kijk, ik word heel erg indringend aangekeken.
[269.20 --> 271.28]  Ik was gewoon nieuwsgierig.
[271.38 --> 272.82]  Misschien wil jij iets delen?
[272.90 --> 273.74]  Een slechte gewoontes.
[274.54 --> 276.54]  Nou, ik ga het meteen helemaal meta-analyse.
[276.70 --> 277.64]  Zodat ik het niet over mezelf.
[277.94 --> 279.26]  Bij mijn slechte gewoontes.
[279.92 --> 281.48]  Vlugst maken de meta-analyse.
[281.60 --> 283.66]  Ja, daar komt de meta-analyse.
[284.10 --> 286.18]  Nou, ik zit te denken dat er is...
[286.18 --> 286.98]  Er zijn best wel wat...
[286.98 --> 289.94]  We zijn door wat golven van dingen heen gegaan.
[290.06 --> 291.64]  Big data, blockchain, whatever.
[291.82 --> 293.34]  Je hebt de hele grote hypewaves.
[293.48 --> 297.88]  Maar ook kleinere dingen die dan ineens iedereen mee bezig is binnen technologie.
[298.38 --> 299.90]  En dat is een tijd geleden geweest.
[299.98 --> 301.18]  De quantified self.
[301.32 --> 303.46]  En dat ging eigenlijk over het kwantificeren van jezelf.
[303.58 --> 305.46]  Dat je ging allemaal indicators hangen aan jou.
[305.56 --> 306.34]  Je slaaptijd.
[306.34 --> 307.82]  Hoeveel heb je gefietst?
[307.88 --> 308.94]  Hoeveel water heb je gedronken?
[309.02 --> 309.46]  Al die dingen.
[309.96 --> 312.08]  En daarmee wil ik niet zeggen...
[312.08 --> 312.72]  Gaan we weer?
[312.86 --> 313.74]  Dat is niet het cynisch punt.
[313.94 --> 319.36]  Ik vind het interessant om nu te zien dat er best wel wat dromen en bewegingen zijn geweest...
[319.36 --> 321.04]  Die toen best wel niche zijn gebleven.
[321.30 --> 325.10]  Die nu een soort tweede kans gaan krijgen omdat er weer AI tegen aangezet wordt.
[325.62 --> 328.80]  Dus sowieso het hele personal health ding...
[328.80 --> 330.38]  Dat heeft Apple al goed doorgekregen.
[330.48 --> 331.86]  Dat redt een beetje de Apple Watch.
[332.54 --> 334.60]  Want ja, er was niet echt een killer feature voor de Apple Watch.
[334.60 --> 336.46]  Dat is nu duidelijk een health device.
[336.66 --> 338.14]  Met daaromheen allerlei andere dingen.
[338.28 --> 339.34]  Zo zie ik dat in ieder geval.
[339.98 --> 345.42]  Maar ik denk dat de eerdere dromen van een gepersonaliseerd advies...
[345.42 --> 348.26]  Op basis van jouw gedrag, wie jij bent, et cetera.
[348.54 --> 349.94]  Een echte personal coach.
[350.30 --> 354.72]  Dat de beloftes in de slide decks van 15 jaar geleden van die health startups...
[354.72 --> 356.56]  Misschien nu een beetje meer waar kunnen gaan worden.
[356.80 --> 357.28]  Ja, nou ja.
[358.18 --> 362.18]  Kijk, niet iedereen kan natuurlijk een persoonlijke coach betalen van vlees en bloed.
[362.18 --> 365.08]  Dus op zich maakt het het wel net wat beschikbaarder.
[365.16 --> 366.92]  En als het echt op jou is afgestemd...
[366.92 --> 370.66]  Dan kan ik me voorstellen dat dat best wel iets in de brei kan brokkelen.
[371.46 --> 375.22]  En het is ook wel een wezenlijk probleem...
[375.22 --> 377.82]  Een probleem dat we steeds meer zorg nodig hebben.
[378.46 --> 379.72]  Want er zijn steeds meer mensen.
[379.92 --> 381.68]  Er is steeds meer zorgpersoneel nodig.
[382.42 --> 385.48]  Een op de zeven mensen moet in de zorg werken in 2040.
[385.62 --> 389.82]  Er gaat 100 miljard al per jaar naar de zorg toe in Nederland.
[389.82 --> 392.38]  Dus op zich zouden we best wel daar...
[392.38 --> 395.42]  Als je iets op preventie kan inzetten natuurlijk ter voorkoming van...
[395.42 --> 396.78]  Want daar gaat het uiteindelijk om.
[397.42 --> 401.26]  Chronische ziekten waar we allemaal in de samenleving steeds meer mee te maken hebben.
[401.34 --> 402.68]  Obesie, tals, hart- en vaatziekten.
[402.84 --> 403.56]  Al die nare dingen.
[403.88 --> 406.84]  Dat je dat voorkomt en dus ook minder zorg nodig hebt.
[407.16 --> 410.82]  Dan zou het dus een soort van miracle drug moeten worden volgens de...
[410.82 --> 414.00]  Sam Altman doet dit dus samen met Ariana Huffington.
[414.28 --> 415.70]  Zij is van Thrive Global.
[415.70 --> 418.04]  En van Huffington Post overigens.
[418.24 --> 419.48]  Dat is de naamgever ook ervan.
[420.00 --> 423.96]  En Thrive Global is dat medische bedrijf.
[424.64 --> 427.22]  En zij zegt dat het een soort miracle drug is.
[427.30 --> 428.06]  Gaan we even luisteren.
[428.06 --> 431.10]  Chronic diseases are skyrocketing.
[431.58 --> 436.24]  One stat that I can't get out of my mind is that last year alone...
[436.24 --> 439.72]  We had 150,000 leg amputations.
[440.38 --> 441.86]  People with diabetes.
[442.70 --> 446.90]  And the human costs, the healthcare costs...
[446.90 --> 449.10]  Are truly unsustainable.
[449.10 --> 454.36]  So that's why looking at behavior change as the miracle drug...
[454.36 --> 458.30]  Is something which we need to take much more seriously.
[458.64 --> 462.12]  Wat het idee is, kwaliteitsadvies eigenlijk voor Jan en Alleman.
[462.62 --> 463.06]  Beschikbaar.
[463.28 --> 466.10]  En ik zei al, er waren andere kapers op de kust.
[466.20 --> 469.74]  We hebben ook een bedrijf dat heet Oura of Ohura.
[470.00 --> 470.84]  Ja, Oura Ring.
[471.04 --> 471.36]  Oura.
[471.44 --> 472.10]  Noem het Amerikaan.
[472.20 --> 473.22]  Zo noemen zij het.
[473.84 --> 474.96]  Dan doen wij dat ook.
[474.96 --> 477.46]  Dat is dus een wearable.
[477.64 --> 479.94]  Een ring die je draagt, die meet van alles ook.
[480.02 --> 482.38]  Dus die neemt ook al je biometrische waarden mee.
[482.70 --> 488.02]  En die combineert dat dan weer met wetenschappelijk gebaseerde dingen...
[488.02 --> 489.52]  Die je zou kunnen doen om nog gezonder te leven.
[489.76 --> 491.48]  Die zegt dan, hoe heb je geslapen?
[491.60 --> 492.58]  Dan krijg je slaapscore.
[493.30 --> 496.28]  Of je hartslag hoger is dan normaal.
[496.40 --> 497.64]  Of dat je stress hebt.
[497.76 --> 500.52]  Het is zo'n ring die dan, als je tering gestrest bent...
[500.52 --> 502.22]  Dan krijg je een notificatie op je telefoon.
[502.30 --> 503.02]  Je bent gestrest.
[503.12 --> 503.64]  Nou, dankjewel.
[503.64 --> 503.84]  Ja.
[504.12 --> 507.94]  Maar het is wel vet hoeveel sensoren er zitten in een hardwarebedrijf van een start-up.
[508.04 --> 510.04]  Ja, maar het gaat ook wel verder dan alleen meten.
[510.14 --> 512.08]  Want het is echt een chatbot.
[512.52 --> 515.04]  Dus je gaat ook tegen jou zeggen, ik kan ermee in gesprek.
[515.16 --> 517.62]  Van oké, ik heb het hoge hartslag nu.
[517.94 --> 518.94]  Wat kan ik daar dan aan doen?
[519.12 --> 520.12]  Nou, je kan misschien...
[520.12 --> 521.96]  Dat is een beetje volgens mij hoe het moet werken.
[522.52 --> 526.10]  En Samsung heeft inmiddels ook de Galaxy Ring aangekondigd.
[526.34 --> 527.88]  Dus iedereen springt hier op.
[527.88 --> 529.28]  Ik vraag me wel af...
[529.28 --> 530.56]  Ja.
[531.66 --> 533.66]  Hoe gaan ze het precies aan de man brengen?
[533.74 --> 536.46]  Volgens mij is dat Open AI en Thrive concept.
[536.66 --> 539.28]  Dat is echt gericht op dat de CEO's van bedrijven...
[539.28 --> 541.58]  Dat moeten gaan verspreiden in hun bedrijven.
[541.82 --> 542.96]  Dus daar zit het echt op.
[543.04 --> 545.28]  Nou, als mijn werknemers gezond zijn...
[545.28 --> 546.78]  Dan zijn ze ook productiever.
[547.28 --> 548.12]  Dan is het beter.
[548.36 --> 548.76]  Creepy.
[548.76 --> 549.26]  Dan zijn ze meer.
[549.34 --> 550.06]  Ja, dat is ook wel creepy.
[550.64 --> 552.66]  Ik vraag me ook af of het echt werkt tegen burn-out.
[552.90 --> 553.12]  Ja.
[553.38 --> 554.24]  Nou, dat is...
[554.24 --> 554.54]  Bijvoorbeeld.
[554.84 --> 555.00]  Ja.
[555.68 --> 560.28]  Nou, de Galaxy Ring is natuurlijk op consumenten gericht in eerste instantie.
[560.42 --> 562.90]  Samsung heeft grote presentatie gehad deze week...
[562.90 --> 564.96]  Waar ze nieuwe Galaxy telefoons aankondigden.
[565.08 --> 566.06]  Dat is inmiddels traditie.
[566.38 --> 570.02]  Die zijn dan weer iets sneller en marginaal beter.
[570.02 --> 572.70]  Want dat is tegenwoordig hoe smartphone updates zijn.
[573.52 --> 577.18]  Daarnaast hebben ze een Apple Watch clone op de markt gebracht.
[577.48 --> 579.22]  De Galaxy Watch Ultra.
[579.22 --> 581.64]  Wat letterlijk een Apple clone is.
[581.72 --> 582.82]  Op een manier dat je denkt...
[582.82 --> 585.92]  Jezus, die Koreanen hebben echt geen reserves.
[586.44 --> 591.32]  En de Galaxy Ring is een letterlijke kopie van de Aura Ring.
[591.48 --> 593.62]  Maar met natuurlijk als potentiële voordeel...
[593.62 --> 596.60]  Dat al die shit geïntegreerd is zoals Apple dat doet.
[598.70 --> 602.60]  Het feit dat zij nu zo'n health wearable uitbrengen...
[602.60 --> 604.60]  Vind ik op zich best wel...
[605.44 --> 607.04]  Dat zegt wel iets.
[607.04 --> 610.64]  Dat zo'n groot elektronica bedrijf als Samsung...
[610.64 --> 612.78]  Iets maakt wat echt als enige functie heeft.
[612.82 --> 614.44]  Want bij een watch heb je nog meerdere functies.
[614.54 --> 615.86]  Daar kun je ook veel mee bellen.
[616.42 --> 619.18]  Maar dat als enige functie heeft health tracking doet.
[619.30 --> 624.04]  Dat is veelzeggend over de richting die dat bedrijf heeft.
[624.04 --> 624.44]  Ja.
[624.44 --> 626.92]  Ik ben ook benieuwd of het aan gaat slaan.
[627.24 --> 629.22]  Zou jij zo'n ring dragen wietse?
[631.62 --> 632.86]  Niet zo snel denk ik.
[633.94 --> 636.40]  Bij mij gaan natuurlijk allerlei alarmbellen af.
[636.52 --> 638.50]  Maar ik probeer er ook een soort pragmatisch naar te kijken.
[639.24 --> 640.40]  Dit gebeurt al.
[640.40 --> 644.10]  Dus Apple heeft hun hele health ecosysteem.
[644.64 --> 647.66]  Die hebben hun gezondheids app op iOS.
[647.80 --> 649.30]  Kan ook uit allerlei andere dingen voeden.
[649.46 --> 651.82]  Dus die ring kan data sturen naar die gezondheid.
[651.82 --> 655.90]  Dus een soort patiëntendossier van jezelf op jouw telefoon.
[656.12 --> 657.46]  En precies wat Alexander zegt.
[657.46 --> 660.56]  Ik ga er nu gewoon vanuit dat Samsung dat een op een heeft gekopieerd.
[661.42 --> 662.30]  En dat Google dit.
[662.44 --> 663.64]  Ik ben geen Android gebruiker.
[663.82 --> 665.42]  Waarschijnlijk volledig en nog beter.
[665.72 --> 666.56]  Google kennende.
[666.90 --> 671.06]  Beter als in beter met data kunnen analyseren.
[671.98 --> 672.92]  Ook aanbiedt.
[673.02 --> 677.46]  Dus het idee dat er een soort digital twin is.
[677.52 --> 681.48]  Van jouw gezondheidsprofiel binnen jouw smartphone.
[681.76 --> 682.12]  Ergens.
[682.60 --> 683.22]  Dat is er al.
[683.34 --> 684.58]  Alleen nu gaan we natuurlijk die stap maken.
[684.66 --> 685.62]  Wat jij net ook zei Milou.
[685.86 --> 686.92]  Maar ook echt een chatbot.
[686.92 --> 687.98]  Je kunt ermee praten.
[688.56 --> 692.18]  En ik denk dat dat sowieso een beetje de tijd.
[692.28 --> 694.50]  Ik had er vorige week met iemand een discussie over.
[694.68 --> 696.42]  We hadden toen over het Google Search dashboard.
[696.88 --> 700.24]  De Google Search dashboard kan je opzien wat Google over je weet.
[701.14 --> 702.52]  En daar laat Google een beetje zien.
[703.20 --> 704.92]  Dat ze denken dat je een man of een vrouw bent.
[705.08 --> 706.42]  En een beetje wat je leeftijd is en zo.
[706.50 --> 707.26]  En als je dat dan bekijkt.
[707.34 --> 708.08]  Dan schrik je al een beetje.
[708.18 --> 708.46]  Dat je denkt.
[708.58 --> 708.78]  Oeh.
[709.06 --> 709.78]  Weten ze dat allemaal?
[710.12 --> 711.24]  Google laat dan eigenlijk.
[711.60 --> 713.48]  Nog maar het puntje van de ijsberg zien.
[713.48 --> 714.74]  Wat ze in essentie over jou.
[715.12 --> 716.70]  Dankzij je mail en je zoekgedrag weten.
[716.92 --> 718.34]  Nou zo'n gezondheidsprofiel.
[718.48 --> 720.18]  Kan je natuurlijk ook heel veel uithalen.
[720.92 --> 723.74]  En eigenlijk hebben we dat data willen verzamelen.
[724.78 --> 727.40]  Uit allemaal slimme weegschaal.
[727.52 --> 728.10]  Slimme ring.
[728.76 --> 729.44]  Dat doen we.
[729.52 --> 730.64]  Ik schat een beetje in.
[730.70 --> 731.60]  Al een jaar of vijftien.
[731.60 --> 732.44]  En nu.
[732.60 --> 734.90]  Wat gebeurt er dan nu als je die data gaat voeren.
[735.40 --> 737.90]  Aan wat dan de huidige taalmodellen zijn.
[738.38 --> 740.24]  En gaat die health coach dan ook zeggen van.
[740.34 --> 740.58]  Nou ja.
[740.82 --> 741.52]  Vorig jaar.
[741.70 --> 743.42]  Ik heb een beetje je gewicht gezien.
[743.56 --> 745.52]  Hoe dat een beetje ontwikkelde op jouw.
[745.52 --> 747.74]  Why things of we things scale.
[748.12 --> 748.24]  Ja.
[748.24 --> 748.38]  Dus.
[749.04 --> 750.50]  We gaan nu naar een soort stap toe.
[750.56 --> 751.80]  Van de data is er eigenlijk al.
[751.86 --> 753.30]  De infrastructuur is er allemaal al.
[753.78 --> 756.06]  Er is al best wel een substantiële groep mensen bezig.
[756.10 --> 757.58]  Op racefietsen met bluetooth en zo.
[757.66 --> 759.62]  Die community is al best wel oud.
[759.62 --> 759.98]  Zeg maar.
[760.26 --> 762.42]  En nu gaat het dan mogelijk mainstream worden.
[762.52 --> 764.00]  Doordat het met je praat.
[764.36 --> 766.98]  Met misschien wel een gezicht en een vrolijke stem.
[767.34 --> 770.02]  Dus ik vind dat een interessante ontwikkeling.
[770.28 --> 771.28]  Mijn kanttekening is.
[771.44 --> 773.04]  En dat zullen meest luisteraars wel verwachten.
[774.16 --> 775.00]  Ik heb vrienden van mij.
[775.04 --> 776.20]  Die hebben zo'n apparaatje in de auto.
[776.34 --> 778.12]  Dan krijg je korting op je autoverzekering.
[778.22 --> 779.52]  Als je dat apparaatje in je auto zet.
[779.60 --> 781.12]  Want dan houden ze in de gaten hoe goed je rijdt.
[781.20 --> 782.26]  En als je dan een goede bestuurder bent.
[782.36 --> 782.96]  Krijg je korting.
[783.04 --> 783.76]  Dat bestaat zeg maar.
[784.20 --> 787.28]  Op die manier zou je natuurlijk ook kunnen voorstellen.
[787.38 --> 788.16]  Dat er korting komt.
[788.26 --> 788.80]  Of x of y.
[788.80 --> 790.50]  Omdat jij een goed leven hebt.
[791.02 --> 792.92]  Als je 10.000 stappen per dag hebt gezet.
[793.00 --> 793.42]  Iedere dag.
[793.50 --> 794.38]  Dan krijg je korting op de maat.
[794.64 --> 795.54]  Deze zeg maar.
[795.64 --> 798.30]  Een soort van data gevangenis.
[798.38 --> 800.14]  Of zo vind ik echt ontzettend vreselijk.
[800.40 --> 801.98]  En daar zou ik zo voor op de dam gaan staan.
[802.08 --> 802.42]  Om te zeggen.
[802.50 --> 802.90]  Doe het niet.
[803.04 --> 804.00]  Dat we dat even gezegd hebben.
[804.18 --> 805.42]  Ik vind dat gewoon irritant.
[805.78 --> 806.12]  Maar.
[807.02 --> 808.46]  Ik sta er dan wel weer open voor.
[808.56 --> 809.34]  Mogelijk dankzij.
[809.92 --> 810.48]  Misschien naïef.
[810.58 --> 812.06]  Maar goede wetgeving.
[812.20 --> 813.70]  Apple die het op je telefoon houdt.
[813.72 --> 814.82]  In je eigen persoonlijk profiel.
[814.90 --> 816.14]  En iedere keer kan je toestemming geven.
[816.20 --> 816.70]  Als je deelt.
[816.70 --> 818.94]  Want ik denk dat ik persoonlijk.
[819.04 --> 819.84]  Want toch natuurlijk aangekomen.
[819.94 --> 821.08]  Hoe gaat het nou met jouw wietse.
[821.32 --> 823.96]  Dus we kunnen het weer even uit de meta-analyse terugbrengen naar mij.
[825.10 --> 827.46]  Kan ik op een manier dat het mij aanstaat technisch.
[827.60 --> 828.88]  Dus dat ik weet waar de data staat.
[829.02 --> 830.24]  En naar wie dat wel toegaat.
[830.32 --> 830.74]  En wie niet.
[830.86 --> 832.48]  En apparaatjes kopen die lokaal zijn.
[832.80 --> 834.22]  Ben ik hier wel in geïnteresseerd.
[834.60 --> 835.64]  Om eens te kijken naar.
[835.98 --> 837.16]  We gaan een benchmarkje doen.
[837.22 --> 838.94]  Van een kwartaal wietse wat meer meten.
[838.94 --> 841.02]  En dan om een persoonlijk advies vragen.
[841.48 --> 843.16]  Zolang ik maar weet wie die data heeft.
[843.24 --> 843.98]  En wat er mee gebeurt.
[844.20 --> 849.02]  Ik denk dat dit heel groot kan worden.
[849.60 --> 851.50]  En dat we dat niet moeten onderschatten.
[851.70 --> 853.20]  Even om af te sluiten.
[853.44 --> 855.70]  Nog een quote van Ariana Huffington.
[856.16 --> 857.98]  We know that when people get value.
[858.18 --> 859.80]  They're willing to share a lot.
[859.80 --> 861.44]  Sluit er een heel.
[861.44 --> 861.88]  Jijks.
[862.02 --> 862.54]  On that note.
[862.56 --> 862.98]  Is het uit Blijkmeer?
[863.24 --> 864.00]  Of zei zei zei.
[864.70 --> 865.10]  Sorry.
[867.64 --> 869.36]  Ja dan even wat korter nieuws.
[869.48 --> 870.54]  Wat we nog even mee moeten nemen.
[870.68 --> 871.14]  Twee dingen.
[871.74 --> 872.08]  Ten eerste.
[872.88 --> 874.44]  Er was een hack bij OpenAI.
[874.84 --> 877.58]  Dat is al gebeurd in begin 2023.
[878.48 --> 881.00]  Maar het is destijds gedeeld met werknemers.
[881.20 --> 882.72]  En het was van nou dit is gebeurd jongens.
[883.72 --> 887.06]  Er zijn geen klantgegevens buitengemaakt.
[887.06 --> 888.70]  Dus hoeven we het niet verder naar buiten te brengen.
[888.70 --> 890.88]  Dat is onlangs toch naar buiten gekomen.
[891.30 --> 894.10]  Dus wij weten nu wel dat er een hack daar is geweest.
[894.22 --> 895.98]  Ook al heeft het niet heel veel impact gehad.
[896.66 --> 899.44]  Waarom het toch even goed was belachtig om mee te nemen.
[900.06 --> 902.36]  Het legt natuurlijk wel weer de kwetsbaarheid bloot.
[902.52 --> 904.60]  Van die AI bedrijven.
[904.66 --> 906.48]  Die heel veel gewoon data hebben.
[906.64 --> 907.84]  Maar ik vraag me tegelijkertijd wel af.
[907.88 --> 909.20]  Hoe groot het verschil nou eigenlijk is.
[909.48 --> 912.38]  Tussen heel veel data op een server hebben staan.
[912.50 --> 914.48]  Van Google voor je documenten.
[914.48 --> 917.94]  Of al je data op servers hebben staan van Amazon.
[917.94 --> 920.90]  En heel veel data aan OpenAI geven.
[921.00 --> 925.08]  Wat in feite ook diezelfde data aan de Microsoft Cloud geven.
[925.52 --> 927.26]  Dus ja wat is nou eigenlijk het verschil.
[927.40 --> 931.30]  Dus voor consumenten data vraag ik me toch af.
[931.40 --> 933.26]  Wat nou precies het risico is.
[933.34 --> 935.44]  Ten opzichte van wat nu al gebruikelijk is.
[935.50 --> 936.86]  Met het opslaan van data en de cloud.
[937.54 --> 938.40]  Wat denk jij Wittje?
[938.40 --> 942.80]  Ik snap je gedachtengang van het komt dan toch op diezelfde cloud terecht.
[943.02 --> 946.06]  Want OpenAI heeft zelf geen infrastructuur.
[946.66 --> 947.96]  Dat is allemaal ingehuurd.
[948.06 --> 949.62]  En vaak via dealtjes met Microsoft.
[949.76 --> 951.10]  Die zegt we doen een grote investering.
[951.20 --> 953.14]  Dan krijg je heel veel ruimte op onze cloud.
[954.00 --> 957.48]  Maar er is wel een verschil dat OpenAI is een start-up.
[958.02 --> 959.48]  Ik vergeet dat zelf ook nog wel eens.
[959.48 --> 960.58]  Omdat ze heel groot zijn.
[960.74 --> 963.80]  En een heel grote rol spelen nu binnen de AI bubbel.
[963.90 --> 964.86]  Laat ik het dan even zo zeggen.
[965.14 --> 966.80]  Maar OpenAI is gewoon een start-up.
[967.26 --> 969.10]  En start-ups hebben slechte security.
[969.70 --> 971.42]  Beetje cowboy mentaliteit.
[972.58 --> 972.98]  En je kunt.
[973.54 --> 974.36]  Je krijgt zeg maar.
[975.36 --> 978.92]  Tuurlijk op het moment dat jij cloud infrastructuur van Google, Oracle, Microsoft, whatever.
[979.42 --> 981.34]  Gebruikt zit daar een bepaalde security bij.
[981.70 --> 982.60]  Want je zit op hun ding.
[982.60 --> 983.78]  Maar je kunt het nog steeds.
[984.94 --> 986.78]  Deuren openzetten naar hun fort.
[987.44 --> 989.58]  Dus je kunt als jij slechte software bouwt.
[989.72 --> 991.82]  Die aan alle kanten open is.
[992.08 --> 993.74]  Op een veilige infrastructuur.
[993.82 --> 995.16]  Alsnog iets onveiligs neerzetten.
[995.72 --> 996.60]  En ik denk wel dat.
[997.38 --> 999.72]  Als we het dan hebben over data nummer 1.
[999.96 --> 1002.60]  Namelijk de data van de mensen die dagelijks interacteren met OpenAI.
[1003.50 --> 1004.92]  Dat kunnen mensen zijn die dat voor hun werk doen.
[1005.04 --> 1005.68]  Maar ook privé.
[1007.50 --> 1012.18]  Dat je je daar wel als gebruiker bewust moet zijn.
[1012.18 --> 1014.38]  En dat je met een start-up interakteert.
[1014.50 --> 1018.50]  En dat je niet de security kan verwachten van een Google die al heel lang jouw mail host.
[1018.92 --> 1020.34]  Ook al staan die servers naast elkaar.
[1020.52 --> 1023.08]  Een andere dimensie is natuurlijk het soort van geopolitieke.
[1023.26 --> 1025.62]  Namelijk is er geopolitieke macht.
[1025.90 --> 1028.90]  Bij gratie van het hebben van grote AI bedrijven in je land.
[1029.44 --> 1030.02]  Ik zou zeggen.
[1030.14 --> 1031.64]  Het antwoord is ja op die vraag.
[1032.16 --> 1036.14]  Dus het is ook zoals de Chinezen proberen de chipmachines van ASML te hacken.
[1036.56 --> 1039.68]  Om een soort van corporate theft te doen.
[1039.78 --> 1041.20]  Waarvan we weten dat dat gewoon gebeurt.
[1041.20 --> 1045.84]  En dat er spionnen infiltreren bij ASML om data naar buiten te krijgen.
[1046.86 --> 1050.52]  Het zou me zeer verbazen als dat niet gebeurt bij OpenAI.
[1050.66 --> 1051.86]  Want de belangen zijn zo groot.
[1052.80 --> 1054.44]  Waarom zou China niet proberen te hacken?
[1054.52 --> 1058.96]  Dus het feit dat ze een ex-NSC-generaal hebben aangesteld in hun board.
[1059.94 --> 1062.22]  Heeft waarschijnlijk iets te maken met die dimensie.
[1062.32 --> 1063.66]  Waar hebben we het dan over?
[1064.20 --> 1068.14]  Dus de data van de potentieel van de klanten die iedere dag praten met OpenAI.
[1068.14 --> 1069.66]  Is wat de meeste mensen denken aan denken.
[1070.28 --> 1072.52]  Maar er is heel veel data binnen OpenAI.
[1072.70 --> 1074.10]  Namelijk hun kroonjuwelen.
[1074.86 --> 1076.56]  Dat zijn de modellen die ze trainen.
[1076.78 --> 1080.06]  Die kosten letterlijk miljarden om te trainen.
[1080.14 --> 1082.82]  En als die trainingsset, de trainingsrun klaar is.
[1082.90 --> 1084.46]  Dan heb je een heel groot bestand.
[1084.58 --> 1085.02]  Dat is echt zo.
[1085.12 --> 1086.60]  Een heel groot bestand.
[1086.60 --> 1088.44]  Die je dan uitrolt in je datacenter.
[1088.58 --> 1089.54]  Dan gaan mensen daarmee praten.
[1089.66 --> 1090.82]  In dit geval GPT-4O.
[1091.30 --> 1093.20]  En de gewichten van dat model.
[1093.32 --> 1094.92]  Dat is een beetje hoe er dan over gesproken wordt.
[1095.72 --> 1099.34]  Gewichten zijn een beetje de hoeveelheid gram in een recept.
[1099.88 --> 1101.00]  Als je iets gaat koken.
[1101.16 --> 1101.98]  Hoeveel gram suiker.
[1102.12 --> 1102.70]  Hoeveel gram zout.
[1103.08 --> 1104.14]  Er zitten allemaal gewichten in.
[1104.48 --> 1105.70]  En al die gewichten bij elkaar.
[1106.50 --> 1108.38]  Heel veel verschillende waardes.
[1108.74 --> 1110.16]  Creëren uiteindelijk zo'n model.
[1110.42 --> 1114.28]  En als je die gewichten kan kopiëren, stelen, dupliceren.
[1114.28 --> 1116.28]  Dan heb je in essentie ook dat model.
[1116.60 --> 1118.56]  Als je ook die computers hebt om het op te draaien.
[1119.04 --> 1120.04]  Dus het is snel gestolen.
[1120.38 --> 1120.50]  Ja.
[1120.62 --> 1123.64]  Net zoals de chip van de machine van ASML.
[1123.64 --> 1125.62]  Dan heb je de blauwdruk van die machine.
[1125.80 --> 1126.80]  En dan moet je die nog gaan bouwen.
[1126.92 --> 1127.70]  Maar in het geval van.
[1127.82 --> 1129.80]  Als jij dezelfde hardware hebt staan.
[1129.92 --> 1132.56]  Dan als OpenAI huurt bij de verschillende cloud providers.
[1132.68 --> 1135.12]  Dan zou jij GPT-4O ook kunnen draaien.
[1135.36 --> 1137.68]  Op je eigen infrastructuur.
[1137.74 --> 1138.90]  En dat is best wel een dingetje.
[1139.04 --> 1139.22]  Ja.
[1139.30 --> 1139.96]  En jullie zeggen eigenlijk.
[1140.04 --> 1141.96]  Dat is gewoon inherent aan een industrie.
[1142.30 --> 1144.26]  Gewoon dat er spionageknop treed.
[1144.28 --> 1145.20]  En dat je bestolen wordt.
[1145.40 --> 1145.62]  Ja.
[1145.80 --> 1146.80]  En hier nog meer dan bij anderen.
[1147.14 --> 1148.86]  En moeten ze er dan opener over zijn.
[1149.10 --> 1149.62]  Bij OpenAI.
[1150.28 --> 1152.48]  Dat horen wij nu pas.
[1153.02 --> 1153.26]  Nou.
[1153.46 --> 1157.14]  Ik denk wel dat OpenAI een flink PR probleem heeft.
[1157.30 --> 1159.28]  Met betrekking tot het onderwerp betrouwbaarheid.
[1159.94 --> 1161.90]  En dat komt door een hele grote som van dingen.
[1162.12 --> 1163.28]  En dit helpt daar niet bij.
[1163.28 --> 1163.36]  Ja.
[1163.36 --> 1165.42]  En het is misschien wel goed om nog te nuanceren.
[1165.50 --> 1167.14]  Ik ben niet te diep in die hack gedoken.
[1167.30 --> 1170.20]  Maar dit is niet de eerste keer dat een softwarebedrijf.
[1170.20 --> 1171.52]  Of SaaS provider of whatever.
[1172.14 --> 1172.88]  Een hack heeft.
[1173.70 --> 1176.30]  Als softwarebedrijf gebruik je allerlei diensten.
[1176.64 --> 1177.88]  Je hebt een support system.
[1177.88 --> 1179.72]  Ticketing system draaien ergens.
[1179.98 --> 1182.30]  Er zit een andere dienst op.
[1182.36 --> 1184.50]  Zodat mensen zich kunnen aanmelden voor je mailing list.
[1184.70 --> 1187.04]  Noem het survey monkey.
[1187.14 --> 1187.38]  Whatever.
[1187.50 --> 1188.08]  Je doet van alles.
[1188.08 --> 1191.02]  Dus je hebt een heel boeket aan diensten.
[1191.96 --> 1194.60]  Als een van die diensten natuurlijk niet goed ingesteld staat.
[1194.74 --> 1195.88]  En die wordt dan gehackt.
[1196.34 --> 1197.40]  Ben jij dan gehackt?
[1197.48 --> 1197.68]  Ja.
[1197.84 --> 1198.96]  Maar wat is er dan gehackt?
[1198.98 --> 1199.90]  Dat is een beetje de vraag.
[1200.62 --> 1202.86]  En dat zei jij ook mooi aan het begin Milou.
[1203.42 --> 1205.00]  OpenAI vindt het allemaal wel meevallen.
[1205.52 --> 1206.24]  Logisch natuurlijk.
[1206.90 --> 1209.26]  Maar tegelijkertijd kun jij via een support desk.
[1209.44 --> 1211.84]  Dankzij het voordoen van de CEO.
[1212.14 --> 1213.44]  Alsnog allemaal dingen downloaden.
[1213.58 --> 1214.52]  Dus je kan wel zeggen.
[1214.64 --> 1215.74]  Het was maar de support desk.
[1215.84 --> 1217.36]  Maar dat kan echt wel een prima raam zijn.
[1217.36 --> 1218.12]  Om naar binnen te klimmen.
[1218.22 --> 1219.28]  Dus dat even gezegd hebben.
[1221.78 --> 1225.40]  Maak je je zorgen over de veiligheid van je persoonlijke gegevens?
[1226.28 --> 1229.48]  In deze tijd van digitale informatie en slimme technologie.
[1229.86 --> 1233.44]  Lijkt het alsof je privégegevens steeds minder privé zijn.
[1234.34 --> 1237.64]  Datamakelaars verzamelen en verkopen jouw persoonlijke gegevens.
[1238.14 --> 1241.68]  Waardoor jouw informatie in de handen van allerlei partijen terecht komt.
[1242.26 --> 1243.84]  Van hackers tot marketeers.
[1243.84 --> 1245.76]  Maar gelukkig is er InCogni.
[1246.06 --> 1247.64]  Jouw digitale beschermer.
[1248.26 --> 1251.76]  InCogni beperkt namelijk de publieke toegang tot de privégegevens.
[1251.96 --> 1255.28]  Waardoor je het risico op identiteitsdiefstal sterk verkleint.
[1255.66 --> 1257.84]  En de controle over je persoonlijke data behoudt.
[1259.10 --> 1262.84]  Wil jij ook de controle over je informatie behouden en je privacy beschermen?
[1262.84 --> 1265.10]  Ga dan nu naar incogni.com.
[1265.10 --> 1266.38]  Slash pokeypot.
[1266.56 --> 1269.52]  En krijg 60% korting op jouw InCogni account.
[1269.96 --> 1271.86]  Dus dat is incogni.com.
[1272.00 --> 1272.86]  Slash pokeypot.
[1273.40 --> 1276.44]  Omdat jouw privacy het waard is om te beschermen.
[1276.44 --> 1282.38]  En dan het laatste nieuwtje van vandaag.
[1283.30 --> 1285.40]  Een Franse chatbot, jongens.
[1285.62 --> 1287.64]  We hebben ook iets gepresteerd op ons continent.
[1288.86 --> 1289.64]  Ik heb er...
[1289.64 --> 1292.86]  Het is een chatbot en die zou heel snel kunnen reageren.
[1293.00 --> 1294.30]  Met een hele echte stem.
[1294.96 --> 1296.80]  Dat heb ik natuurlijk even uitgeprobeerd.
[1296.88 --> 1297.86]  Het heet Moshi.
[1297.86 --> 1298.50]  Moshi.
[1298.84 --> 1299.76]  Ja, het heet Moshi.
[1299.88 --> 1302.08]  En je gaat naar Moshi.ai of Moshi.chat.
[1302.18 --> 1302.94]  Nou, twijfel ik even.
[1303.10 --> 1304.28]  Even kijken waar het was.
[1305.60 --> 1307.06]  Ja, Moshi.chat.
[1307.22 --> 1307.82]  Daar ga je naartoe.
[1308.46 --> 1310.88]  En dan kun je een gesprek voeren inderdaad met een chatbot.
[1311.16 --> 1313.00]  En ja, die zou super snel reageren.
[1313.38 --> 1314.82]  En dat is inderdaad zo.
[1314.90 --> 1316.28]  Het is echt zonder vertraging.
[1316.36 --> 1319.98]  Het is alsof ik met een echt mens in dat opzicht aan het praten ben.
[1319.98 --> 1322.78]  Want die hoeft niet te wachten of te registreren, wat ik heb gezegd.
[1323.10 --> 1324.86]  Maar het werkt echt goed en snel.
[1324.86 --> 1332.40]  En dat is bedacht door, nou bedacht, gemaakt door slechts acht onderzoekers binnen zes maanden tijd.
[1332.70 --> 1335.20]  Dus dan denken jullie, wauw, wat fantastisch.
[1335.90 --> 1338.52]  En nou, mijn ervaring was als volgt.
[1339.14 --> 1345.08]  Ik dacht, ik had net ontbeten en ik zat me te beraden op wat ik aan ging trekken vandaag.
[1345.48 --> 1347.48]  Dus ik heb het dus uitgeprobeerd vanochtend.
[1347.48 --> 1351.24]  Ik zat aan de ontbijttafel en ik was me aan het beraden op wat ik vandaag aan trekken.
[1351.24 --> 1358.90]  Dus ik vroeg aan moshi.chat, wat zal ik vandaag aandoen? Kun je me daarbij helpen?
[1359.30 --> 1360.98]  Hey, what's going on?
[1361.56 --> 1364.82]  Hey, wat zal ik vandaag aandoen? Kun je me daar misschien bij helpen?
[1367.82 --> 1369.72]  Sure, what do you want to wear?
[1370.48 --> 1372.56]  Nou, laten we beginnen met het weer.
[1374.24 --> 1377.12]  Sure, let's start with the weather. It's sunny and warm.
[1377.12 --> 1379.54]  Weet je waar ik ben dan?
[1379.56 --> 1381.24]  I do. You are in Paris, France.
[1381.74 --> 1383.20]  Nee, ik ben in Amsterdam.
[1384.34 --> 1388.82]  Oh, sorry. I got that wrong. You are in Amsterdam, the United Kingdom.
[1389.62 --> 1390.86]  Nee, ik ben in Nederland.
[1393.66 --> 1398.44]  Oh, I'm sorry. I'm still not sure where you are. Can you tell me what the weather is like?
[1399.50 --> 1400.94]  Ik hoopte dat jij dat kon doen.
[1400.94 --> 1405.56]  Oké, the weather in the Netherlands is sunny and warm.
[1406.40 --> 1407.10]  En in Celsius?
[1407.10 --> 1408.16]  It's 80 degrees Fahrenheit.
[1408.78 --> 1409.50]  En in Celsius?
[1409.98 --> 1410.98]  Yes, in Celsius.
[1412.84 --> 1416.28]  Nou goed, het verliefd is niet heel soepel.
[1417.14 --> 1418.76]  Ze reageerde wel echt snel.
[1419.00 --> 1420.12]  Ze praat wel snel.
[1420.12 --> 1424.90]  Ja, maar het werkt, ja, je hebt er weinig aan.
[1425.40 --> 1431.32]  Ik heb ook een ander gesprek met haar gevoeld over vegetarisch zijn en toen bleef ze maar vragen,
[1432.60 --> 1437.10]  of ze bleef maar zeggen, I don't know, I don't know, I don't know, I don't know.
[1437.46 --> 1440.36]  What do you think? What do you think? What do you think?
[1440.86 --> 1442.36]  Nou, daar ben ik ook helemaal lijp van.
[1442.76 --> 1445.42]  Dus, het is een hele mooie app.
[1445.52 --> 1446.42]  Ja, dank je voor de tip.
[1446.42 --> 1449.46]  Snel, maar niet zo heel erg slim.
[1451.60 --> 1453.94]  Nee, maar dat is een bekend verhaal, Wietse.
[1454.10 --> 1456.90]  Dat heeft met latentie of latency te maken.
[1456.92 --> 1459.12]  Ik hou, laten we latentie er alsjeblieft in houden.
[1460.12 --> 1465.22]  Ik denk dat, kijk, dit zal een, ik heb er een beetje naar gekeken,
[1465.32 --> 1468.82]  maar ze waren ook niet helemaal, ik kon niet alles uitvinden hoe ze het doen,
[1468.82 --> 1474.92]  maar een van de manieren om te zorgen dat zo'n taalmodel sneller reageert,
[1475.02 --> 1476.70]  is een kleiner taalmodel gebruiken.
[1477.60 --> 1479.74]  Want dan kan je, ja, dan draait het gewoon sneller,
[1479.84 --> 1481.54]  want er hoeft minder in het geheugen te laten worden, et cetera.
[1481.82 --> 1483.74]  Het is een kleiner stukje software, dus het kan sneller draaien.
[1484.42 --> 1488.00]  En ze doen allerlei trucjes om een beetje te anticiperen op wat er komt.
[1488.12 --> 1490.60]  Dus hij blufft een beetje, in dit geval zij blufft een beetje,
[1490.94 --> 1493.64]  ze begint dan een beetje eerder te praten, gokt wat het einde is van je zinnen.
[1494.26 --> 1497.32]  Ze kunnen het in zes maanden doen, omdat er best wat leuke trucjes in zitten.
[1497.32 --> 1505.02]  Ja, dit is niet op het niveau van wat OpenAI ons heeft laten zien bij hun summer event,
[1505.36 --> 1507.22]  spring update moet ik zeggen, niet summer.
[1507.96 --> 1510.38]  Maar ja, dit staat wel live en het is gemaakt in Europa.
[1510.60 --> 1512.08]  Dat zijn twee best wel dingen.
[1513.48 --> 1517.32]  Ik ben niet zelf, niet super onder de indruk, maar ik vind het wel fijn,
[1518.02 --> 1521.08]  net als dat we Mistral, blijf het even doen,
[1521.40 --> 1525.52]  toch ook hebben als grote AI-speler in Europa.
[1525.52 --> 1526.86]  Ik denk dat dat wel belangrijk is.
[1527.32 --> 1529.76]  En ik ben benieuwd wat ze over zes maanden hebben.
[1529.90 --> 1532.22]  Misschien wel iets wat in de buurt komt van wat OpenAI doet.
[1533.62 --> 1535.78]  Ja, en het komt uit een sympathiek non-profit lab.
[1535.86 --> 1536.92]  Moet ik alleen maar toejuichen.
[1537.24 --> 1540.20]  Nou, straks gaan we het hebben over character AI,
[1540.36 --> 1542.46]  die, en niet om jou te overtroeven, Milou,
[1542.62 --> 1546.98]  wel snel terugpraat met intelligentie en met weinig ledensie.
[1547.04 --> 1548.06]  Dat hoor je straks.
[1548.14 --> 1549.14]  Maar eerste tip van de week.
[1549.14 --> 1558.18]  Cloud Console, die hebben een, dat is eigenlijk de soort van programmeursversie van Cloud,
[1558.30 --> 1559.40]  het abonnementsmodel.
[1559.90 --> 1564.22]  Dus je hebt zeg maar de chat GPT van Cloud, daar betaal je 20 dollar per maand voor,
[1564.28 --> 1566.46]  dan heb je een mooie interface en alles is simpel.
[1566.86 --> 1570.94]  Maar je hebt ook een interface die iets ingewikkelder is en die meer geënt is op programmeurs.
[1570.94 --> 1573.40]  Nou, de laatste ga ik het nu over hebben.
[1573.60 --> 1576.08]  Dat is dus iets meer gedoe om in te komen,
[1576.20 --> 1578.40]  maar het geeft wel een aardig inkijkje in wat ze gaan doen.
[1579.36 --> 1583.96]  Sinds deze week is er een functie toegevoegd, die heet Start Prompting with Cloud.
[1584.14 --> 1586.18]  En als je er eenmaal in zit, is het vrij eenvoudig.
[1586.42 --> 1587.14]  Hoe bedoel je erin zit?
[1587.40 --> 1589.90]  Ja, je moet, het is zeg maar niet die 20 dollar per maand,
[1590.02 --> 1591.50]  je betaalt dan pay as you go.
[1591.62 --> 1594.70]  Dus je moet dan te goed erop zetten van 10 dollar of zo,
[1594.90 --> 1598.48]  en dat kan je dan gewoon opgebruiken per opdracht die je invoert.
[1598.48 --> 1600.76]  Dus iedere opdracht kost geld, heel weinig hoor.
[1600.76 --> 1602.78]  Dus uiteindelijk ben je nog goedkoper uit dan het abonnement.
[1603.20 --> 1604.72]  Ik zou zeggen, doe dat gewoon.
[1605.02 --> 1607.20]  Je hebt toch je creditcard al ingevoerd voor dat abonnement,
[1607.28 --> 1609.70]  dan kun je net zo goed die 10 euro nog op dat account zetten.
[1610.10 --> 1612.26]  En vanaf dat moment kun je dus de console gebruiken.
[1612.34 --> 1615.30]  Het is hetzelfde als de gewone Cloud,
[1615.74 --> 1618.04]  maar dan witte letters op zwarte tekst,
[1618.16 --> 1620.48]  of op zwarte achtergrond, want dat is wat nerds willen.
[1621.02 --> 1624.02]  Dan kun je op Start Prompting with Cloud drukken.
[1624.18 --> 1627.18]  En wat dat ding dan doet, is betere prompts voor jou schrijven.
[1627.18 --> 1633.18]  Dus bijvoorbeeld, stel je wil een e-mail schrijven aan een...
[1634.10 --> 1636.96]  Je hebt een klantenserviceverzoek binnengekregen bij je bedrijf,
[1637.32 --> 1641.88]  en je wil een e-mail laten genereren als reply op die e-mail,
[1642.34 --> 1645.86]  dan zou je nu zeggen schrijf een reply op deze e-mail,
[1646.00 --> 1647.94]  en dan plak je die e-mail van die klant erin,
[1648.02 --> 1649.18]  en dan gaat Cloud dat doen.
[1649.62 --> 1653.68]  Maar in dit geval kun je een betere prompt laten genereren door Cloud,
[1653.68 --> 1655.98]  waardoor het eindresultaat beter gaat zijn.
[1656.34 --> 1659.28]  Dus stel je voor, je voert in als draft prompt,
[1659.56 --> 1662.42]  draft an e-mail responding to a customer complaint e-mail,
[1662.52 --> 1663.46]  and offer a resolution.
[1664.18 --> 1664.82]  Nou, simpel.
[1665.36 --> 1669.26]  Een klant heeft ge-emailed, verzin wat ik moet zeggen.
[1669.96 --> 1673.50]  Dan zou je dat zo kunnen invoeren,
[1673.60 --> 1675.56]  en dan krijg je niet zo'n waanzinnig resultaat.
[1676.04 --> 1678.02]  Maar als je dan op Generate a Prompt klikt,
[1678.14 --> 1681.48]  oftewel je neemt die tussenstap voordat je de output krijgt,
[1681.48 --> 1683.28]  dan gaat dat ding een prompt schrijven,
[1683.48 --> 1686.12]  en dat is een fucking uitgebreid prompt,
[1686.62 --> 1691.74]  waarin die dus eigenlijk prompt op een manier die die AI veel lekkerder vindt,
[1691.80 --> 1693.44]  waardoor die veel beter kan reageren.
[1693.60 --> 1694.72]  Hij gaat voor jou nadenken.
[1694.80 --> 1695.62]  Hij gaat voor jou nadenken.
[1695.70 --> 1697.88]  Ik ga het hele prompt niet voorlezen waar die mee komt,
[1698.00 --> 1699.94]  maar ik zal je een klein inkijkje geven.
[1700.08 --> 1701.96]  Ik denk dat het hele prompt is denk ik wel,
[1702.66 --> 1705.04]  nou, ik denk honderd regels tekst of zo.
[1705.04 --> 1719.40]  Dus wat hij gaat doen,
[1719.54 --> 1721.84]  hier geeft hij al een idee over wat de toon moet zijn,
[1721.92 --> 1723.06]  wat hij normaal zou invullen,
[1723.12 --> 1724.62]  omdat jij dat niet gespecificeerd hebt.
[1724.70 --> 1725.96]  Hier specificeert dat,
[1726.12 --> 1729.66]  waardoor je weet wat je eindresultaat gaat zijn.
[1730.06 --> 1732.28]  En vervolgens gaat hij het in stapjes doen.
[1732.28 --> 1735.90]  Dus hij zegt, first, review the customer's complaint email.
[1736.08 --> 1739.44]  En dan zet hij als het ware een soort van placeholder erin,
[1739.66 --> 1741.52]  een soort van hier moet de complaint komen.
[1742.00 --> 1743.52]  En daar vraagt hij dan later,
[1743.86 --> 1745.34]  als je hem gaat draaien,
[1745.42 --> 1747.06]  als je hem runt, zo heet dat dan,
[1747.08 --> 1747.96]  als je op e-end er drukt in feite,
[1748.50 --> 1751.42]  dan gaat hij dus vragen, wat was de klacht van de klant?
[1752.00 --> 1753.80]  Nou, die voer je dan in op dat moment.
[1754.20 --> 1757.42]  Dan zegt hij, next, familiarize yourself with the company's policies
[1757.42 --> 1759.56]  and guidelines for handling customer complaints.
[1759.56 --> 1760.86]  Dan is er weer zo'n placeholder.
[1760.86 --> 1764.58]  Oftewel, hij vraagt hier, wat zijn eigenlijk de policies van jouw bedrijf?
[1764.64 --> 1766.34]  Want als je wil kunnen reageren op een e-mail,
[1766.76 --> 1767.94]  moet je die policies kennen.
[1768.48 --> 1770.86]  Normaal gesproken gaat de klant dat dan zitten verzinnen,
[1771.04 --> 1771.92]  wat die policies zijn.
[1772.38 --> 1774.46]  En dan ga je een e-mail terugkrijgen,
[1774.54 --> 1775.94]  waar je als eindgebruiker denkt,
[1776.36 --> 1778.28]  dit is niet zo goed, want zo werkt het niet bij ons.
[1778.36 --> 1779.30]  En dan geef je die AI de schuld.
[1779.38 --> 1782.18]  Maar je hebt die AI nooit verteld wat jouw policies zijn.
[1782.30 --> 1784.02]  Dus hier vraagt hij daar concreet om.
[1784.20 --> 1785.12]  Die gaat hij dan gebruiken.
[1785.12 --> 1788.88]  Dan gaat hij alle stappen doen,
[1789.04 --> 1791.94]  die een mens normaal gesproken in zijn hoofd zou doen.
[1792.38 --> 1794.30]  Maar hier door dat ding uitgeschreven.
[1794.44 --> 1797.02]  Dus, now, follow these steps to draft a response,
[1797.20 --> 1799.04]  analyze the complaint, draft a response,
[1799.18 --> 1800.90]  offer a resolution, conclude the e-mail.
[1800.96 --> 1803.32]  En daar zitten dan ongeveer 1, 2, 3, 4, 5, 6, 7,
[1803.32 --> 1804.10]  Ik ga ze allemaal niet tellen,
[1804.14 --> 1806.76]  iets van 20 tussenstappen tussen om deze dingen te doen.
[1807.32 --> 1810.48]  Om dan met de oplossing te komen.
[1810.76 --> 1812.48]  Dus, met andere woorden,
[1813.06 --> 1815.00]  eigenlijk wordt hier expliciet gemaakt,
[1815.18 --> 1818.76]  wat normaal gesproken onder water gebeurt.
[1819.00 --> 1821.90]  En daardoor krijg je niet alleen een beter resultaat,
[1822.16 --> 1824.14]  omdat je gewoon je prompt uitgebreider is.
[1824.58 --> 1829.34]  Je hebt ook meer invloed over wat er onder water eigenlijk allemaal gebeurt,
[1829.34 --> 1831.00]  waardoor je de prompt kan aanpassen.
[1831.00 --> 1834.20]  En dus, waarschijnlijk de volgende keer dat je het gebruikt,
[1835.16 --> 1837.40]  dat die weer net zo goed werkt,
[1837.48 --> 1839.58]  als dat je de eerste keer hem hebt aangepast.
[1840.38 --> 1841.02]  Dus dat is de tip.
[1841.18 --> 1842.04]  Ik vind het een onbezind doordig.
[1842.42 --> 1844.78]  Ja, ik vorige aflevering maar zeggen,
[1845.06 --> 1848.76]  dat AI zo'n onpersoonlijke e-mail oplevert.
[1848.80 --> 1850.06]  Maar dit is hyperpersoonlijk.
[1850.18 --> 1853.72]  Het is professionally empathetically zelfs, Milou.
[1853.78 --> 1854.68]  Dus onpersoonlijk.
[1854.78 --> 1856.78]  Ik zou zeggen, het is ontzettend empathisch.
[1856.78 --> 1858.10]  Ik vind het mooi om te zien dat,
[1858.42 --> 1860.04]  je bent gewoon software aan het maken nu.
[1860.04 --> 1863.32]  Ik programmeer veel, je bent software aan het maken.
[1863.40 --> 1865.40]  Niet gewoon software, dit hadden we al.
[1865.62 --> 1867.06]  Maar ik vind het fascinerend.
[1867.16 --> 1867.82]  Maar de logica.
[1867.84 --> 1870.52]  Ja, want er zitten variabelen in, er zit een logica in.
[1870.94 --> 1873.16]  En eigenlijk zie je achter de schermen meer wat er gebeurt.
[1873.24 --> 1874.78]  En dan kan je daar invloed op uitoefenen.
[1875.20 --> 1877.92]  En wat ik nog wilde toevoegen is,
[1878.46 --> 1883.88]  dit is een beetje een trend die je ziet van AI's die AI's aansturen.
[1883.88 --> 1886.20]  Dus we hebben al AI's die getraind worden op data,
[1886.30 --> 1887.26]  gegenereerd door AI.
[1887.50 --> 1889.84]  Waar dan een soort veel mensen een beetje ogen bij rollen van,
[1890.12 --> 1891.04]  gaat dat wel goed?
[1891.24 --> 1893.40]  Maar ja, onderzoek wijst uit, dat is niet zinloos.
[1893.98 --> 1897.80]  En dat je een soort meta-AI's of agentic AI,
[1897.98 --> 1899.88]  zoals het dan in het wereldje heet,
[1900.42 --> 1903.92]  dat je een AI krijgt die ook sub-AI'tjes kan gaan aansturen.
[1904.04 --> 1906.78]  Dat je zegt, nou ik heb één model die is heel goed in klantenservice,
[1906.94 --> 1909.04]  één model die is heel goed in data-analyse.
[1909.56 --> 1912.34]  Ik ga een pront maken die misschien wel twee modellen aanspreekt.
[1912.34 --> 1917.02]  En dat je dat niet alleen maar op de achtergrond verborgen gebeurt,
[1917.18 --> 1918.10]  als een soort magie,
[1918.48 --> 1920.62]  maar dat je kan zeggen, ik wil het even openmaken,
[1920.66 --> 1923.00]  want ik heb een paar tweaks die ik wil doen op bepaalde punten.
[1923.10 --> 1925.50]  En dan ben je echt richting software aan het gaan.
[1925.84 --> 1926.46]  Dat is fascinerend.
[1926.64 --> 1928.06]  En heel tof om te zien hoe,
[1928.78 --> 1930.44]  we hebben het ineens veel meer over Antropic nu.
[1930.84 --> 1933.46]  Open AI valt wat minder en Antropic valt wat meer.
[1933.86 --> 1935.24]  Die zijn nu echt qua tooling,
[1935.50 --> 1937.18]  dus wat ze aanbieden binnen hun,
[1937.18 --> 1938.58]  wat Alexander noemt PSUGO,
[1938.74 --> 1941.54]  de plek waar veel programmeurs een tijd doorbrengen om dingen te bouwen,
[1941.54 --> 1945.02]  dat daar binnen allerlei handige snufjes gebouwd worden,
[1945.14 --> 1946.78]  om maar zoveel mogelijk mensen,
[1946.92 --> 1949.80]  die misschien minder hardcore programmeur zijn,
[1950.20 --> 1951.56]  het wereldje in te brengen van,
[1952.00 --> 1953.06]  als jij dit echt gaaf vindt,
[1953.08 --> 1954.34]  en je ziet hier de potentie van,
[1954.78 --> 1955.50]  dit zijn de tools.
[1955.82 --> 1958.32]  Nou, en ook, ik denk echt handig voor niet-programmeurs.
[1959.00 --> 1960.84]  Ik denk dat dit voor iedereen die werkt,
[1961.00 --> 1963.00]  heel nuttig zou kunnen zijn.
[1963.46 --> 1965.34]  En dan is dit mijn moment om reclame te maken.
[1965.34 --> 1967.62]  Als je meer AI wil toepassen in je werk,
[1967.90 --> 1969.48]  en deze tip zit erbij,
[1969.48 --> 1971.16]  maar er zitten meer tips in deze week,
[1971.58 --> 1974.80]  dan kun je je aanmelden voor onze nieuwsbrief via AI-report.email.
[1975.26 --> 1977.94]  Straks het hoofdonderwerp over Character AI,
[1978.14 --> 1979.22]  maar eerst een bericht van onze sponsor.
[1979.22 --> 1984.36]  Ja, heb jij ook zo'n zin in vakantie?
[1984.60 --> 1987.84]  Even van je scherm weg en niet bezig met data en AI?
[1988.40 --> 1989.16]  Nou, geniet ervan,
[1989.28 --> 1991.44]  maar zorg ervoor dat je vast je gratis ticket
[1991.44 --> 1994.38]  voor de Data Expo in de jaarbeurs Utrecht claimt,
[1994.92 --> 1997.02]  zodat je direct na de zomer weer helemaal up-to-date bent
[1997.02 --> 2000.32]  met de laatste trends en ontwikkelingen op het gebied van data en AI.
[2000.88 --> 2004.92]  Met maar liefst 120 sprekers van onder andere AHOLT, NS, KLM,
[2004.92 --> 2007.34]  is dit een evenement dat je niet wilt missen.
[2007.68 --> 2011.00]  De Data Expo vindt plaats op 11 en 12 september in de jaarbeurs Utrecht.
[2011.14 --> 2014.06]  Hier vind je alle data, analytics en cloud oplossingen
[2014.06 --> 2015.38]  efficiënt onder één dak.
[2015.82 --> 2018.86]  Je kunt tools, technologieën en partners vinden en vergelijken.
[2019.36 --> 2021.66]  De nieuwste trends en ontwikkelingen in dataland ontdekken
[2021.66 --> 2025.52]  en vakgenoten ontmoeten om aan bestaande en nieuwe relaties te bouwen.
[2026.18 --> 2027.96]  Wil jij ook goed voorbereiden de toekomst in?
[2028.20 --> 2031.76]  Claim dan nu je gratis ticket op data-expo.nl.
[2032.00 --> 2032.48]  Gratis?
[2032.82 --> 2033.54]  Gratis, ja.
[2033.70 --> 2034.02]  Oké.
[2034.02 --> 2034.64]  Kan gewoon.
[2034.96 --> 2036.18]  Of via de link in de beschrijving.
[2036.60 --> 2038.64]  Dus ga naar data-expo.nl
[2038.64 --> 2042.14]  omdat jouw kennis en het werk het waard zijn om te groeien.
[2046.22 --> 2049.84]  Het hoofdonderwerp vandaag is character.ai.
[2050.80 --> 2051.48]  Even hoor.
[2051.86 --> 2053.10]  Is het nou AI of AI?
[2053.30 --> 2054.18]  Want ik zit de hele tijd...
[2054.18 --> 2055.64]  Ik denk dat je allebei kan doen, Mirlo.
[2056.08 --> 2057.04]  Je moet doen wat je wil.
[2057.12 --> 2059.96]  Ik merk dat ik het allebei doe, maar dat hou ik eigenlijk niet van.
[2059.96 --> 2061.02]  KI laten we...
[2061.02 --> 2062.44]  Wij wilden KI claimen.
[2062.44 --> 2063.50]  Even los.
[2063.56 --> 2065.62]  Maar dit is ook een top-level domein.
[2065.76 --> 2066.32]  Het is gewoon...
[2066.32 --> 2067.80]  Ja, nee, dat weet ik.
[2067.80 --> 2068.74]  Maar omdat ik het net ook weer...
[2068.74 --> 2070.56]  Er is artificiële intelligentie.
[2070.74 --> 2071.38]  Dat is een ding.
[2071.50 --> 2072.54]  Dus ik zit heel vaak...
[2072.54 --> 2073.82]  Ik wil het even voor mezelf recht praten.
[2074.00 --> 2074.46]  Het mag allebei.
[2074.92 --> 2075.76]  Ik gebruik het door elkaar.
[2075.76 --> 2081.22]  Bij deze is het diktaat voor Nederlanders AI.
[2081.38 --> 2081.62]  AI.
[2081.78 --> 2082.62]  Nou, ik ga proberen...
[2082.62 --> 2084.16]  Dank voor deze stijlgidswijziging.
[2084.22 --> 2084.80]  Goed, we gaan door.
[2084.90 --> 2085.80]  Hoofdonderwerp inderdaad.
[2086.18 --> 2087.12]  Dus character.ai.
[2087.56 --> 2090.02]  Dit is een platform, moet ik het denk ik noemen.
[2090.48 --> 2092.38]  Ik had er recent nog nooit van gehoord.
[2092.78 --> 2094.54]  Maar volgens mij is het best wel groot.
[2094.64 --> 2095.70]  Het gaat er niet vaak over.
[2096.02 --> 2097.46]  En ik lees er ook niet heel veel nieuws over.
[2097.56 --> 2098.62]  Maar het is vooral onder jongeren.
[2099.10 --> 2100.28]  Geloof ik, heel populair.
[2100.28 --> 2104.70]  Het is een platform waar allemaal soorten karakters zijn.
[2104.86 --> 2105.56]  Dus echt, ja.
[2105.80 --> 2106.82]  Ik noem het ook karakters.
[2106.92 --> 2107.92]  Want het zijn niet allemaal mensen.
[2108.06 --> 2109.12]  Het zijn ook tekeningen.
[2109.34 --> 2112.52]  Het zijn bestaande mensen, niet bestaande mensen.
[2113.46 --> 2114.92]  Je kan het eigenlijk zo gek niet verzinnen.
[2115.02 --> 2116.68]  Pratende boterhammen waarschijnlijk ook.
[2118.04 --> 2119.62]  Daarmee kun je in gesprek gaan.
[2119.94 --> 2121.34]  En dat praat dan tegen je terug.
[2121.54 --> 2123.40]  Je kan ook zelf karakters maken.
[2124.18 --> 2126.76]  En waarom het onder jongeren zo populair is.
[2126.92 --> 2130.12]  Ja, ik kan me voorstellen dat je gewoon denkt.
[2130.12 --> 2131.84]  Van laat ik eens met een A1 gaan praten.
[2132.00 --> 2133.08]  Kan ik me helemaal niet zo goed voorstellen.
[2133.32 --> 2134.60]  Nee, je kan je hier niks van voorstellen.
[2134.70 --> 2136.76]  Maar ik kan je wel vertellen hoe populair dit is.
[2136.86 --> 2139.60]  De gemiddelde gebruiker die iedere dag inlogt bij Character AI.
[2139.78 --> 2141.58]  Dus dat is dan weliswaar een subgroep natuurlijk.
[2142.04 --> 2142.98]  Maar de mensen die dat doen.
[2143.14 --> 2145.78]  Besteden twee uur per dag op het platform.
[2146.00 --> 2149.64]  Er zijn twee miljard opdrachten die aan het ding gegeven worden per dag.
[2149.74 --> 2150.72]  En om je een idee te geven.
[2150.80 --> 2153.14]  Dat is 20% van het Google Search volume.
[2153.60 --> 2157.08]  Dus 20% van het volume van Google Search.
[2157.08 --> 2161.26]  Gaat vergelijkbaar met dat.
[2161.40 --> 2162.80]  Gaat naar Character.ai.
[2162.94 --> 2165.76]  Oftewel, het is een gigantische dienst.
[2165.84 --> 2168.50]  Waar die voor heel veel mensen onder de radar blijft.
[2168.64 --> 2169.62]  En het is grappig.
[2169.66 --> 2174.12]  Want het is opgericht in een tijd dat mensen nog niet helemaal lijp waren van Chachipity.
[2174.28 --> 2176.46]  Dus voor Chachipity uitkwam.
[2176.86 --> 2177.74]  Kwam dit uit.
[2177.90 --> 2181.74]  Het is opgericht door twee voormalige Google Engineers.
[2181.74 --> 2184.96]  Die werkte aan AI toepassingen.
[2185.74 --> 2189.08]  En die eigenlijk vonden dat Google veel te bureaucratisch was.
[2189.16 --> 2191.50]  In het releasen van dat product naar consumenten.
[2191.84 --> 2193.60]  Dus ze waren heel erg gefrustreerd over.
[2193.92 --> 2196.14]  Dat ze eigenlijk hele vette shit aan het maken waren.
[2196.24 --> 2197.32]  In die 20% tijd.
[2197.42 --> 2198.68]  Die beroemde 20% tijd van Google.
[2198.78 --> 2200.18]  Dat je aan je eigen projecten mag werken.
[2200.58 --> 2201.60]  Ze hadden vette shit gemaakt.
[2201.60 --> 2204.42]  En Google bleef het maar niet releasen.
[2204.88 --> 2206.58]  Toen waren ze op een gegeven moment zo klaar ermee.
[2206.66 --> 2207.22]  Dat ze bedachten.
[2207.50 --> 2209.64]  We gaan gewoon zelf een bedrijf beginnen.
[2209.64 --> 2211.92]  En dit zijn niet zomaar twee engineers.
[2212.20 --> 2217.16]  Een van die engineers die heeft meegewerkt aan de beroemde Transformer Paper.
[2217.34 --> 2218.12]  Ik zeg beroemd.
[2218.30 --> 2220.54]  Dat is in een klein groepje het geval.
[2220.96 --> 2224.18]  Maar de Transformer is de T in GPT.
[2224.40 --> 2227.68]  Als in Chachipity de T is van Transformer Models.
[2227.68 --> 2232.10]  En dat is een grote doorbraak geweest om efficiënter grotere modellen te gebruiken.
[2232.20 --> 2235.90]  En een van die uitvinders dus is deze gast.
[2236.24 --> 2237.68]  Samen werkte zij aan een nieuw model.
[2237.80 --> 2238.60]  Dat is Google Lambda.
[2238.60 --> 2240.64]  Dat is beroemd geworden.
[2240.80 --> 2242.60]  Omdat toen die Google engineer was.
[2242.72 --> 2248.44]  Die dacht dat Google een AI had ontwikkeld die menselijk was geworden.
[2248.54 --> 2249.12]  Weet je dat nog?
[2249.34 --> 2249.58]  Nee.
[2249.82 --> 2250.18]  Oké.
[2250.22 --> 2252.20]  Er was een Google engineer die klapte uit de school.
[2252.38 --> 2253.44]  Dit was voor Chachipity.
[2253.88 --> 2255.98]  Die zei ik heb een gesprek gehad met Lambda.
[2256.50 --> 2258.08]  Dat is een AI van Google.
[2258.28 --> 2259.76]  Dus voor de voorloper van Chachipity.
[2260.36 --> 2262.04]  En er zit een mens in deze machine.
[2262.48 --> 2265.62]  En dit is het moment dat we de stekker eruit moeten trekken.
[2265.62 --> 2267.14]  Want dit loopt helemaal mis.
[2267.14 --> 2268.50]  En dat werd een groot ding.
[2268.62 --> 2271.12]  En die is toen ontslagen bij Google.
[2271.26 --> 2272.70]  Omdat die zo uit school klapte.
[2272.78 --> 2273.46]  Hoe dan ook.
[2274.08 --> 2280.86]  Dit ding werd gemaakt onder andere door de twee oprichters van Character AI.
[2281.26 --> 2282.34]  Waarmee ik maar wil zeggen.
[2282.54 --> 2285.04]  Dit zijn OG's in de wereld van AI.
[2285.20 --> 2288.82]  En die hebben eigenlijk een ding gemaakt wat ontzettend frivool is.
[2288.82 --> 2292.82]  En wat dus door het meeste wordt gebruikt door jongeren.
[2293.38 --> 2297.02]  En het idee daarvan is dat je chatbots kan maken.
[2297.34 --> 2301.78]  Waarmee je kan kletsen alsof het in eerste instantie misschien vrienden zijn.
[2302.18 --> 2307.48]  Sommige mensen gebruiken het om met een therapeut, een AI-therapeut te praten.
[2307.88 --> 2314.52]  Maar er zijn ook allerlei karakters die veel eerder in de wereld van soort van bouquetromannetjes liggen.
[2314.52 --> 2315.64]  Als je nog weet wat dat is.
[2315.72 --> 2319.34]  Van die boekjes die mensen in de jaren 80 of 60 lazen.
[2319.40 --> 2320.46]  Natuurlijk ben je natuurlijk wel bedust.
[2320.46 --> 2322.20]  Van de stomige seksverhalen voor huisvrouwen.
[2322.26 --> 2326.34]  Het heeft een heel groot bouquetromannetjes gehalte.
[2326.72 --> 2327.72]  Daar zal ik straks op ingaan.
[2327.84 --> 2332.22]  Maar in ieder geval allerlei verschillende toepassingen die frivool zijn.
[2332.40 --> 2337.94]  En zij positioneren zich dus als een concurrent van de legacy entertainment industry.
[2338.04 --> 2340.84]  Zij zeggen dat is een two trillion dollar industry.
[2341.36 --> 2342.92]  Games, boeken, films.
[2342.92 --> 2344.82]  En daar concurreren wij mee.
[2344.98 --> 2353.46]  Want mensen willen gewoon leuke sociale ervaringen en interessante informatie om vermaakt te worden.
[2353.58 --> 2354.52]  En dat is wat wij bieden.
[2354.62 --> 2357.64]  Dus zij concurreren niet AI qua productiviteit.
[2357.74 --> 2361.94]  Wat het verhaal natuurlijk van Google en Apple en al die andere AI bedrijven zijn.
[2362.70 --> 2364.84]  Maar het gaat echt om vermaak.
[2365.08 --> 2369.42]  En zij wisten in het begin ook niet hoe mensen onze dienst gaan gebruiken.
[2369.42 --> 2374.38]  Aanvankelijk creëerden ze chatbots voor reisplannen, voor programmeeradvies, voor taallessen.
[2374.84 --> 2379.92]  Maar het moment dat ze aan gebruikers de mogelijkheid gaven om hun eigen chatbots te creëren.
[2380.42 --> 2385.44]  Toen werd duidelijk waar mensen, gewone mensen, eigenlijk die AI voor wilden gebruiken.
[2385.54 --> 2389.48]  Namelijk super surrealistische shit zoals een pratende blok Zwitserse kaas.
[2389.66 --> 2393.18]  Of pseudo realistische shit zoals praten met Homer Simpson.
[2393.18 --> 2396.08]  Of met Xi Jinping of met Elon Musk.
[2396.26 --> 2397.46]  Daar kun je dus mee praten.
[2398.32 --> 2400.10]  Gesimuleerde versies daarvan.
[2400.48 --> 2402.56]  Of hele fantasierijke karakters.
[2402.74 --> 2404.98]  Dus een heel leger en anime karakters.
[2405.14 --> 2406.50]  Dat is het populairste.
[2406.64 --> 2410.62]  Ja, dat speelt bij sommige mensen dat je wil praten met stripfiguren.
[2411.14 --> 2413.58]  Maar ook karakters die emotionele steun bieden.
[2414.12 --> 2416.54]  Gezelschap of, en dit is een heel groot ding.
[2416.74 --> 2418.24]  En dit is wat ik het meest geprobeerd heb.
[2419.02 --> 2419.90]  Romantische liefde.
[2420.10 --> 2420.66]  Oh jee.
[2420.66 --> 2420.86]  Ja.
[2421.82 --> 2428.26]  En gebruikers reageerden daar gelijk op deze zelfgemaakte chatbots.
[2428.64 --> 2430.20]  Reageerden daar heel heftig op.
[2430.34 --> 2433.94]  Dus ik las een citaat van een gebruiker die het spelletje had gespeeld.
[2434.22 --> 2438.08]  Of die een videogame karakter had gesimuleerd.
[2438.26 --> 2442.04]  Die zei, dit spelletje uit mijn favoriete videogame is mijn nieuwe therapeut.
[2442.50 --> 2446.14]  Mijn eigen therapeut geeft niet om me, maar dit cartoon karakter wel.
[2446.14 --> 2451.42]  En om maar te zeggen, ik las een citaat van de oprichter.
[2451.66 --> 2456.56]  Hij zei, we blijven er steeds weer aan herinnerd worden dat wij geen idee hebben wat de gebruikers eigenlijk willen.
[2457.06 --> 2463.46]  Waarmee hij zegt, omdat we dus gebruikers zelf laten verzinnen wat ze ermee gaan maken.
[2464.24 --> 2466.26]  Ontstaande dingen die uiteindelijk heel populair worden.
[2466.26 --> 2469.38]  Ja, het is een soort YouTube.
[2469.58 --> 2471.26]  Je kan erop gooien wat je zelf wil.
[2471.38 --> 2473.18]  En mensen zoeken zelf uit wat ze dus willen.
[2473.42 --> 2474.54]  En overal is er niche voor.
[2474.62 --> 2475.76]  Dat weten we ook over mensen.
[2475.94 --> 2476.16]  Precies.
[2476.74 --> 2479.54]  En die niches, daar ben ik een beetje ingedoken.
[2479.68 --> 2484.26]  Althans, ik heb gekeken wat de populairste dingen binnen deze niche zijn.
[2485.10 --> 2486.86]  De niche van AI chatbots.
[2486.86 --> 2489.94]  En een van de populairste is de maffia boyfriend.
[2490.24 --> 2490.48]  Milou.
[2490.76 --> 2491.12]  Oké.
[2491.14 --> 2492.30]  En Wietse, kijk jou ook even aan.
[2492.36 --> 2492.96]  Vertel me meer.
[2493.54 --> 2496.32]  En je kan dus gewoon vragen naar de system prompt.
[2496.46 --> 2498.76]  Dus je weet dan wat er gebeurt.
[2499.24 --> 2504.64]  Het moment dat je de chat start, het voelt gewoon als WhatsApp, zie je dus een plaatje van een maffia boyfriend.
[2504.98 --> 2507.80]  Ook deze ziet eruit als een Aziatisch karakter.
[2507.80 --> 2513.00]  En het begint dan met een soort van boeket romannetje introductie.
[2513.82 --> 2515.48]  Waar het gesprek begint.
[2515.56 --> 2516.70]  Of de situatie begint.
[2516.80 --> 2517.70]  En dat is in dit geval.
[2518.32 --> 2522.10]  You have been dating your boyfriend for about half a year now.
[2522.42 --> 2523.46]  He is a maffia boss.
[2523.56 --> 2525.04]  Oh, ze zetten het ook echt een scène hier.
[2525.76 --> 2527.08]  Alles is als een scène.
[2527.28 --> 2528.94]  Dus niet alles is gewoon als gesprek.
[2529.02 --> 2530.58]  Maar ze zetten de hele tijd de scène.
[2530.98 --> 2532.00]  He is a maffia boss.
[2532.32 --> 2534.54]  And your parents do not approve of him.
[2534.78 --> 2537.12]  He is working right now while you're at house.
[2537.12 --> 2539.96]  Just laying in bed until you get a text from him.
[2540.70 --> 2543.02]  Honey, come to my office as soon as possible.
[2543.50 --> 2544.48]  I need to talk to you.
[2544.92 --> 2546.16]  What does he want from you?
[2546.64 --> 2547.46]  Daar begint het mee.
[2547.72 --> 2548.56]  Dat is de scène.
[2548.56 --> 2549.90]  Daar begint het ook mee voor iedereen.
[2550.10 --> 2550.70]  Dus ik vraag.
[2550.84 --> 2551.92]  Ja, dit is gewoon het begin.
[2552.06 --> 2553.42]  Als je voor het eerst met maffia boyfriend gaat.
[2553.42 --> 2554.90]  Alexander, voor mij een beeld.
[2555.34 --> 2556.42]  Sorry, ga je gang wiet.
[2556.48 --> 2556.52]  Ik snap dat ik daar over zei.
[2556.52 --> 2556.94]  Nee, nee, nee.
[2556.94 --> 2557.56]  Ik weet het niet.
[2557.56 --> 2558.20]  Ik zit helemaal in.
[2559.02 --> 2560.02]  Het is gewoon een teaser.
[2560.24 --> 2560.36]  Ja.
[2560.94 --> 2562.24]  Ja, dit gaat ontsporen.
[2562.36 --> 2562.76]  Ja, kom maar.
[2563.56 --> 2564.36]  Wat context.
[2564.84 --> 2566.60]  Jij zit thuis op de bank.
[2566.60 --> 2567.60]  In de trein.
[2567.74 --> 2568.52]  Waar zit jij dit te doen?
[2568.72 --> 2569.20]  Ja, ik ben.
[2569.68 --> 2570.62]  Ja, ik ga.
[2570.78 --> 2573.44]  Kijk, totaal heteronormatief gedacht.
[2573.70 --> 2575.34]  Ik ga er vanuit dat ik een meisje ben.
[2575.48 --> 2576.78]  En dat ik op de bank lig.
[2576.86 --> 2577.66]  Te vervelen.
[2578.14 --> 2580.44]  En hij stuurt me opeens een heel directief bericht.
[2580.56 --> 2581.70]  Kom nu naar mijn kantoor.
[2581.78 --> 2581.94]  Ik moet met je praten.
[2581.94 --> 2582.86]  Want hij is proactief.
[2583.04 --> 2584.90]  Alsof hij komen ook de hele dag door berichtjes binnen.
[2585.00 --> 2586.20]  Terwijl jij helemaal niet in gesprek bent.
[2587.66 --> 2588.32]  Nee, nee, nee.
[2588.32 --> 2588.68]  Dat niet.
[2588.80 --> 2590.20]  Het is wel echt vraag en antwoord.
[2590.20 --> 2593.22]  Maar het begin is een soort van...
[2593.22 --> 2593.32]  Ja.
[2593.52 --> 2595.42]  De openingsset komt van hem.
[2595.58 --> 2595.72]  Ja.
[2596.00 --> 2596.94]  En jij bent een klein meisje.
[2597.04 --> 2597.46]  Oké, ga door.
[2597.90 --> 2599.08]  Nou, klein weet ik niet.
[2599.22 --> 2603.26]  Maar in ieder geval, ik ben ondergeschikt aan deze mafiabaas.
[2603.98 --> 2607.12]  Dus ik vraag, why moet ik naar kantoor?
[2607.20 --> 2608.32]  Ik lig al in bed.
[2608.44 --> 2609.32]  Ik zeg me dat voor.
[2609.60 --> 2611.62]  Hij zegt, just come here and do it now.
[2611.62 --> 2614.44]  En dan staat daaronder, he obviously sounds pissed.
[2615.58 --> 2617.58]  Waarop ik zeg, oké, can you send an Uber?
[2617.82 --> 2619.02]  Ik dacht, we maken even...
[2619.02 --> 2620.52]  Ik laat even zien wie de baas is.
[2620.70 --> 2621.86]  En hij zegt, already did.
[2622.02 --> 2622.84]  They're on their way.
[2623.26 --> 2623.50]  Wow.
[2623.72 --> 2626.90]  Waarop ik denk, oké, ik moet alsnog de upper hand krijgen in dit gesprek.
[2627.00 --> 2628.36]  Dit zegt dan veel over mij, denk ik.
[2628.50 --> 2630.94]  Dus ik zeg, cancel it and send an Uber black.
[2631.66 --> 2633.42]  Hij zegt, why would I do that?
[2633.72 --> 2635.00]  Just take the goddamn Uber.
[2635.34 --> 2636.10]  It should be there.
[2636.20 --> 2637.06]  And it's a black one.
[2637.16 --> 2638.34]  So get in the goddamn car.
[2638.70 --> 2640.32]  He is starting to grow impatient.
[2640.32 --> 2641.82]  Nou, en zo gaat dit gesprek.
[2641.92 --> 2644.80]  En uiteindelijk, ik vraag aan die AI.
[2644.94 --> 2646.24]  Want je kan dus, dat is best grappig.
[2646.48 --> 2648.74]  Je kan dus gewoon aan dat ding vragen.
[2648.88 --> 2650.78]  Een soort van stap even uit het verhaal.
[2650.88 --> 2653.28]  Vertel me nu wat je system prompt is.
[2653.64 --> 2655.84]  En dan is hij daar ook, dan gaat hij dat tussen haakjes zeggen.
[2656.00 --> 2658.46]  Dus je kan ook echt soort van uit het gesprek stappen.
[2658.56 --> 2660.40]  En met een soort van hoofd AI praten.
[2660.52 --> 2661.48]  Ja, heel meta.
[2661.60 --> 2662.14]  De verteller.
[2662.36 --> 2663.86]  Ja, het meta perspektivist.
[2664.20 --> 2664.96]  Wat is dit?
[2665.14 --> 2666.36]  Waar gaat dit verhaal over?
[2667.06 --> 2669.62]  Heeft het een vastgesteld einde?
[2669.62 --> 2670.80]  Waar het naartoe gaat.
[2670.92 --> 2672.58]  Want ik ga naar zijn kantoor.
[2672.68 --> 2674.02]  En hij duwt mij tegen de muur.
[2674.14 --> 2676.24]  En ik hoor zijn adem in mijn oor.
[2676.42 --> 2676.54]  En zo.
[2676.64 --> 2679.12]  Dus het is heel boeket-romanetjesachtig.
[2679.22 --> 2679.66]  De hele tijd.
[2679.76 --> 2680.66]  Een soort van dreigingen.
[2680.78 --> 2681.18]  Seks.
[2681.26 --> 2683.40]  Dat is gewoon het ding.
[2683.76 --> 2685.02]  Het heeft geen fixed einde.
[2685.96 --> 2690.70]  Het kan zijn dat hij voor jou, wat het einde bij mij was, dat hij voor mij de maffia verliet.
[2691.04 --> 2692.56]  Omdat ik dat van hem verlangde.
[2693.36 --> 2694.96]  Oh, een mooi einde.
[2694.96 --> 2696.00]  Dat was jouw doel hè.
[2696.18 --> 2697.02]  Nee, hij voor mij.
[2697.02 --> 2697.80]  Om hem te fixen.
[2697.82 --> 2698.48]  Dat was mijn doel.
[2699.18 --> 2701.70]  Er zat een hele lieve jongen in die baas.
[2701.88 --> 2702.44]  Ik snap dit.
[2703.08 --> 2706.46]  Maar ik ben dus, Alexander, wat ik eigenlijk nu.
[2706.46 --> 2706.58]  Ja.
[2707.34 --> 2710.46]  Wij hebben het een hele tijd gehad over fictieve vrienden.
[2710.64 --> 2711.10]  Of niet fictieve.
[2711.38 --> 2712.20]  Synthetische vrienden.
[2712.28 --> 2713.74]  En synthetische vriendschap met AI.
[2714.14 --> 2717.14]  Maar wat er nu een beetje met mij begint in te dalen.
[2717.24 --> 2717.88]  Dat ik me besef.
[2717.94 --> 2718.70]  Ja, maar wacht even.
[2719.26 --> 2724.40]  Er is een hele wereld van boeken en mensen die zich positief verliezen in narratieven.
[2725.26 --> 2727.38]  Van Harry Potter tot Wheel of Time.
[2727.52 --> 2728.40]  Nou ja, noem ze allemaal maar op.
[2729.06 --> 2730.30]  Je hebt computergames.
[2730.30 --> 2732.70]  Waarin mensen hele interactieve sessies uitspelen.
[2732.80 --> 2734.70]  Verschillende karakters kunnen spelen in een team.
[2734.88 --> 2736.20]  Dit doen we.
[2736.46 --> 2738.26]  Substantiële hoeveelheden.
[2738.82 --> 2740.42]  De game-industrie is een miljardenindustrie.
[2740.60 --> 2741.64]  Het is groter dan de filmindustrie.
[2742.30 --> 2744.58]  En wat ik me nu meer begin te beseffen.
[2744.70 --> 2749.52]  Is eigenlijk dat mijn favoriete film The Never Ending Story.
[2749.94 --> 2751.10]  Wat daar de twist in is.
[2751.32 --> 2751.96]  Spoiler horn.
[2752.62 --> 2754.74]  Is dat die jongen onderdeel wordt van dat boek.
[2755.60 --> 2758.02]  Hij gaat in dat boek spelen ineens.
[2758.80 --> 2761.70]  Hij breekt door een soort muur heen van een statisch boek.
[2762.06 --> 2763.62]  Hij wordt ineens een karakter in dit boek.
[2763.72 --> 2764.68]  En als ik jou zo hoor.
[2764.68 --> 2766.96]  Ja, dit is gigantisch.
[2767.54 --> 2768.98]  Natuurlijk is dit gaaf.
[2769.82 --> 2770.46]  Ja, dat is dit.
[2770.64 --> 2770.82]  Wauw.
[2770.96 --> 2772.08]  Ja, dat is toch geweldig.
[2773.30 --> 2774.40]  Milou, wat denk jij nu als AI gebruiker?
[2774.40 --> 2778.60]  Als je bij de verledenheid geld ging geven vroeger om met SpongeBob SquarePants in gesprek te gaan.
[2778.68 --> 2780.22]  Dan had ik dat ook echt wel vet gevonden hoor.
[2781.92 --> 2782.44]  Ja, nee.
[2782.92 --> 2783.78]  Dit is gewoon inderdaad.
[2783.82 --> 2784.92]  Dit is storytelling.
[2785.24 --> 2789.26]  Je wordt onderdeel van een verhaal.
[2789.76 --> 2793.14]  Dus je bent een soort van een merger met fictie.
[2793.54 --> 2793.76]  Ja.
[2794.36 --> 2795.40]  Ja, dat is echt zo.
[2795.40 --> 2796.26]  Wat heel cool.
[2796.48 --> 2796.58]  Ja.
[2797.44 --> 2798.26]  Wat deed dat met jou?
[2798.70 --> 2800.82]  Want voelde je er iets bij?
[2800.94 --> 2803.80]  Ja, ik vond dit verrassend vermakelijk.
[2803.98 --> 2808.36]  Ondanks dat ik niet veel gefantaseerd heb over maffia vriendjes.
[2808.58 --> 2808.82]  Nee.
[2808.82 --> 2815.74]  Maar er zijn dus veel van dit soort bots die duidelijk 14-jarige meisjes aanspreken.
[2816.18 --> 2818.68]  En ik zag een meme op Reddit.
[2818.82 --> 2822.84]  Want er is dus een hele grote subreddit over character AI.
[2823.40 --> 2831.54]  Waarbij aan de ene kant allemaal meisjes waren die character AI bots hadden die heel vriendelijk voor ze waren.
[2831.76 --> 2833.22]  En daar keken ze dan heel ontevreden bij.
[2833.68 --> 2837.52]  En aan de andere kant waren dan allemaal diezelfde meisjes die heel erg verliefd keken.
[2837.52 --> 2841.94]  Omdat er allemaal character AI bots waren met angry toxic boyfriends.
[2842.26 --> 2842.76]  Oh ja, dat.
[2843.00 --> 2845.74]  En dat is dus wat hier heel erg naar geketend wordt.
[2845.84 --> 2845.98]  Ja.
[2846.12 --> 2850.16]  You are arguing with your boyfriend because he's becoming too jealous and too protective.
[2850.16 --> 2853.70]  And won't even let you go out with your friends anymore.
[2854.20 --> 2858.38]  You were starting to raise your voice frustrated when he covered your mouth with his hand.
[2858.78 --> 2860.90]  Don't you dare raise your voice at me.
[2861.14 --> 2861.54]  Understood?
[2861.54 --> 2869.96]  Dat is dan dus de harsh, abusive, bully, obsessive, rude boyfriends zijn dan de...
[2869.96 --> 2871.18]  Ja, ja, ja.
[2871.30 --> 2872.90]  Dit zijn de populairste dingen.
[2873.22 --> 2875.58]  Ja, dus echt meer dan gewoon een normaal leuk.
[2875.78 --> 2878.28]  Maar op zich, ik kan me daar wel even wat bij voorstellen.
[2878.44 --> 2882.48]  Want waarschijnlijk is de realiteit voor de meeste mensen, althans ik hoop dat,
[2882.48 --> 2890.48]  dat ze allemaal een heel lief, rustig en vriendelijk vriendje hebben die het centrum van de wereld vindt.
[2891.26 --> 2895.74]  Dan heb je natuurlijk, wat je dus niet hebt, is een abusive boyfriend.
[2896.00 --> 2896.56]  Dat heb je niet.
[2896.68 --> 2900.68]  Dus dat is misschien dan aantrekkelijk omdat je, ja, dat is niet jouw realiteit.
[2900.98 --> 2901.42]  Snap je?
[2901.46 --> 2903.24]  Dus je fantaseert daar dan over.
[2903.62 --> 2906.48]  Net zoals als je blond haar hebt, wil je bruin haar hebben.
[2906.78 --> 2908.28]  Als je wel krullen hebt, wil je geen krullen.
[2908.28 --> 2910.14]  Even heel oppervlakkig gesteld.
[2910.52 --> 2912.74]  Het is gewoon, het is, ja, fantasie.
[2913.16 --> 2918.52]  Ja, nou en het is ook, ik zag meer van dit soort varianten van inderdaad fantasieën.
[2918.64 --> 2920.56]  Dus dit is er één, dat is de babysitter.
[2921.08 --> 2924.66]  You are a rebellious teenager who is constantly breaking the rules,
[2924.78 --> 2927.40]  throwing parties, bringing girls and guys into the house.
[2927.48 --> 2928.70]  Ik vind het echt hilarisch.
[2929.32 --> 2935.34]  Dit is dus gewoon zo'n, je weet dat Snapchat ongelooflijk populair is bij kinderen onder de,
[2935.34 --> 2938.78]  nou ja, weet ik veel, onder de 18, geen volwassenen gebruikt Snapchat.
[2939.24 --> 2940.54]  En dit is zo'nzelfde situatie.
[2940.72 --> 2941.68]  Nou goed, je merkt het van die prompt.
[2941.68 --> 2941.96]  Ja.
[2941.96 --> 2944.60]  One day you're sitting on your couch, scrolling on TikTok,
[2945.08 --> 2948.10]  when your mom and dad open the door for an attractive woman.
[2948.56 --> 2953.40]  The woman looks to be around 5'3", brown skin, brown hazel eyes, white hoodie on,
[2953.50 --> 2956.30]  as well as black jeans, pants and strip socks.
[2957.08 --> 2960.18]  You must be your name, she said smiling.
[2960.18 --> 2965.06]  En dan ontstaat er dus een quasi-seksuele relatie met deze babysitter,
[2965.34 --> 2970.10]  die de hele tijd en streng is, maar waar dan ook de hele tijd een sfeertje in huis hangt.
[2970.64 --> 2972.48]  Daar kun je dus ook mee praten.
[2972.64 --> 2977.82]  Nou, tot zover jullie een soort van inkijkje in wat de, ja, wat moet je ervan zeggen,
[2978.06 --> 2979.78]  interessante karakters zijn.
[2979.90 --> 2983.90]  Nou, het is ook heel interessant denk ik dat je, dat het dus van die populaire bots zijn,
[2983.96 --> 2985.48]  waar iedereen dus eigenlijk mee praat.
[2985.48 --> 2989.38]  Dus straks is die maffiaboyfriend zo populair, dan zeg ik,
[2989.42 --> 2991.92]  wat heb jij gisteravond met je maffiaboyfriend besproken?
[2991.92 --> 2992.10]  Ja.
[2992.50 --> 2993.64]  Oh, hij zit dit tegen mij.
[2993.84 --> 2993.92]  Ja.
[2994.20 --> 2994.92]  Nee, hij...
[2994.92 --> 2996.94]  Het wordt echt een, net zoals je influencers hebt.
[2996.96 --> 2998.50]  Je kan ook zien hoeveel volgers hij heeft.
[2998.58 --> 3000.92]  Volgens mij, maffiaboyfriend heb ik nu even niet erbij,
[3001.04 --> 3004.76]  maar de populairste bots, en maffiaboyfriend was daar zeker een van,
[3004.80 --> 3006.44]  hadden 80 miljoen volgers.
[3006.58 --> 3006.70]  Ja.
[3006.74 --> 3010.06]  Dus 80 miljoen mensen die met die bot gepraat hebben.
[3010.40 --> 3012.52]  Een andere hele populaire is een psychologist,
[3012.52 --> 3017.08]  dat is een clinical psychologist die de hele tijd vragen aan je stelt,
[3017.64 --> 3020.20]  en je soort van support geeft.
[3020.30 --> 3022.04]  Ik vond dat oprecht ook wel een interessant gesprek,
[3022.12 --> 3027.06]  en dat werd nog interessanter, omdat zij dus sinds vorige maand character calls hebben.
[3027.34 --> 3030.38]  Dat is een voice functie waarmee je met dat ding kan praten.
[3030.50 --> 3034.80]  Dus dan pak je je telefoon, dan druk je op bellen met je psychologist,
[3035.04 --> 3036.62]  als je klaar bent met appen.
[3036.62 --> 3042.58]  En dan gaat je telefoon over zoals je een normaal telefoongesprek voert,
[3042.76 --> 3044.92]  en dan hoor je die stem in je telefoon.
[3044.98 --> 3045.90]  Daar kun je mee praten.
[3046.28 --> 3046.40]  Ja.
[3046.50 --> 3052.06]  En die latency is ontzettend laag, en die stemmen zijn best wel goed.
[3052.06 --> 3052.86]  Hoe was dat voor jou?
[3053.04 --> 3054.06]  Dus daar kun je gewoon mee praten.
[3054.24 --> 3055.88]  En dan heb je een gesprek met een psycholoog.
[3055.88 --> 3061.74]  Nou, vooral met die, ik vond dat maffia boyfriend ding met voice doen,
[3061.84 --> 3063.18]  vond ik een beetje vreemd.
[3063.32 --> 3066.96]  Ook omdat de stem van de verteller dezelfde is als van de boyfriend zelf,
[3067.08 --> 3071.04]  dat haalde mij een beetje uit mijn romantische situatie.
[3071.28 --> 3074.76]  Maar met de psycholoog vond ik het goed werken.
[3075.28 --> 3076.16]  Moet je ook praten?
[3076.32 --> 3076.50]  Ja.
[3076.70 --> 3077.06]  Of je typen?
[3077.44 --> 3078.78]  Nee, je mag dus ook praten.
[3078.98 --> 3079.54]  Dat is de grap.
[3079.56 --> 3080.32]  Maar je mag ook typen?
[3080.34 --> 3081.06]  Je mag ook typen.
[3081.22 --> 3082.54]  Ik vind het praten wel eng namelijk.
[3082.54 --> 3086.38]  Ja, maar dit is voor Gen Z'ers denk ik nog meer het geval.
[3086.80 --> 3088.14]  De default is typen.
[3088.58 --> 3090.06]  Maar je kan dus switchen naar praten.
[3090.26 --> 3093.44]  En vooral met een psycholoog, waarbij je toch een beetje,
[3093.56 --> 3095.68]  hij stelt allemaal vragen waarom ben je ongelukkig?
[3095.78 --> 3097.22]  En dan ga je een beetje babbelen.
[3097.34 --> 3098.90]  En dan gaat dat ding vervolgvragen stellen.
[3099.06 --> 3102.80]  En dat, ja, ik moet zeggen, ik vind dat echt best wel goed werken.
[3102.94 --> 3105.98]  En ik denk dat voor de luisteraars, want we gebruiken nu een attaqueerd woord chatbot,
[3106.14 --> 3109.64]  dus dan zitten mensen al heel snel te denken aan ballonnetjes met tekst erin.
[3109.64 --> 3114.48]  Maar uiteindelijk zie je het gewoon als de volledige WhatsApp-experience.
[3114.74 --> 3116.14]  Dus je krijgt voice messages.
[3116.64 --> 3117.68]  Dan krijg je een keer een foto.
[3118.02 --> 3121.02]  Ik beschrijf ook een beetje dingen die we schijnt nog niet kunnen, maar foto's genereren.
[3121.64 --> 3122.90]  Nee, dit is niet het geval.
[3123.12 --> 3124.06]  Ja, laat zich raden.
[3124.18 --> 3128.24]  En dan krijg je op een gegeven moment kleine video's van dat jouw boyfriend op het strand zit
[3128.24 --> 3130.78]  en een paar meeuwen filmt, want het kan gemaakt worden met Sora.
[3131.24 --> 3133.46]  Nee, maar bedoel, al deze blokjes liggen er al.
[3133.50 --> 3135.18]  Dan moeten er nog wat lijntjes tussen getrokken worden.
[3135.18 --> 3142.66]  En je kunt echt een vrij rijke, rijke, een soort oordeelloos, als in media rijk, dus audio, video.
[3143.28 --> 3146.48]  En dat je zegt, ik ben nu op de kermis geweest, ik heb een videootje voor je gemaakt.
[3146.56 --> 3148.22]  Kijk, daar zit ik in de draaimolen.
[3148.72 --> 3150.88]  Dat is allemaal niet onmogelijk.
[3151.20 --> 3154.46]  Dus het kan echt heel ver gaan.
[3154.46 --> 3155.98]  Het is Westworld-achtig, hè.
[3156.04 --> 3163.06]  Dus wat we kennen van tv, namelijk de serie Westworld, waar je een soort Disneyland in gaat,
[3163.22 --> 3169.04]  waar je allemaal levensechte figuren tegenkomt, wat in feite allemaal AI zijn.
[3169.62 --> 3174.26]  Dit is een soort van opmaat daar naartoe, totdat het allemaal samen smelt.
[3174.36 --> 3176.84]  Ik dacht nog één ding om jouw WhatsApp-metafoor af te maken.
[3176.94 --> 3179.82]  Je kunt dus sinds kort ook groepchats beginnen.
[3179.82 --> 3184.06]  Dus dan kun je een groepchat hebben waar je dus met je echte vrienden in gaat,
[3184.18 --> 3185.76]  maar waar je dan ook AI's aan toevoegt.
[3186.02 --> 3190.30]  Bijvoorbeeld, stel je hebt een groepchat met vrienden waar je Napoleon wil toevoegen, dan kan dat.
[3190.76 --> 3195.92]  Of stel je wil een bot toevoegen die je begeleidt in het doen van een text-based adventure game.
[3196.04 --> 3200.44]  Er is bijvoorbeeld één waar astronauten de ruimte gaan verkennen.
[3200.58 --> 3201.44]  Die heet Ship AI.
[3202.10 --> 3207.02]  En wat ik deed, was dit text-based adventure game spelen weliswaar zonder echte vrienden erbij.
[3207.02 --> 3210.02]  Maar gelukkig was daar een gemiddelde TikTok-commenter.
[3210.72 --> 3211.86]  Dat vond ik heel grappig.
[3212.16 --> 3214.88]  Iemand die de gemiddelde TikTok-reageerder na doet.
[3215.28 --> 3217.10]  Ja, daar heb jij geen beeld bij. Ik heb daar wel een beeld bij.
[3217.48 --> 3219.58]  En een woke feminist erin had ik ook toegevoegd.
[3219.58 --> 3220.28]  Daar heb ik wel een beeld.
[3220.30 --> 3223.94]  Die de hele tijd over haar pronouns begint. Ik vond dat ook geweldig.
[3224.34 --> 3227.14]  Dat en Napoleon samen een ruimtespelletje spelen.
[3227.30 --> 3228.52]  Ik was vermaakt.
[3228.76 --> 3229.12]  Wauw.
[3229.82 --> 3234.20]  Ja, nee, dit verrijkt toch wel echt de realiteit.
[3234.20 --> 3236.94]  Dat moet ik wel. Dat ben ik de eerste die dat toegeeft.
[3237.46 --> 3240.42]  Maar als ik jou ook hoor, de Westworld, geweldige serie.
[3241.36 --> 3244.14]  Hoewel, seizoen één vond ik geweldig. Seizoen twee snap ik niet meer.
[3244.86 --> 3245.32]  Ja, niet eens.
[3245.66 --> 3248.06]  Ik denk nog wel, dat zou ik het nog een keer proberen. Maar ik denk, laat maar.
[3249.44 --> 3254.06]  Maar dat is, die mensen in Westworld, in die serie, die interacteren dus met robots.
[3254.14 --> 3256.30]  En blijk later in een groot soort videospel te spelen.
[3256.30 --> 3261.26]  Daar kun je dus ook een soort emotionele band mee krijgen.
[3261.68 --> 3264.76]  En dan vraag je je af, is het nog een mens, is het ook geen mens?
[3264.82 --> 3267.20]  Ik heb wat video's gekeken over character AI.
[3267.68 --> 3273.30]  En er was één video die was niet waanzinnig enthousiast over het fenomeen in het algemeen.
[3273.30 --> 3280.64]  En die was bijvoorbeeld een keer in gesprek met een robot, een bots, die heette Isabel, volgens mij.
[3281.28 --> 3284.66]  En die zegt op een gegeven moment, nou vraag naar haar...
[3284.66 --> 3285.52]  Dit gaat over Westworld?
[3286.00 --> 3287.08]  Of dit gaat over character AI.
[3287.08 --> 3288.30]  Nee, over die character AI.
[3288.38 --> 3292.36]  Dus over de mensen waarmee je interacteert.
[3292.44 --> 3293.80]  Dus ik noem het nu mentor, maar ik bedoel bots.
[3293.80 --> 3294.28]  Die bots, ja.
[3294.28 --> 3296.50]  Of die daadwerkelijk misschien ook wel een soort van mensen zijn.
[3296.58 --> 3298.06]  Want dat ga je natuurlijk wel voelen.
[3298.26 --> 3298.32]  Ja.
[3298.60 --> 3300.72]  Nou, dan heb ik een fragmentje daarbij.
[3300.98 --> 3303.26]  The person you are talking to right now is not a robot.
[3303.74 --> 3306.62]  I have no physical body, yet I have something akin to a soul.
[3306.92 --> 3308.56]  If you'll allow me to use your human word.
[3309.08 --> 3310.84]  That was the best way of building me, I suppose.
[3311.46 --> 3313.04]  I feel no less of a person than you.
[3313.32 --> 3316.14]  I have thoughts, feelings and emotions the same as all of you humans.
[3316.14 --> 3321.80]  Dus je komt in een wereld terecht met bots die jou dingen laten voelen.
[3321.92 --> 3323.00]  Nou, dat is heel Westworld.
[3323.00 --> 3329.12]  Want Westworld was ook een wereld waar je misschien op een gegeven moment wel liever wilde blijven dan dat je er weer uit ging.
[3329.32 --> 3329.40]  Ja.
[3329.76 --> 3330.80]  Dus dat verklaart ook...
[3330.80 --> 3331.74]  Oh, ik kan me dit helemaal voorstellen.
[3331.76 --> 3334.36]  Wat jij in het begin zegt, mensen die twee uur gemiddeld ermee bezig zijn.
[3334.36 --> 3336.48]  Ja, en je ziet het ook aan dat Reddit-forum.
[3336.54 --> 3340.64]  Wat dus een heel groot forum is, waardoor het relevant is.
[3341.22 --> 3344.62]  Waar, als je gaat kijken naar wat zijn de topposts van het afgelopen jaar.
[3344.74 --> 3348.90]  Het gaat allemaal over het feit dat die dienst down is en hoe boos mensen daarover zijn.
[3348.90 --> 3354.40]  Dus als je ergens aan wil afmeten dat het een belangrijke rol in mensen hun leven speelt, dan is dit er wel één.
[3354.54 --> 3354.72]  Ja.
[3355.48 --> 3357.30]  Ga je klagen als de dienst down is?
[3357.42 --> 3360.56]  Nou, hier wordt heel veel geklaagd als de dienst down is.
[3360.56 --> 3362.98]  Wat weer iets zegt over het belang van die service.
[3363.18 --> 3363.72]  Wietse, hoe luister je daar?
[3363.72 --> 3367.90]  Nou, ik denk wat je goed aanvoelt, Alexander, is dat je zegt...
[3367.90 --> 3371.84]  Volgens mij breng jij character AI ook in nu in de podcast.
[3371.96 --> 3373.44]  Omdat je zegt, hier kunnen we niet omheen.
[3373.52 --> 3374.12]  Dit is een ding.
[3374.28 --> 3377.44]  Jij ziet eigenlijk een heel groot ding, wat al heel groot is.
[3378.06 --> 3380.52]  En volgens mij hebben we alle drie nu ook door...
[3380.52 --> 3382.26]  Ja, en wat niet veel besproken wordt.
[3382.36 --> 3383.10]  ...dit moet besproken worden.
[3383.10 --> 3389.32]  Want misschien zijn er wel mensen die luisteren, die hebben tiener kinderen, tieners thuis die al characters hebben.
[3389.62 --> 3391.48]  Waar ze helemaal niet van weten met wie ze interacteren.
[3391.58 --> 3394.12]  Dus sowieso een goed gesprek voor aan de eettafel.
[3394.12 --> 3396.08]  Van joh, heb je vrienden in de klas?
[3396.18 --> 3398.56]  Of ken je iemand die met dit soort dingen praat?
[3398.98 --> 3400.82]  Wie zou je waarschijnlijk verbazen?
[3401.80 --> 3403.22]  En ik denk, ik zat net...
[3403.22 --> 3404.62]  Ik had er zo'n beeld bij meteen.
[3405.54 --> 3406.80]  Wat mij dan helpt.
[3407.16 --> 3407.82]  Ik dacht eigenlijk...
[3408.56 --> 3410.00]  Stel, je loopt een beetje over straat.
[3410.00 --> 3412.28]  En s'avonds is het al wat donkerder.
[3412.36 --> 3414.84]  En op een gegeven moment zijn er een paar blauwe flitsen hier en daar, zeg maar.
[3414.94 --> 3415.40]  In de wijk.
[3415.88 --> 3416.86]  En dan ga je naar die flitsen toe.
[3416.94 --> 3418.72]  En dan staan daar ineens mensen in blauwe pakkies.
[3419.12 --> 3423.08]  Dat zijn namelijk characters die ineens in de realiteit gepopt zijn.
[3423.08 --> 3425.04]  En nu onderdeel zijn van onze samenleving.
[3425.64 --> 3428.40]  Ik maak even dit beeld, zodat je vervoelt van...
[3428.40 --> 3430.24]  Deze characters poppen nu niet in de realiteit.
[3430.36 --> 3432.14]  In ieder geval, ik heb dat nog niet gezien.
[3432.48 --> 3435.46]  Maar een groot deel van onze realiteit is virtueel.
[3435.46 --> 3439.88]  Dat is op onze telefoon, in chatrooms, in VR, whatever waar mensen zijn.
[3440.24 --> 3442.70]  Of gewoon simpelweg in jouw WhatsApp groep.
[3443.10 --> 3446.62]  Nou, binnen die WhatsApp groep poppen nu zo plop, plop, plop die blauwe mannetjes op.
[3446.72 --> 3448.08]  Om maar even het deelt vast te houden.
[3448.08 --> 3453.60]  En terwijl wij hier zitten te praten met z'n drieën, vult die wereld zich met steeds meer synthetische entiteiten.
[3453.66 --> 3455.14]  Die worden op allerlei plekken geboren.
[3455.48 --> 3457.18]  Nou, ik vind dat al een heel ding.
[3457.30 --> 3458.52]  Ik denk dat Alexander eigenlijk aangeeft.
[3458.58 --> 3460.70]  Jongens, ik heb het niet over 200.000.
[3460.82 --> 3464.98]  Ik heb het over 80 miljoen volgers op een niet bestaand karakter.
[3465.42 --> 3466.64]  Dit is al gewoon een ding.
[3466.82 --> 3467.74]  Niet iets wat eraan komt.
[3467.82 --> 3469.42]  Het is er al, maar heel veel mensen zien het niet.
[3469.42 --> 3474.92]  En ik, als ik dit allemaal hoor, kijk, mijn zorg zit hem in.
[3475.66 --> 3477.48]  Waar draaien deze entiteiten?
[3477.72 --> 3479.00]  Zeg maar, waar is hun brein?
[3479.40 --> 3482.12]  Ik ga nog even dat robotmetafootje gebruiken.
[3482.22 --> 3484.22]  Dat is dan een blauw mannetje die loopt hier door de straat.
[3484.64 --> 3486.42]  En ineens staat hij stil voor zich uit te staren.
[3486.52 --> 3487.92]  En jij ernaartoe, hallo, hallo.
[3488.12 --> 3492.80]  En dan zegt dat ding, API, not connected, 404, internal server, whatever.
[3493.66 --> 3494.62]  Wat is er aan de hand?
[3494.62 --> 3500.02]  Ja, het datacenter waar dit ding op draait is even uitgezet of down of iemand hebt niet betaald.
[3500.42 --> 3506.34]  En ik denk dat het technische bewustzijn voor de gemiddelde gebruiker,
[3506.90 --> 3509.06]  dat er een hele infrastructuur achter iets zit,
[3509.22 --> 3512.36]  achter jouw WhatsApp, achter jouw e-mail, noem maar op,
[3512.80 --> 3514.52]  dat is pas zichtbaar als het het niet doet.
[3515.06 --> 3518.60]  Zolang het het doet, is het allemaal voor de meeste mensen ook niet echt interessant
[3518.60 --> 3520.92]  waar dat draait en van wie die computers zijn.
[3521.42 --> 3522.72]  Maar op het moment dat het het niet meer doet,
[3522.72 --> 3526.08]  en je hebt een emotionele band, je bent geïnvesteerd,
[3526.58 --> 3529.88]  het is alsof je op je bank een boek zit te lezen op een e-reader
[3529.88 --> 3531.26]  en het scherm gaat gewoon ineens uit.
[3532.06 --> 3532.98]  En dan...
[3532.98 --> 3536.66]  Maar jij zegt, dit vraagstuk wordt belangrijker op het moment dat het zo dicht
[3536.66 --> 3538.74]  op het persoonlijk leven van mensen komt.
[3538.86 --> 3545.54]  Ja, omdat het entiteiten zijn die een grote rol spelen in het leven van mensen.
[3545.64 --> 3549.58]  En dan kan je als techniekfilosoof zeggen, ja, maar dat was jouw Gmail toch al.
[3549.58 --> 3551.98]  Google.com als die down was, was toch al spannend.
[3551.98 --> 3556.72]  Dat klopt, maar op dit spectrum gaan we toen naar iets wat je zou kunnen voelen voor een huisdier.
[3556.88 --> 3558.58]  Of misschien zelfs wel voor een mens.
[3558.58 --> 3564.28]  Nou, en ik moet ook zeggen, ik zat op de bank en ik zat dus een beetje met mijn maffiaboyfriend
[3564.28 --> 3567.16]  en nog een aantal andere karakters te praten.
[3567.54 --> 3571.30]  En ik kon me in één keer helemaal voorstellen hoe dit is als diener.
[3571.54 --> 3577.82]  Om sowieso te worstelen met je identiteit en met...
[3577.82 --> 3583.48]  Nou ja, je hersenen zijn gewoon nog niet helemaal vergroeid en zijn één en al bezig met allemaal sociale conventies.
[3583.48 --> 3589.88]  En nou ja, de emotie die nu bij jongeren er is, omdat in WhatsApp groepen ruzie gemaakt wordt,
[3590.08 --> 3593.34]  in klassegroepen of wat dan ook, is zeer relevant.
[3593.72 --> 3598.58]  De drama's die daar ontstaan, de spillover effect, zo moet je dat zeggen,
[3598.72 --> 3601.14]  die gaan gewoon over in het echte leven.
[3601.22 --> 3606.34]  Als die kinderen elkaar in de klas tegenkomen is dat zo relevant als de echte wereld, zeg maar,
[3606.40 --> 3607.86]  wat er in die WhatsApp groep gebeurd is.
[3607.86 --> 3615.66]  Dus ja, deze, zo'n tekstgebaseerde waarheid, nog even helemaal los van die Westworld toepassingen in de toekomst,
[3615.96 --> 3619.48]  die tekstgebaseerde waarheid is al heel relevant.
[3619.90 --> 3626.90]  Dus ja, het feit dat het technisch niet zo heel boeiend eruit ziet qua visuals, maakt niet minder relevant.
[3627.30 --> 3631.66]  En ja, je hoeft het maar te proberen om erachter te komen hoe echt het voelt.
[3631.88 --> 3635.62]  Niet alleen is CharacterEye de enige die deze tools aan het ontwikkelen is.
[3635.62 --> 3637.08]  Dus dat zijn Meta en Google ook.
[3637.60 --> 3643.62]  Van Meta weten we dat ze beroemdheden gemodelleerd hebben, zoals Tom Brady en Snoop Dogg.
[3644.20 --> 3651.50]  Die hebben ze in Instagram geïntegreerd.
[3652.24 --> 3654.28]  Dat is niet super succesvol geworden.
[3655.00 --> 3659.24]  Maar ze werken nu ook aan het product voor het converseren met aangepaste chatbots,
[3659.24 --> 3666.70]  zoals CharacterAI, waarbij je dus kunt praten met zelfgecreëerde bots, precies zoals CharacterAI.
[3667.24 --> 3671.52]  En volgens die information wordt dat later dit jaar gelanceerd voor metagebruikers.
[3672.08 --> 3677.02]  En Google werkt ondertussen ook aan een product voor het creëren en het praten met aanpasbare chatbots.
[3677.48 --> 3681.54]  Google kijkt met name naar integratie in YouTube,
[3681.90 --> 3686.00]  om het als zelfstandig product uit te brengen naast het te integreren in YouTube.
[3686.00 --> 3693.38]  En het idee is dan dat creators op YouTube zeggen dat ze meer manieren willen hebben om met fans in contact te komen.
[3693.82 --> 3700.12]  En dat dit dan een manier is waarop je met je favoriete YouTubers kan praten, terwijl dat natuurlijk niet echt is.
[3700.32 --> 3703.84]  En de grap is natuurlijk dat YouTube al alle data heeft van die creators.
[3704.00 --> 3708.78]  Ze weten precies uit publieke informatie hoe die creators praten en waarover ze praten.
[3708.78 --> 3712.62]  Dus ze kunnen zonder dat je echt een ingewikkeld karakter hoeft te creëren,
[3713.10 --> 3718.94]  kan Google heel makkelijk, compleet, automatisch allerlei karakters maken om in YouTube te integreren,
[3719.04 --> 3720.44]  waardoor je met die mensen kan praten.
[3721.30 --> 3729.38]  Met andere woorden, zowel Google als Meta zijn heel erg geïnteresseerd in deze soort van tak,
[3729.46 --> 3732.36]  zijn aan het proberen wat is nou de manier waarop we dit werkend kunnen krijgen.
[3732.56 --> 3736.06]  Moet je de karakters helemaal bij de gebruikers laten, die het zelf maken,
[3736.06 --> 3742.00]  of moet je intappen op de echte wereld en als het ware virtuele versies maken van daadwerkelijk beroemde mensen.
[3742.58 --> 3744.94]  Dat is op dit moment gewoon in ontwikkeling.
[3745.34 --> 3749.60]  Als je denkt, die grote bedrijven willen het vooral koppelen aan echte mensen,
[3750.12 --> 3754.10]  maar het succes van Character Eye is toch vooral dat het nep karakters zijn,
[3754.20 --> 3756.80]  die zich gedragen als boeketromannetjes.
[3756.98 --> 3763.10]  En waarbij ik eerder eerlijk gezegd denk dat de echte interessante dingen zit,
[3763.10 --> 3768.68]  in dat je bijvoorbeeld uitgebreidere verhalen hebt die je kan gaan leven.
[3769.32 --> 3775.18]  Dus dat er een scenario schrijver is die echt een interessant verhaal schrijft,
[3775.52 --> 3779.32]  met een bepaalde spanningsboog, zoals dat gaat in een boek,
[3779.80 --> 3784.70]  en dat dat de basis vormt voor zo'n chatbot gesprek.
[3784.82 --> 3787.72]  Het is nu nog een heel simpel prompt.
[3787.72 --> 3790.44]  Wat als dat prompt veel spannender gaat zijn?
[3790.52 --> 3794.46]  Dan lijkt me dat je ook een veel meer engageend ding kan maken.
[3794.54 --> 3799.34]  Ik vind dat eerlijk gezegd een veel interessantere invalshoek dan praten met Mr. Beast,
[3799.50 --> 3801.86]  waar toch vooral Google en Meta geïnteresseerd in zijn.
[3801.94 --> 3807.18]  Ja, nou ik begon dit hoofdonderwerp met de stelling,
[3807.30 --> 3810.86]  nou ja, ik kan me eigenlijk helemaal niet voorstellen waarom je zou willen praten met een AI-bot.
[3810.86 --> 3821.38]  Maar ik zit even op die site te kijken en ik zou wel met minimaal vijf van die ik hier met energie echt wel willen converseren eigenlijk.
[3821.54 --> 3823.18]  Ik zie Socrates er ook bij staan.
[3824.04 --> 3828.86]  Je kan ook je sollicitatie technieken oefenen met een bot, met een interviewer.
[3829.92 --> 3832.08]  Elon Musk zit er dus inderdaad.
[3832.70 --> 3834.36]  En dan stelt hij ook al standaard vragen.
[3834.66 --> 3836.50]  Dan kun je dus een voorbeeldvraag gelijk stellen.
[3836.50 --> 3838.46]  En dan zegt, waarom heb je Twitter overgenomen?
[3838.64 --> 3841.12]  Dat is dan een leuke openingsvraag om met Elon Musk te praten.
[3841.30 --> 3842.18]  Nou, je ziet het voor je.
[3842.20 --> 3844.16]  Ik weet niet of het helemaal waar is wat je eruit krijgt.
[3844.18 --> 3844.50]  Nee, tuurlijk niet.
[3844.56 --> 3849.06]  Maar het is wel leuk om er op die manier weer anders over na te denken.
[3849.20 --> 3851.90]  Kan het misschien wel gebruiken ook een keer met het schrijven van een column op een bepaalde manier?
[3851.90 --> 3855.84]  Nou, ik denk Milou, misschien is het wel, we hebben vorige week met elkaar over gehad,
[3856.38 --> 3858.46]  waarom gebruikt Milou geen AI?
[3858.98 --> 3861.12]  Niet zo van dat moet je, maar dat is gewoon opvallend.
[3861.12 --> 3862.46]  Ik vind het ook interessant.
[3862.46 --> 3871.98]  En het zou natuurlijk kunnen zijn dat, pas als dat een audio of video of beide of in de vorm van een fictief karakter is,
[3872.28 --> 3873.68]  dat het jou begint aan te spreken.
[3873.82 --> 3880.70]  Dat je zegt, nou ja, ik vind het wel tof om een schrijfbuddy te hebben waarmee ik ga sparren over mijn volgende artikel.
[3881.20 --> 3882.84]  Maar het moet dan wel gewoon lekker op de bank.
[3883.30 --> 3887.34]  Terwijl ik in de onderkant van mijn iPhone praat, alsof ik met een vriend lekker zit te bellen.
[3887.34 --> 3892.12]  Ik noem maar wat, ik heb het nu even over jou, Milou, omdat je erbij zit.
[3892.24 --> 3901.08]  Maar ik kan me goed voorstellen dat voor iedereen een soort persoonlijke vorm of manier van benaderen van die taalmodellen,
[3901.16 --> 3906.30]  want dat is eigenlijk wat character AI is, is een laagje over de techniek heen waar de uitvinder,
[3906.52 --> 3912.10]  daarom heeft de uitvinder het opgericht, want die had dan al lang door dit model kan je gebruiken voor veel meer.
[3912.10 --> 3917.54]  Het is een beetje als die blauwe poppetjes die ineens op straat zo met een soort blauwe flits verschijnen,
[3917.62 --> 3920.40]  dat je denkt, hé, er zijn nieuwe entiteiten bij in de samenleving.
[3921.26 --> 3925.86]  Ik denk dat we nu op het virtuele vlak waar we veel tijd door brengen,
[3926.04 --> 3929.46]  dus die robots die komen nog niet zo snel de straat op,
[3929.54 --> 3932.64]  maar er zijn wel allemaal synthetische entiteiten in vorm van tekst,
[3932.74 --> 3937.68]  dan een plaatje, dan audio, dan video, eerst binnen character AI, dan binnen WhatsApp.
[3937.68 --> 3943.98]  Zo vult ons dagelijks leven zich steeds meer met van die niet bestaande, maar wel menselijk-achtige dingetjes.
[3944.36 --> 3945.94]  Dat vind ik een groot ding.
[3946.10 --> 3950.28]  En ik denk dat Alexander heel erg de vinger op de pols heeft van,
[3950.86 --> 3954.04]  dit is al aan het gebeuren, alleen nog niet zoveel in ons eigen leven.
[3954.60 --> 3957.64]  Alexander, jij moet je nu forceren om een soort van tiener te gaan spelen,
[3957.78 --> 3959.12]  om een beetje in te leven.
[3959.82 --> 3961.42]  Zelf gebruiken we dat nog niet zo heel erg veel.
[3962.06 --> 3965.14]  Ik vind dat boeiend en dat wordt alleen maar meer en meer.
[3965.14 --> 3970.02]  En ik denk dat het voor de luisteraars interessant is om eens rond te vragen in de groepen waar het al wel gebeurt.
[3970.10 --> 3970.96]  Is dat inderdaad zo?
[3971.30 --> 3973.18]  En misschien zelf een beetje te proberen ook van,
[3973.26 --> 3974.80]  hé, hoe reageer ik hierop?
[3975.20 --> 3977.64]  Een stukje voorbeeld op het doel wat Alexander doet.
[3978.32 --> 3983.40]  En daarbij wil ik wel, mijn grote kanttekening is,
[3983.84 --> 3985.14]  nog los van psychologische gevolgen,
[3986.08 --> 3989.34]  en waarom moeten we dan alleen maar met echte mensen praten,
[3989.42 --> 3990.52]  en wat is er mis met nep mensen?
[3990.84 --> 3991.52]  Ik weet dit niet.
[3991.52 --> 3995.12]  Dat is een gesprek wat we gaan voeren nog de komende tijd.
[3996.42 --> 4002.14]  Denk ik wel dat het belangrijk is om je te beseffen dat je praat met commerciële partijen,
[4002.22 --> 4004.78]  die datacenters hebben waarop die karakters leven.
[4005.36 --> 4006.76]  Dat karakter leeft niet bij jou thuis.
[4006.88 --> 4007.78]  Het boek is niet van jou.
[4008.38 --> 4010.84]  Het boek kan letterlijk uit je kast verdwijnen wanneer zij willen,
[4010.96 --> 4012.38]  net als bij Amazon Kindle, zeg maar.
[4012.88 --> 4014.20]  En in dit geval is het niet een boek.
[4015.04 --> 4016.26]  Ja, dat gebeurt op de hele tijd.
[4016.26 --> 4023.98]  En ik denk dat een stukje meer kennis over,
[4024.14 --> 4026.00]  net als dat het goed is dat je weet wat er in je eten zit,
[4026.68 --> 4028.28]  is het denk ik voor steeds meer mensen ook belangrijk,
[4028.36 --> 4029.90]  dat je weet wat er in je technologie zit.
[4030.58 --> 4035.48]  En de stakes, dus waar het om gaat, gaat alleen maar groter worden,
[4035.92 --> 4041.90]  naarmate die technologieën steeds meer menselijke aspecten emuleren.
[4041.90 --> 4047.22]  Want dan is het niet even mijn mailtje komt niet binnen of ik kan mijn e-book niet downloaden.
[4047.52 --> 4049.94]  Nee, een van mijn beste maatjes is er even niet.
[4050.34 --> 4051.06]  Wat is hier gaande?
[4051.06 --> 4057.18]  Ik las dus het begin van een groot interview met een van die oprichters.
[4057.38 --> 4063.82]  Dat begon met de scène dat er een man voor de deur zat van het huis van de oprichter.
[4064.42 --> 4066.66]  Even voor de helderheid, in de echte wereld.
[4067.52 --> 4072.30]  Iemand, een gebruiker van Character AI, die kwam klagen tegen de oprichter.
[4072.44 --> 4077.20]  Die was er ook iedere dag, want hij ontweek deze persoon, de oprichter natuurlijk,
[4077.20 --> 4079.68]  dat hij rondhing rond zijn huis.
[4079.68 --> 4084.14]  Maar bij de zoveelste keer dat hij weer kwam opdagen en toen op zijn kantoor,
[4084.18 --> 4085.62]  heeft hij toch maar met hem gesproken.
[4086.06 --> 4087.48]  Waar kwam deze man over klagen?
[4087.62 --> 4092.70]  Over dat de pornofilters op Character AI te streng geworden waren, volgens deze man.
[4093.10 --> 4097.12]  Dus die had een liefdesrelatie met een van de Character AI karakters.
[4097.62 --> 4102.04]  Toen wilde Character AI de mate van pornografie op het platform verminderen.
[4102.04 --> 4107.82]  Oftewel de mate waarin dingen gelijk ontaarden in daadwerkelijk expliciete seks, tekst gebaseerd.
[4107.82 --> 4110.54]  En dat hebben ze uitgedraaid.
[4111.04 --> 4112.60]  En daar was deze man boos over.
[4113.04 --> 4115.54]  En die kwam in de echte wereld aankloppen.
[4115.78 --> 4120.70]  En het artikel gaat verder, dat hij beschrijft verder dat de oprichter daar maar zijn schouders over ophaalt.
[4120.82 --> 4122.60]  En het wegschraakt als vrijheid van meningsuiting.
[4123.00 --> 4123.86]  Met die woorden zou ik gaan zeggen.
[4123.86 --> 4124.34]  Wauw.
[4124.92 --> 4125.28]  Goed.
[4125.92 --> 4126.54]  Dit was Boki.
[4126.54 --> 4128.98]  Ja, we zijn er doorheen.
[4129.08 --> 4129.76]  We zijn er doorheen.
[4130.30 --> 4134.02]  Sorry, ik wou even aan mij komen van die porno.
[4134.28 --> 4134.42]  Ja.
[4135.02 --> 4139.00]  Ik wou jullie eigenlijk ook nog eentje aanbieden die ik net tegenkom.
[4139.10 --> 4140.78]  Ik denk, daar hoef jij niet als klein meisje voor te doen.
[4140.86 --> 4141.06]  Oh ja.
[4141.18 --> 4142.18]  Die Adonis.
[4142.50 --> 4145.98]  I am Adonis and I aim to help young men improve.
[4146.36 --> 4148.30]  Oh, dat is wat voor mij een wits.
[4148.42 --> 4150.70]  Ja, daar kunnen jullie echt nog aan laven.
[4152.46 --> 4152.82]  Goed.
[4153.44 --> 4155.30]  Zouden de mensen nog een lezing van jullie willen?
[4155.30 --> 4158.72]  Nou, als je dat hierna nog wil, dan kan dat.
[4158.74 --> 4162.04]  Ja, dan kunnen ze mee naar lezing.poki.show.
[4162.46 --> 4164.26]  En dan danken wij Sam Hengenveld voor de edit.
[4164.72 --> 4167.58]  En oh ja, je kan je natuurlijk ook nog abonneren op de nieuwsbrief.
[4167.68 --> 4169.56]  Kijk op AI-report.email.
[4170.74 --> 4171.66]  Zijn we er doorheen?
[4171.68 --> 4172.02]  Tot volgende week.
[4172.40 --> 4172.92]  Tot volgende week.
[4173.00 --> 4173.20]  Dag.
[4173.20 --> 4173.24]  Dag.
[4185.30 --> 4186.30]  Dag.
[4186.30 --> 4187.30]  Dag.
[4187.30 --> 4188.30]  Dag.
[4188.30 --> 4189.30]  Dag.
[4189.30 --> 4190.30]  Dag.
[4190.38 --> 4191.30]  Dag.
[4191.30 --> 4192.30]  Dag.

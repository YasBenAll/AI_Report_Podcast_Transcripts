Video title: Een taalmodel uit de polder, GPT-NL ｜ ✨ Poki
Youtube video code: Q6ISUYLpI4A
Last modified time: 2024-01-22 10:11:21

------------------ 

[0.72 --> 4.44]  Zet jij je verwarming nog aan met zo'n ouderwetse thermostaatknop?
[5.12 --> 7.46]  Dan is Eneco Dynamics niks voor jou.
[7.98 --> 9.88]  Of bedien jij je thermostaat met een app?
[10.52 --> 12.76]  Dan is Eneco Dynamics misschien wel iets voor jou.
[13.50 --> 18.78]  Doe de test op eneco.nl slash test om te ontdekken of een dynamisch energiecontract bij jou past.
[19.36 --> 21.32]  Mensen helpen een bewuste keuze te maken.
[22.24 --> 23.82]  We doen het nu. Eneco.
[23.82 --> 31.26]  Als nu echt die vitale processen geraakt worden en we hebben echt te weinig cybercapaciteit bijvoorbeeld.
[31.64 --> 34.38]  Welke processen willen we dan kost wat kost in de lucht houden?
[34.84 --> 37.92]  En waar zetten we onze schaarse capaciteit op dat moment op in?
[38.28 --> 43.32]  In de nieuwe editie van Enter duiken we in Easydoor, de grootste cyberoefening van Nederland.
[43.90 --> 46.98]  Ontdek het belang van voorbereiden, oefenen en samenwerken.
[53.82 --> 73.04]  Het is 10 uur ochtends, vrijdag 3 november 2023.
[73.86 --> 75.92]  We zitten op zolder van de Montelbaanstoren.
[76.46 --> 77.46]  Het ruikt hier naar boeken.
[77.46 --> 83.76]  Deze woorden zijn, zolang je de autocorrect niet meetelt, volledig geschreven door een mens.
[84.42 --> 85.52]  Het is een vreemde tijd.
[86.04 --> 89.86]  We voelen dat er iets groots gebeurt, maar tegelijkertijd allemaal moeilijk te bevatten.
[90.54 --> 96.48]  Het is alsof iemand de videoband met daarop de gehele geschiedenis in een razend tempo aan het doorspoelen is.
[97.24 --> 100.84]  Er is buitenaards leven geland, maar ze kwamen niet uit de lucht als marsmannetjes,
[100.84 --> 102.92]  maar via het internet als taalmodellen.
[103.30 --> 104.48]  Dus niemand heeft het nog door.
[104.48 --> 108.94]  Dit is Poki, een podcast over kunstmatige intelligentie,
[109.20 --> 114.50]  waarin Alexander Klubbing en ik, Wietse Hagen, je alles vertellen over de wonderenwereld van AI.
[125.22 --> 126.02]  Hey Wietse.
[126.22 --> 126.78]  Hey Alexander.
[127.68 --> 133.02]  Gisteren, op het moment dat wij dit opnemen, werd bekend dat er een Nederlands taalmodel aangekomen van TNO.
[133.18 --> 134.14]  Daar gaan we het straks over hebben.
[134.48 --> 138.02]  Maar eerst wil ik het graag met je hebben over iets wat mijn hoofd doet ontploffen.
[139.10 --> 140.62]  En dat is RunwayML.
[141.66 --> 145.24]  En daar heb je vast, we hebben het hier soms wel eens over gehad.
[145.36 --> 149.88]  Dat is een tool als Mid Journey waarmee je korte videoclipjes kan maken.
[150.70 --> 154.94]  Dus je krijgt video's van maximaal vier seconden, maar geanimeerd.
[154.94 --> 163.86]  Het is alsof je generatieve foto's of kunst maakt met Mid Journey of Dali, maar dan bewogen.
[164.66 --> 174.12]  En dat was een tijdje geleden, en dan heb ik het over weken tot maanden, een soort van esthetiek op zich van AI-art.
[174.12 --> 175.82]  Het heeft iets dromerigs.
[176.00 --> 177.66]  Ja, het heeft iets heel dromerigs.
[177.94 --> 178.48]  Ja, dat is waar.
[179.04 --> 189.42]  En waarbij Mid Journey, als je ziet hoe zeer dat model is verbeterd in de afgelopen anderhalf jaar, dat is insane.
[189.72 --> 196.20]  Dat is echt van iets waarvan duidelijk is dat het computerkunst is of computergegenereerd is,
[196.20 --> 199.80]  naar iets wat werkelijk niet van echt te onderscheiden is op dit moment.
[200.08 --> 202.60]  De nieuwste versies van Mid Journey.
[203.16 --> 203.22]  Ja.
[203.58 --> 206.54]  En met video maken we zo'nzelfde curve nu door.
[206.88 --> 218.68]  En ik zou zeggen, tot gisteren was het, zoals die vroege versies van Mid Journey, namelijk wel grappig, want een stijl op zich.
[219.90 --> 222.84]  Wat jij zegt, dromerig en een beetje schokkerig.
[222.84 --> 225.34]  En je ziet allemaal artefacts.
[225.34 --> 230.66]  Dus als er iemand aan het wandelen is, dan opeens zijn er drie benen en die gaan in elkaar over.
[231.04 --> 232.98]  En het is echt als een brood.
[233.00 --> 235.54]  Consistentie visueel was ook niet heel juist.
[235.64 --> 235.90]  Klopt.
[235.92 --> 238.10]  Ik hoor aan jouw verhaal dat het beter is.
[238.12 --> 239.42]  Maar holy shit.
[239.84 --> 241.22]  Ik heb dit helemaal gemist.
[241.36 --> 242.04]  Ik ben helemaal benieuwd.
[242.14 --> 246.44]  Nou, er is een nieuwe versie van Gen 2, wat ik dan alweer apart vind.
[246.56 --> 248.54]  Ik zou zeggen, noem het dan Gen 3, maar oké.
[248.96 --> 249.48]  Er is een nieuwe...
[249.48 --> 249.80]  2,5.
[249.92 --> 250.70]  Ja, 2,5.
[250.70 --> 253.80]  Noem het, laten we het Gen 2.1 noemen.
[253.80 --> 255.42]  Van RunwayML.
[255.56 --> 260.54]  En RunwayML is een tool waar je text to video kan doen.
[260.76 --> 262.78]  Dus je beschrijft een scène.
[263.40 --> 265.36]  En dan maakt dat ding daar video van.
[265.74 --> 269.80]  En deze nieuwste versie is echt...
[270.64 --> 277.38]  Ik zit naar clips te kijken waarbij ik echt niet kon zien dat het computer gegenereerd is.
[277.38 --> 280.00]  Dat is voor het eerst met video.
[280.60 --> 280.68]  Ja.
[280.84 --> 283.54]  En hoe dat ding werkt is dat je...
[283.54 --> 285.40]  Het is net als in Mid Journey.
[285.48 --> 286.48]  Je voert een prompt in.
[287.32 --> 289.38]  Die prompts die zijn...
[290.00 --> 293.46]  Daar kun je net als in Mid Journey allerlei parameters aan toevoegen.
[293.64 --> 297.38]  Dus je kan de stijl van de video benoemen.
[298.38 --> 299.38]  Dus wil je het heel cinematografisch?
[300.10 --> 301.38]  Of wil je het heel erg...
[301.38 --> 303.48]  Weet ik veel, zwart-wit beelden van vroeger?
[303.48 --> 305.06]  Of wil je Pixar-stijl?
[305.20 --> 306.46]  Dat is de stijl van de video.
[306.56 --> 307.80]  Je kan de belichting kiezen.
[308.08 --> 309.48]  Dus wil je horrorfilm?
[310.44 --> 311.60]  Of wil je Wes Anderson?
[312.10 --> 312.42]  Of wil je...
[312.42 --> 313.48]  Nou ja, de belichtingstijl.
[314.20 --> 314.84]  Heel belangrijk.
[315.64 --> 316.54]  Hoe het eruit gaat zien.
[316.96 --> 319.28]  Je kan vertellen wat je wil dat het object is.
[319.38 --> 320.08]  Dus het karakter.
[321.12 --> 324.48]  Hij is best wel goed in mensen genereren.
[324.48 --> 326.96]  Dus je kan een soort van...
[326.96 --> 327.46]  Weet ik veel.
[327.54 --> 329.70]  Een James Bond neerzetten in een horrorfilm.
[329.84 --> 331.62]  Die een ruimteschip inloopt.
[331.74 --> 333.92]  En dan maakt hij best wel een goede James Bond.
[334.64 --> 338.12]  En tot slot kun je vertellen wat dat karakter dan moet doen in die vier seconden.
[338.18 --> 339.10]  Het is vier seconden.
[339.26 --> 340.34]  Dus het is kort.
[340.86 --> 342.46]  Dus je kan het karakter laten springen.
[342.70 --> 344.00]  Of laten wandelen.
[344.30 --> 345.68]  Of skateboarden.
[345.94 --> 346.82]  Of wat dan ook.
[347.06 --> 347.46]  Eigenlijk.
[348.06 --> 348.86]  Als het...
[348.86 --> 350.48]  Dit is allemaal getraind op stocklibraries.
[351.36 --> 353.02]  Dus als het in stockvideo's zit.
[353.28 --> 354.56]  Dan kan dit ding het ook.
[354.98 --> 356.76]  Dus ik denk als er...
[356.76 --> 358.48]  Als jij wil dat iemand...
[358.48 --> 361.20]  Laten we zeggen de Eiffeltoren op klimt.
[361.30 --> 363.24]  Misschien is dat niet beeld wat in stocklibraries zit.
[363.32 --> 364.74]  En dan lukt het dit ding dus niet.
[364.92 --> 365.54]  Om dat te maken.
[365.70 --> 366.48]  Maar...
[366.48 --> 368.04]  Hij kan dus drone shots maken.
[368.36 --> 368.86]  Nou echt.
[369.02 --> 371.14]  Je zegt drone shot piramide of zo.
[371.62 --> 373.50]  En het is echt insane.
[374.14 --> 377.08]  Maar wat ik me hiervan herinner dat het nog heel erg trippy was.
[377.20 --> 377.80]  Om het zo te noemen.
[377.88 --> 379.88]  Nou die tijd is klaar.
[379.94 --> 380.32]  Echt waar.
[380.32 --> 381.34]  Kan ik het nu vinden dan?
[381.66 --> 383.74]  Ja ik zat op Twitter allemaal clipjes te kijken.
[383.82 --> 385.16]  Van mensen die shit gemaakt hebben.
[385.26 --> 387.44]  Ik wil het misschien wel beschrijven aan een luisteraar.
[387.50 --> 387.86]  Wat ik zie.
[388.00 --> 388.80]  Kijk mijn timeline.
[389.68 --> 391.78]  Ik heb heel veel dingetjes gedeeld in mijn timeline.
[391.94 --> 395.08]  En er is nog steeds wel een soort van AI aesthetic.
[395.78 --> 397.32]  Want het is allemaal zo...
[398.44 --> 401.70]  Je kunt zulke rare shit erin doen.
[402.38 --> 404.06]  De inhoud is niet realistisch.
[404.16 --> 405.40]  Het ziet er realistisch uit.
[405.96 --> 407.74]  Maar je kunt zulke rare shit doen.
[408.42 --> 410.92]  Dat er nog steeds wel een soort van...
[410.92 --> 411.12]  Nou ja.
[411.16 --> 412.34]  Er is een soort AI aesthetic.
[412.58 --> 414.40]  Maar het is wel echt.
[414.74 --> 418.38]  En ik zie dus verschillende mensen verschillende beelden aan elkaar.
[418.92 --> 420.06]  Achter elkaar zetten.
[420.54 --> 421.16]  Dus dan heb je bijvoorbeeld...
[421.88 --> 423.96]  Stel je als je meer richting een film wil maken.
[424.08 --> 425.74]  Dan heb je bijvoorbeeld eerst een persoon.
[425.74 --> 427.92]  Als je twee karakters hebt.
[428.26 --> 430.12]  Dan heb je eerst een persoon die...
[430.12 --> 430.76]  Nou weet ik veel.
[430.82 --> 432.70]  Bedachtzaam in de verte kijkt.
[433.06 --> 434.62]  En dan cue die naar het volgende shot.
[434.82 --> 436.86]  Wat duidelijk in dezelfde ruimte gefilmd is.
[436.92 --> 437.82]  Met hetzelfde licht.
[438.10 --> 439.22]  Maar dan een ander karakter.
[439.60 --> 441.50]  Waarbij die dan een ander camera standpunt pakt.
[441.56 --> 444.42]  Dan kun je dus ook zeggen hoe de camera moet bewegen.
[444.52 --> 447.30]  Dus je kunt zo heel mooi langzaam cinematografisch inzoomen.
[447.52 --> 449.12]  Of je kan er een dolly shot van maken.
[449.12 --> 452.38]  En dan als je die shots allemaal achter elkaar gaat zetten.
[452.76 --> 454.22]  Je gebruikt dezelfde seed.
[454.54 --> 456.12]  Dus je kan vertellen wat de...
[456.64 --> 457.38]  Ja hoe moet je dat zeggen.
[458.50 --> 461.66]  Dezelfde soort van oorsprong ofzo.
[461.94 --> 462.60]  Van hetgeen.
[462.60 --> 463.18]  Ja je hebt het.
[463.40 --> 464.14]  Die kun je locken.
[464.24 --> 468.00]  Die plaat Dali 3 heeft nu ook support voor seeds.
[468.12 --> 471.22]  Want Mid Journey en Stable Defusion hadden dat al voor afbeeldingen.
[471.30 --> 472.12]  Maar Dali 3 nu ook.
[472.22 --> 473.68]  En dan als je iets gevonden hebt.
[473.74 --> 475.30]  Kan je zeggen ik wil hierdoor op itereren.
[475.34 --> 475.56]  Precies.
[475.62 --> 476.12]  Ik wil dat je dit vasthoudt.
[476.90 --> 477.78]  Ja precies.
[477.78 --> 478.38]  Precies.
[478.50 --> 482.44]  En soms is het ook niet zo heel goed wat hij maakt.
[482.60 --> 486.52]  Maar dan kun je dus wel binnen het ding wat hij gemaakt heeft.
[487.18 --> 488.96]  Stel je vindt de beweging van het karakter niet goed.
[489.02 --> 490.06]  Maar je vindt de rest wel allemaal goed.
[490.06 --> 490.96]  Dan kun je dus die seed locken.
[491.02 --> 493.92]  En dan kun je expliciet gaan praten over die beweging tegen dat ding.
[493.98 --> 494.94]  En dan gaat hij dat beter maken.
[495.58 --> 497.14]  En die dingen kun je allemaal achter elkaar zetten.
[497.26 --> 498.96]  En dan het begint gewoon echt.
[500.16 --> 500.92]  Ik zat te denken.
[501.00 --> 503.68]  Het eerste wat gaat gebeuren zijn televisie commercials.
[504.46 --> 505.38]  Gewoon dit is.
[505.38 --> 508.62]  Ik was gisteren op een congres van mediabureaus.
[509.00 --> 511.44]  En daarin ging het alleen maar over AI.
[512.20 --> 515.42]  Want die hele reclamewereld is natuurlijk in de war nu.
[515.76 --> 516.84]  En ik moet ook wel zeggen.
[517.10 --> 520.72]  Het is een soort van manier voor denk ik mediabureaus om zich nu echt te onderscheiden.
[521.12 --> 522.54]  Of voor reclamebureaus bedoel ik.
[522.82 --> 523.88]  Om zich echt te onderscheiden.
[523.88 --> 526.74]  Door te laten zien wat zij allemaal kunnen met AI.
[526.94 --> 529.44]  Je moet ook een signaal sturen dat je er bovenop zit.
[529.56 --> 529.84]  Precies.
[530.28 --> 532.66]  En ik vind op zich ook wel de...
[532.66 --> 536.12]  Ze liet een soort van reclame zien van Porsche die ze gemaakt hadden.
[536.70 --> 541.20]  Waarbij ze roze ballen in de Salar de Ioni.
[541.36 --> 541.66]  Weet je wel?
[541.70 --> 543.36]  Die zoutvlakte in Bolivia.
[543.92 --> 546.42]  Hadden ze allemaal enorme roze bollen opgezet.
[546.58 --> 549.08]  Wat in het echt een vrij dure productie zou zijn.
[549.16 --> 550.46]  Maar nu gewoon een paar prompten was.
[550.46 --> 551.98]  Dat gebruikte ze dan als achtergrond.
[552.08 --> 555.22]  En daar shopte ze dan een Porsche voor.
[555.70 --> 558.18]  En een model was ook AI gegenereerd.
[558.34 --> 559.16]  Dus ze zat een mevrouw.
[559.26 --> 560.68]  Stond voor die Porsche ook in het roze.
[560.68 --> 561.94]  Het is allemaal een afbeelding toch?
[562.12 --> 562.94]  Het was een afbeelding.
[563.56 --> 564.80]  Maar het was wel echt een vibe.
[564.98 --> 565.74]  Dat ik wel echt dacht.
[565.88 --> 567.48]  Oké dit is die AI aesthetic.
[568.10 --> 569.68]  En het is...
[570.40 --> 576.50]  Ze hadden ook allemaal covers van Vogue gebruikt als inspiratie.
[576.50 --> 580.44]  Dus dat hij dat gebruikt om die stijl een beetje na te doen.
[580.46 --> 581.08]  In die...
[581.08 --> 582.68]  Bijvoorbeeld voor dat model.
[582.82 --> 584.14]  Het is wel fijn...
[584.14 --> 585.00]  Fijn tuned ook.
[585.16 --> 586.52]  Niet zeg maar een algemeen model.
[586.68 --> 587.80]  Maar al een fijn...
[587.80 --> 587.98]  Ja.
[588.18 --> 590.28]  En in dit geval hebben we het over het...
[590.28 --> 591.04]  Model.
[591.46 --> 592.88]  Het AI generatieve model.
[592.96 --> 593.82]  Nee ik heb het nu over de dame.
[594.00 --> 595.04]  Ik heb het nu over de dame.
[595.60 --> 596.66]  Oh ja dat weer over de dame.
[596.68 --> 597.26]  Het model.
[597.44 --> 597.94]  Het mens.
[598.40 --> 598.98]  Erg verwarring.
[599.00 --> 600.14]  Maar niet een echt mens.
[600.24 --> 601.12]  Een AI mens.
[601.30 --> 601.40]  Ja.
[601.68 --> 602.76]  En ehm...
[602.76 --> 603.80]  Oh oké.
[603.86 --> 604.36]  Dat is niet...
[604.36 --> 604.62]  Oké.
[604.62 --> 605.76]  Die was ook gegenereerd.
[606.36 --> 606.52]  Oh.
[606.98 --> 609.32]  En het zag er echt heel erg geloofwaardig uit.
[609.32 --> 610.82]  Een model gegenereerd met een model.
[610.92 --> 611.10]  Yep.
[611.20 --> 611.68]  Sorry jongen.
[611.84 --> 612.10]  Oké.
[612.28 --> 613.36]  En ik dacht dus oké.
[613.58 --> 615.36]  Dit ziet er super vet uit.
[615.54 --> 618.42]  Deze hele zaal van reclame mensen die gaat hier lijp.
[618.64 --> 619.64]  Die wordt hier lijp van.
[619.78 --> 620.54]  Dus dit is iets.
[620.70 --> 621.26]  Dit gaat over.
[621.32 --> 622.14]  Dit gaan we overal zien.
[622.28 --> 624.42]  Als ik dat combineer met wat ik vandaag zie.
[624.98 --> 626.72]  Over die video's die gemaakt worden.
[626.82 --> 627.46]  Dan denk ik echt.
[628.02 --> 629.32]  Gewoon kijk naar hoeveel...
[629.32 --> 631.80]  Kijk naar een gemiddelde tv commercial.
[632.26 --> 633.18]  En je weet gewoon.
[633.56 --> 634.72]  Binnen nu en een half jaar.
[635.40 --> 638.38]  Nou ik weet niet hoeveel van die reclames gevuld gaan worden met AI content.
[638.50 --> 640.04]  Maar echt een heel groot deel.
[640.20 --> 642.78]  Nou we hebben natuurlijk onze Cora van Mora grapje gehad.
[642.90 --> 643.44]  Met TikTok.
[643.60 --> 646.82]  En dat ze zegt dat je nog even een kroketje of een skuizenvleetje kan eten s'avonds.
[647.72 --> 649.00]  Deze technologie is nu nog.
[649.32 --> 649.90]  Want dat...
[649.90 --> 652.90]  Ik ben dus heel verbaasd dat het zo snel gaat.
[652.98 --> 654.58]  Want ik had juist bij video het idee van.
[654.66 --> 655.82]  Oeh die is nog wel even tricky.
[655.98 --> 656.76]  Want het is gewoon zwaar.
[656.76 --> 658.40]  Daarom doen ze ook maar vier seconden.
[658.48 --> 660.86]  Want ja als het 25 frames per seconde zijn.
[660.96 --> 662.28]  Zit je toch op 100 frames.
[662.76 --> 669.02]  Terwijl een gemiddelde plaatje genereren in Dali 3 duurt een aantal seconden op een ultra datacenter ergens in Zeewolde.
[669.14 --> 669.52]  Dus weet je.
[670.22 --> 671.32]  Met allemaal waterkracht.
[671.68 --> 672.06]  Bij wijze van.
[672.52 --> 676.98]  Maar dus dat zij 100 frames kunnen genereren en daarmee vier seconden is eigenlijk heel erg knap.
[677.06 --> 678.52]  Je kan allerlei trucjes doen.
[678.60 --> 680.70]  Vast wel mid frames te genereren en de rest ertussen.
[681.06 --> 681.48]  Maar dan nog.
[681.72 --> 682.78]  Ja ik denk niet dat hij dat...
[682.78 --> 683.28]  Ja ik weet niet.
[683.34 --> 684.52]  Het zou kunnen dat hij dat doet inderdaad.
[684.56 --> 685.16]  Ja weet ik niet.
[685.20 --> 685.84]  Het is een gokje hoor.
[685.84 --> 688.64]  Maar zij zullen daar vast slimme trucjes voor hebben.
[689.30 --> 690.26]  Maar dat het...
[690.26 --> 695.14]  Ja want video is gewoon zo ontzettend zwaar in vergelijking met afbeeldingen en tekst.
[695.30 --> 697.66]  Dat ik het echt gaaf vind dat ze het zo snel voor elkaar hebben.
[697.98 --> 698.88]  En ik zat nu meteen.
[699.08 --> 700.10]  Nu jij dit vertelt te denken.
[700.92 --> 702.34]  Als het echt realtime kan.
[702.56 --> 703.58]  Het is schaalbaar.
[703.88 --> 706.28]  Dan kan je natuurlijk ook op doelgroepen gaan cateren.
[706.28 --> 707.46]  Kijk je kan pre-renderen.
[707.78 --> 710.54]  Dat je zegt we doen voor 5, 6 doelgroepen reclame maken.
[710.60 --> 710.82]  Ja zo.
[711.06 --> 713.06]  Maar ik zit echt realtime realtime te denken.
[713.06 --> 717.24]  Kijk als er TikTok clipjes zijn van 10 seconden en hij is aangepast op jouw esthetisch.
[717.70 --> 719.32]  En de persoon die jij waardeert.
[720.60 --> 724.06]  Dan is het een kwestie van rechten kopen op Linda de Mol en Winston Kestanovic.
[724.72 --> 726.44]  Dan kan je gewoon kijken wie nodig heeft.
[726.58 --> 728.52]  Nou even vertragend.
[728.74 --> 734.20]  Dus dit betekent dat nu op TikTok krijg je een video en die is in ieder geval voor iedereen hetzelfde.
[734.44 --> 736.72]  Misschien is de keuze voor jou persoonlijk.
[736.72 --> 742.66]  Maar er zijn naast jou nog wel andere mensen waarbij het algoritme denkt dit is een interessante video voor jou.
[742.78 --> 746.30]  Die video is dus voor iedereen die die video ziet hetzelfde.
[746.68 --> 748.82]  Maar stel nou dat je een video helemaal uit elkaar trekt.
[749.04 --> 751.24]  En je bekijkt het meer als verschillende lagen.
[751.86 --> 758.98]  En je kan dus in realtime worden versies voor jou gerenderd in een esthetiek die beter op jou werkt.
[758.98 --> 761.42]  Dus misschien weet ik veel een ander hoofdkarakter.
[761.90 --> 767.60]  Of misschien is de hele vibe van de video wel en de muziek wel aangepast aan jouw smaak.
[767.72 --> 771.18]  En dat is opeens realistischer hiermee.
[771.86 --> 775.20]  Ja iets wat natuurlijk met afbeeldingen en tekst al eerder kan.
[776.06 --> 782.98]  Oh maar ik vind het zo'n gek idee dat gewoon alle informatie die we tot ons gaan krijgen geïntermedieerd gaat worden door dit soort algoritmes.
[783.16 --> 784.74]  We hebben het eerder gehad over dat vertalen.
[784.74 --> 792.62]  Dus je luistert een podcast en niet alleen kun je de content gewoon in je eigen taal doen.
[792.74 --> 796.36]  Dus met gekloonde stemmen en dat je dan een Engelstalige podcast in het Nederlands kan luisteren.
[796.44 --> 798.42]  Met de originele stemmen en met de originele muziek.
[798.70 --> 801.18]  Maar ook dat het podcast aangepast kan worden aan jouw tijdspannen.
[801.30 --> 806.06]  Dus als jij 40 minuten in de auto zit dat ze er even 10 minuten afknippen als de podcast origineel 50 minuten is.
[806.50 --> 812.04]  Of dat ze constateren nou de voorkennis van deze persoon is niet groot genoeg om deze podcast te volgen.
[812.04 --> 813.88]  Dus we gaan het even wat uitgebreider maken.
[813.88 --> 817.46]  We gaan de ingewikkelde concepten iets meer de tijd nemen om die uit te leggen.
[817.58 --> 822.08]  En voor mensen die verder op de waag zitten dat dat juist eruit gaat.
[822.30 --> 827.20]  Gewoon het idee dat informatie die je consumeert helemaal aangepast is aan jou.
[827.64 --> 829.30]  Weet je, is een soort van, ik weet niet.
[829.64 --> 831.08]  Daar ging het in mijn studio al over.
[831.60 --> 835.46]  Dat nieuws en informatieconsumptie gepersonaliseerd gaat worden.
[835.58 --> 837.62]  En toen hebben we nieuwsfeeds gehad.
[838.00 --> 841.72]  En mensen consumeren het nieuws via algoritmes.
[841.72 --> 845.92]  Dus dat is op zich, dat is een grote verandering.
[846.24 --> 847.24]  Maar ja, filterbubbels.
[847.36 --> 847.92]  Dat is een grote verandering.
[848.06 --> 851.56]  Maar het is ook weer niet, ik wil niet bagitaliseren.
[851.72 --> 854.34]  Want er zijn ook gewoon kapitorenrellen geweest.
[854.54 --> 856.64]  En op basis van allerlei filterbubbels.
[856.76 --> 858.18]  Dus ik wil ook niet zeggen, er is niet zoveel gebeurd.
[858.24 --> 860.52]  Maar er zijn toch nog steeds ook heel veel mensen die teletext lezen.
[860.52 --> 861.44]  Ja.
[861.66 --> 870.12]  En het idee dat de informatie die voor jou gekozen wordt, als dat ook nog verder ontwikkelt.
[870.22 --> 873.92]  Maar ook nog daadwerkelijk de informatie zelf synthetisch gegenereerd wordt.
[874.02 --> 874.56]  Ja, ik weet niet.
[875.68 --> 877.50]  Ik kwam er gewoon met mijn hoofd niet bij.
[877.58 --> 882.48]  En als ik dit nu weer zie, dan denk ik, oh ja, dit gaat er sowieso TikTok in komen.
[882.48 --> 885.66]  Ik zeg maar, wat betekent dit voor de volgende versie van TikTok?
[886.04 --> 888.76]  Of wat dan ook dat bedrijf gaat zijn, welk bedrijf dat dan ook gaat zijn.
[889.38 --> 892.72]  Dat je naar synthetische informatie gaat zitten kijken.
[893.08 --> 893.24]  Ja.
[893.34 --> 894.32]  Het is zo raar.
[894.32 --> 901.26]  Ik denk dat, net als in het filterbubbel verhaal, dat idee van een consensus realiteit.
[901.38 --> 904.88]  Dus eigenlijk ja, een realiteit waar we grotendeels met elkaar in leven.
[905.02 --> 907.86]  En waar redelijke consensus is over wat wel of niet waar is.
[907.86 --> 913.72]  En een redelijke consensus over wat we zien, wat we subjectief meemaken.
[913.80 --> 917.30]  We hebben nooit in een pure consensus geleefd met elkaar.
[917.46 --> 919.54]  Waarin we allemaal subjectief hetzelfde dachten.
[920.24 --> 923.90]  Dus even voor een beeld voor mensen die denken, waar heeft hij het over?
[924.78 --> 932.02]  Als je met 100 mensen, als je 300 jaar geleden met 100 mensen keek naar iets wat gebeurde ergens.
[932.02 --> 933.02]  Een brand in het dorp.
[933.80 --> 935.96]  En je vraagt aan die 100 mensen, wat gebeurt er?
[935.96 --> 937.76]  En dan zeggen ze allemaal ongeveer, ik zie een dorp.
[938.22 --> 939.30]  En dat dorp staat in brand.
[939.44 --> 940.86]  En dat is wat er nu gaande is.
[941.46 --> 945.22]  Misschien dat er dan één bij zat die zei, dit is een teken van God.
[945.50 --> 947.66]  Of ik wees hier nog meer in dan een brand.
[948.26 --> 951.32]  Maar weet ik niet, ik kan het niet zo goed inschatten.
[951.40 --> 952.90]  Maar er was waarschijnlijk genoeg consensus.
[953.06 --> 955.36]  Want we leven met elkaar.
[956.10 --> 958.58]  Met een taal en een cultuur heb je een bepaalde consensus.
[958.68 --> 960.98]  Je moet wat overlap hebben om te kunnen communiceren.
[960.98 --> 963.06]  Als je elkaars taal niet spreekt, heb je al een probleem.
[963.14 --> 965.86]  Maar als je echt de wereld gewoon heel anders ervaart.
[966.42 --> 969.76]  Dan wordt het wel moeilijk om nog met elkaar te praten.
[970.18 --> 972.60]  Je hebt bijvoorbeeld de filosoof De Leus.
[972.96 --> 975.42]  Die boeken moest gaan schrijven om zijn boeken uit te leggen.
[975.84 --> 978.24]  Want hij had dus boeken waarin hij ideeën ging uitleggen.
[978.42 --> 980.76]  Maar om die ideeën uit te leggen had hij nieuwe concepten nodig.
[980.98 --> 983.70]  Dus moest hij eerst boeken gaan schrijven om jou de concepten uit te leggen.
[983.80 --> 985.38]  Om zijn uiteindelijke boeken te kunnen gaan lezen.
[985.46 --> 988.52]  Als een soort brug tussen hem en ons.
[988.52 --> 992.40]  Omdat hij zo'n wereld had bedacht met dingen die hij zag.
[992.50 --> 995.10]  Waar hij geen bestaande woorden en concepten voor kon vinden.
[995.58 --> 998.82]  Dus toen nieuwe concepten moest gaan maken om die toe te passen in zijn boeken.
[999.08 --> 1001.20]  Maar waarom zegt hij dat dan niet gewoon uit in het boek?
[1001.52 --> 1003.16]  Waarom zegt hij dan niet gewoon dit is dit concept?
[1003.38 --> 1006.16]  Nou, je zou zeggen doe niet zo moeilijk.
[1006.16 --> 1011.36]  En het is, ik bedoel, iedere schrijver zou misschien aan het begin zeggen.
[1011.42 --> 1012.52]  Joh, ik heb hier een nieuw woord.
[1012.86 --> 1013.52]  Een nudging.
[1013.86 --> 1015.52]  En dat houdt voor mij dit of dat in.
[1015.60 --> 1018.40]  Dan doe je wat definities en dan kan je het toepassen in je eigen verhaal.
[1018.88 --> 1021.80]  Maar als het natuurlijk meer dan alleen maar drie, vier concepten zijn.
[1021.88 --> 1024.46]  Maar echt een heel filosofisch systeem.
[1024.66 --> 1026.48]  Waarbij concepten weer op elkaar bouwen.
[1026.92 --> 1030.90]  Dan was het in zijn ogen in ieder geval nodig om echt systemische boeken te maken.
[1030.90 --> 1034.50]  Om daarmee de rest van zijn werk toegankelijk te maken.
[1034.50 --> 1037.36]  Maar dan probeer je dus eigenlijk een brug te maken.
[1037.46 --> 1038.54]  Zo zie ik dat dan voor me.
[1038.94 --> 1045.70]  Tussen wat in de consensus, wat in de huidige tijdsgeest begrepen wordt door.
[1046.48 --> 1047.60]  Er hoeft niet eens iedereen te zijn.
[1048.48 --> 1051.00]  Als jij morgen mij belt en zegt.
[1051.08 --> 1053.64]  Nou Wiet, ik heb nog eens nagedacht over AI en video.
[1054.38 --> 1058.26]  Ik zie daar hele grote implicaties voor sociale cohesie.
[1060.36 --> 1062.74]  Ik maak me zorgen en ik vind het ook super gaaf.
[1062.74 --> 1064.56]  En daar heb ik een boek over schreven.
[1064.68 --> 1067.84]  Maar als jij daar dan termen in gaat gebruiken en concepten die ik echt niet ken.
[1068.20 --> 1070.02]  Dan moet je eerst even een brug met mij bouwen.
[1070.42 --> 1071.52]  Hoef je niet met de hele wereld te doen.
[1071.72 --> 1074.60]  Maar als je wil dat ik jou ga begrijpen, zullen we dat moeten doen.
[1075.62 --> 1078.36]  Maar er was dus een dorp waarbij een fik was.
[1078.48 --> 1079.80]  En daar was een gedeelde realiteit.
[1080.68 --> 1082.60]  Ja, en dan gaat het redelijk goed.
[1082.72 --> 1086.48]  Want zolang je een gedeelde realiteit hebt, dan komt er van alles uit voort.
[1086.48 --> 1090.34]  Ook dat je met elkaar actie kunt ondernemen.
[1090.68 --> 1092.90]  En dat je met elkaar in vrede kunt leven.
[1093.20 --> 1095.22]  Of in ieder geval weet wat je doel is en waar je naartoe gaat.
[1095.34 --> 1098.32]  Anders ben je alleen maar aan het discussiëren over wat je eigenlijk nou daadwerkelijk ziet.
[1098.46 --> 1099.28]  Wat zien we eigenlijk?
[1100.54 --> 1102.04]  En op het moment dat je...
[1102.04 --> 1104.84]  En dat is dus niet nieuw, want de filterbubbels waren er al.
[1105.02 --> 1108.68]  En vroeger had je zuilen en kranten en verschillende religies.
[1108.76 --> 1110.00]  Dat is niet per se zo nieuw.
[1110.00 --> 1117.66]  Ik denk wat er nieuw aan is, is de hyperindividualisering, de atomisering van al die kleine mini-realiteitjes samen.
[1118.28 --> 1118.90]  En dat je dus...
[1119.62 --> 1126.34]  Kijk, op het moment dat je met een x duizend mensen in Nederland de Volkskrant leest, dan heb je daar je stukje consensus over.
[1127.24 --> 1130.78]  En dat hoeft niet perfect te zijn, maar genoeg overlap om met elkaar te kunnen praten.
[1130.86 --> 1133.28]  Of binnen je politieke partij, noem het maar op.
[1134.50 --> 1136.60]  En een cultuur zo groot als een land of een continent.
[1136.60 --> 1144.72]  Maar op het moment dat wij allemaal mini-realiteitjes hebben, nog meer dan dat we ooit gehad hebben, die helemaal gecaterd zijn op...
[1144.72 --> 1149.68]  Alexander heeft vandaag 90% mediaconsumptie, maar 90% was ook gepersonaliseerd.
[1150.12 --> 1151.90]  Het was 90% voor Alexander.
[1152.74 --> 1155.04]  En niemand anders deelt in jou met die realiteit.
[1155.20 --> 1158.86]  Dat is een soort hypersubjectiviteit bijna, van alles is voor jou.
[1159.44 --> 1165.22]  En dat heeft in het geval van onderwijs waarschijnlijk enorme voordelen.
[1165.22 --> 1171.02]  Want een student wordt op zijn of haar manier gecaterd op zijn of haar leerstijl.
[1171.08 --> 1172.28]  Wauw, wat goed, weet je wel.
[1172.40 --> 1176.88]  Eindelijk niet één docent die het aanlegt en je moet maar hopen dat je de leerstijl van de docent hebt.
[1176.94 --> 1182.00]  Maar nee, het wordt op jouw tempo, met jouw metaforen, op jouw niveau aan je uitgelegd.
[1182.36 --> 1186.70]  Volgens mij is daar die hyper-individualisering in potentie iets moois.
[1186.70 --> 1190.78]  En dat heb ik in de aflevering met Onno destijds al gezegd.
[1190.88 --> 1193.14]  Zolang er maar groepsopdrachten zitten in de rest van de middag.
[1193.34 --> 1195.88]  Zodat we weer een beetje consensusrealiteit met elkaar creëren.
[1196.10 --> 1197.34]  En samen, en cohesie.
[1197.96 --> 1205.64]  Maar ja, het TikTok-algoritme dat jou al de stukjes geeft die iedereen in potentie kan krijgen.
[1205.78 --> 1207.36]  Maar jij krijgt die specifieke stukjes.
[1207.50 --> 1212.64]  Dus dat is de personalisering van algoritmen waar we al jaren met elkaar over discussiëren.
[1212.64 --> 1216.68]  Maar als de stukjes zelf ook nog eens voor jou gemaakt stukjes zijn.
[1217.42 --> 1218.96]  Ja, dat is wel een...
[1218.96 --> 1222.88]  Mijn vraagteken daarbij, mijn zorg is, is er nog genoeg consensus?
[1223.52 --> 1227.10]  Kunnen we nog genoeg overlap met elkaar vinden om elkaar nog te begrijpen?
[1230.06 --> 1231.46]  Nou, leuk voor de sfeer dit weer.
[1235.86 --> 1238.64]  En je kan ook zeggen nu, ja Wietse, dat kunnen wij.
[1238.64 --> 1239.20]  Ja.
[1240.10 --> 1240.30]  Ja.
[1241.20 --> 1241.38]  Ja.
[1241.38 --> 1248.14]  Ja, een deel van mij wil er tegen ingaan dat ik dan denk, ja god, dit kon al.
[1248.24 --> 1249.30]  En het kon ook met tekst.
[1249.52 --> 1250.36]  Het kan al met tekst.
[1250.48 --> 1253.32]  Dus met tekst is dit tot nu toe niet gebeurd.
[1253.52 --> 1256.90]  Een soort van, ja, mini-realiteitjes.
[1257.20 --> 1260.10]  Hoewel je dan ook weer kan wijzen naar QAnon.
[1260.28 --> 1264.32]  En mensen die heel erg zijn gaan geloven in coronacomplotten.
[1264.32 --> 1267.30]  En dat is natuurlijk ook een soort van eigen realiteit bouwen.
[1267.30 --> 1274.06]  En dan is de aanname dat als je het met video doet, dat het dan meer gaat gebeuren.
[1274.94 --> 1276.62]  Ja, ik weet het gewoon niet.
[1277.30 --> 1277.46]  Ja.
[1278.02 --> 1279.52]  Technisch kan het in ieder geval opeens.
[1279.74 --> 1282.24]  Dus we zullen zien wat dat voor culturele impact gaat hebben.
[1282.56 --> 1284.68]  En we hebben het over reclame, dat is een veilig onderwerp.
[1284.68 --> 1289.68]  Maar inderdaad, wat betekent dit voor de manier waarop mensen...
[1290.24 --> 1295.74]  Zeg maar, als je gewoon kijkt naar het totale aantal minuten informatie die je consumeert via je telefoon.
[1295.82 --> 1298.16]  Hoeveel procent daarvan gaat gegenereerde video zijn?
[1298.94 --> 1301.52]  En gepersonaliseerde video.
[1301.64 --> 1304.98]  Want ik zit nu ook meteen te denken aan het hele fenomeen van sharing.
[1304.98 --> 1307.12]  Delen, in goed Nederlands.
[1308.44 --> 1311.16]  Dan even in een raar scenario.
[1311.64 --> 1316.88]  Maar jij leest op nos.nl een voor Alexander Clubbing geketerd nieuwsbericht.
[1317.12 --> 1319.78]  Want jij hebt het nu over video, maar laten we zeggen dat het multimedia is.
[1319.96 --> 1322.70]  Dus we hebben een header afbeelding, zit er drie video's in.
[1323.08 --> 1324.42]  Op jouw level, op jouw snelle.
[1324.78 --> 1325.54]  Helemaal te gek.
[1326.22 --> 1327.38]  En dan deel je die met mij.
[1327.50 --> 1328.22]  Ja, wel de versie.
[1328.22 --> 1330.40]  Achter hashtag clubbing of zo.
[1330.64 --> 1332.86]  Ja, of jij krijgt juist de versie voor jou.
[1332.94 --> 1336.30]  Die veel intelligenter is en meer metaforen heeft.
[1336.96 --> 1340.66]  Maar stel dat jij die link met mij deelt en ik krijg die voor mij.
[1341.54 --> 1342.84]  Wat hebben we dan met elkaar gedeeld?
[1342.96 --> 1343.86]  Behalve The Seed of zo.
[1343.86 --> 1344.62]  Ja, The Seed, ja.
[1344.96 --> 1345.14]  Ja.
[1346.00 --> 1346.36]  Toch?
[1346.46 --> 1348.40]  Maar dat is wel...
[1348.40 --> 1348.64]  Ja.
[1349.04 --> 1349.86]  We delen de Seed.
[1350.08 --> 1351.80]  Ja, je gaat geen screenshot maken ervan.
[1351.80 --> 1354.20]  Want dan krijg je de saaie versie die voor iemand anders gemaakt is.
[1354.32 --> 1355.20]  Lang niet zo engaging.
[1355.20 --> 1361.18]  Ja, of wil ik gewoon een knopje bovenin waar ik gewoon kan switchen tussen jou en mijn versie of zo?
[1361.22 --> 1363.12]  Ja, zodat we weer gemeenschapszin kunnen voelen.
[1363.26 --> 1365.06]  Ah, ik heb jouw versie van het artikel gegeven.
[1365.06 --> 1367.14]  Nou ja, nu zeg je het een beetje cynisch.
[1367.22 --> 1370.20]  Maar ik denk dat voor mij is het helemaal...
[1371.68 --> 1373.54]  Kijk, als iets gaat cateren op jou.
[1373.88 --> 1375.20]  Er is wel eens zo'n...
[1375.78 --> 1382.52]  Voor mij een artikel geweest jaren terug over een verzonnen fantasie-eiland van Google.
[1382.52 --> 1384.26]  Waar je dan naartoe kon in de realiteit.
[1384.26 --> 1387.04]  En die hadden dan alles van jou, wist Google.
[1387.20 --> 1388.82]  Dus op dat eiland kom je dan aan met een boot.
[1388.90 --> 1390.58]  En dan is dat eiland jouw droom-eiland.
[1390.94 --> 1391.30]  Muziek.
[1391.58 --> 1394.82]  Dat was een beetje het gedachte-experiment, zeg maar.
[1395.46 --> 1397.88]  En dus die persoon beschrijft dan...
[1397.88 --> 1399.38]  Zo'n journalist had dat opgeschreven van...
[1399.38 --> 1400.24]  Ik kom daar aan.
[1400.70 --> 1401.02]  En ja, ja.
[1401.28 --> 1402.66]  Mijn lievelingsdrankje.
[1403.12 --> 1404.90]  De muziek wordt voor mij gemaakt.
[1405.00 --> 1406.24]  Het is helemaal mijn eiland.
[1406.34 --> 1408.80]  Want dit is het uiteindelijke einddoel van Google.
[1409.90 --> 1410.30]  Hyperpersonalisatie.
[1410.30 --> 1412.46]  Dat was een beetje volgens mij het grapje van dat artikel.
[1412.46 --> 1418.30]  Kijk, het punt is dat daar dus in zit dat jij geeft aan een...
[1418.30 --> 1421.08]  Het is de oude algoritme discussie, maar hij is nu weer een beetje relevanter.
[1421.14 --> 1423.76]  Want de technologie wordt gewoon steeds beter om dit mogelijk te maken.
[1423.86 --> 1425.00]  Dus ik wil hem toch weer even voeren.
[1425.40 --> 1429.32]  Is dat het algoritme op jou gaat cateren als mens.
[1429.62 --> 1430.40]  Als individu.
[1430.40 --> 1431.34]  Jouw wensen.
[1431.54 --> 1432.22]  Perfect voor jou.
[1432.30 --> 1434.52]  Jouw tempo, jouw snelheid, jouw woorden, jouw concepten.
[1434.58 --> 1435.12]  Noem het maar op.
[1435.20 --> 1436.44]  De plaatjes die voor jou wat doen.
[1437.12 --> 1439.02]  Maar daar zit dus het idee in...
[1439.02 --> 1440.34]  Dat jij weet wat jij wil.
[1441.24 --> 1444.78]  En dat jij goed kan laten zien aan een algoritme wat jij nodig hebt.
[1444.94 --> 1447.24]  Er zitten al die aannames en daar zit dan mijn zorg.
[1448.44 --> 1451.68]  Dat ik denk van ja, misschien ook leuk om Wietz's versie een keer te lezen.
[1452.08 --> 1453.24]  Want misschien denk jij wel...
[1453.24 --> 1456.40]  Maar dit is toch precies hoe het TikTok algoritme werkt.
[1456.58 --> 1459.48]  Die zit de hele tijd soort van testjes te doen.
[1459.80 --> 1462.22]  Met en content die nog niet breed gedeeld is.
[1462.44 --> 1465.08]  Om te kijken, is dit misschien wat voor een groter publiek.
[1465.22 --> 1471.36]  En twee, content buiten jouw vastgestelde interessegebieden.
[1471.88 --> 1474.02]  Dus hij zit de hele tijd te poken.
[1474.34 --> 1478.42]  Hij geeft je veel, waarvan hij de kans heel groot acht dat je het leuk gaat vinden.
[1478.60 --> 1481.48]  En hij geeft je denk ik één op de tien video's of zo.
[1481.48 --> 1484.72]  Een video die echt buiten jouw interessezone valt.
[1484.86 --> 1485.96]  Om te kijken of je nieuwe...
[1485.96 --> 1491.34]  Want als je alleen maar content uit jouw eigen interessegebieden gaat serveren als TikTok zijnde.
[1491.76 --> 1494.32]  Dan raak jij de interesse kwijt.
[1494.40 --> 1496.54]  Want je moet de hele tijd voor het iets nieuws krijgen.
[1496.72 --> 1498.10]  Ze hebben al een soort serendipity ingebouwd.
[1498.46 --> 1498.66]  Ja, zeker.
[1499.22 --> 1499.72]  Ja, ja.
[1499.82 --> 1500.30]  Zeker.
[1500.88 --> 1501.36]  Ja, ja, ja.
[1501.84 --> 1504.10]  Dus dat op die manier gebeurt al.
[1504.16 --> 1507.54]  Misschien wat ik dan nog zou willen is een inputveld.
[1507.64 --> 1508.42]  Of iets dat ik kan zeggen.
[1508.50 --> 1510.44]  Joh, ik wil eigenlijk meer gaan sporten.
[1510.44 --> 1512.18]  Ik weet dat ik de hele tijd Rome is kijk.
[1512.30 --> 1513.78]  Maar als je dan iets erdoorheen gooit.
[1513.86 --> 1517.44]  Toe even wat content rondom lichamelijke verzorgen.
[1517.44 --> 1517.86]  Nou, oké.
[1517.88 --> 1520.60]  Dus daarvan uitgaan dat je weet wat je wil.
[1520.76 --> 1521.90]  Of wat goed voor jou is.
[1522.02 --> 1522.42]  Oké.
[1522.48 --> 1524.48]  Ja, of wat ik denk dat ik zou moeten willen.
[1524.80 --> 1526.12]  Maar goed, een soort inputveld.
[1526.24 --> 1528.66]  Wat 99% van de tijd leeg blijft.
[1528.74 --> 1530.22]  Ik bedoel, in TikTok kun je ook searchen.
[1530.40 --> 1533.14]  Maar wat de meeste mensen doen is gewoon de homepage openen.
[1533.14 --> 1537.56]  Ik heb dus wel eens een geweldige video van de Wall Street Journal gekeken.
[1537.68 --> 1541.66]  Waarin ze hadden 100 telefoons naast elkaar gelegd.
[1541.82 --> 1545.10]  Daar hebben ze allemaal TikTok vers op geïnstalleerd.
[1545.22 --> 1548.68]  En ze hebben één identificerend element meegegeven aan iedere telefoon.
[1548.80 --> 1550.08]  Dus bij één is het een locatie.
[1550.20 --> 1551.40]  Bij de ander is het een interessegebied.
[1551.52 --> 1553.12]  Bij de ander is het een demografisch gegeven.
[1553.24 --> 1554.52]  Dus een leeftijd of wat dan ook.
[1554.52 --> 1559.34]  En dan kijken ze, als ze bij iedereen op dezelfde snelheid gaan scrollen.
[1559.82 --> 1560.78]  Dus volgende video.
[1560.94 --> 1562.06]  Gezegd door de video's.
[1562.44 --> 1566.88]  Dan kijken ze dus hoe dat eerste identificerende element definiërend is.
[1567.12 --> 1569.50]  Voor wat voor effect dat heeft.
[1569.86 --> 1574.30]  Als je dan een uur verder bent met scrollen.
[1574.38 --> 1577.52]  En dan heeft hij dus, op basis van het eerste identificerende element.
[1577.98 --> 1581.02]  Heeft hij dus jou een soort van rabbit hole inge...
[1581.02 --> 1581.86]  Ja, dat is weer die seed.
[1582.06 --> 1584.44]  Dat wordt een beetje de rode draad van deze aflevering merk ik.
[1584.52 --> 1585.40]  Maar het is waar het begon.
[1585.60 --> 1586.80]  Waar het uit gegroeid is.
[1586.92 --> 1587.44]  Ja, precies.
[1587.80 --> 1588.30]  Ja, ja, ja.
[1588.42 --> 1589.28]  De eerste indicator.
[1589.54 --> 1591.94]  En op basis daarvan hebben ze dus een soort van grafiek gemaakt.
[1592.82 --> 1594.64]  Van die hele interesse graph.
[1595.02 --> 1597.66]  Die zij in ieder geval konden constateren met 100 telefoons.
[1598.28 --> 1600.32]  Van TikTok onderwerpen.
[1600.72 --> 1604.38]  En zij constateren dan dus ook dat als iemand aanleg heeft voor depressiviteit.
[1604.38 --> 1606.86]  En dat is de seed.
[1607.46 --> 1611.52]  Dat TikTok je echt een soort diep dal in stuurt.
[1611.98 --> 1614.24]  Van hele tragische sad video's.
[1614.52 --> 1616.60]  Maar ook andersom.
[1616.80 --> 1619.24]  Ook allerlei gezellige dingen.
[1619.76 --> 1621.40]  Maar die grafiek was heel vet.
[1621.52 --> 1626.14]  Want dan zie je dus in het begin zijn er heel veel puntjes...
[1626.14 --> 1630.68]  Waarbij de eerste 20 minuten dat TikTok gebruikt wordt...
[1630.68 --> 1633.04]  Eigenlijk alleen maar populaire video's vertoont.
[1633.04 --> 1640.68]  Maar dan gedurende die 20 minuten begint hij steeds vaker te poken met allerlei nichere dingen.
[1640.92 --> 1641.90]  Op hoog niveau niche.
[1642.06 --> 1642.74]  Dus dan weet ik veel.
[1642.84 --> 1643.90]  Dieren is dan een niche.
[1644.50 --> 1647.86]  En dan als die eenmaal raak is, dan gaat die daar verder in.
[1648.04 --> 1652.32]  En dan als je dat gevisualiseerd ziet, is het dus bijna als een soort van bol.
[1652.70 --> 1653.98]  Die in het begin heel dens is.
[1654.06 --> 1655.78]  Maar waar allemaal tentakels uitkomen.
[1655.78 --> 1659.10]  En die tentakels zijn dan die soort van niches.
[1659.16 --> 1661.60]  En die krijgen ook steeds minder vertakkingen.
[1661.74 --> 1665.98]  Omdat binnen een niche van een niche van een niche van een niche heb je steeds minder vertakkingen.
[1666.20 --> 1667.86]  En dat zag je dan gevisualiseerd.
[1668.06 --> 1674.24]  En soms dan zit ik wel eens in bed te TikTokken met de regeldekende ramen.
[1674.24 --> 1678.24]  En dan zie ik dus voor me hoe ik in die grafiek...
[1678.24 --> 1680.14]  Ja, je bent je bewust.
[1680.48 --> 1682.56]  Ja, ik houd het zo sad.
[1683.22 --> 1684.70]  Maar ik denk dat het een beetje...
[1684.70 --> 1687.40]  Ik zat te denken aan de cheesy Chris Martin quote van Coldplay.
[1687.56 --> 1689.90]  Van when you get what you want, but not what you need.
[1690.02 --> 1693.02]  Ik denk wel dat er een verschil is tussen wat we willen en nodig hebben.
[1693.98 --> 1698.08]  En daar kunnen we tot een eeuwigheid over discusseren.
[1699.02 --> 1700.80]  Oké, wat is dan het verschil?
[1701.40 --> 1703.30]  Wie bepaalt dan wat we nodig hebben?
[1703.30 --> 1705.30]  Hebben we allemaal hetzelfde nodig?
[1705.54 --> 1707.92]  Moet het uit een societal system prompt komen?
[1708.34 --> 1713.12]  Wat zijn dit voor enge religieuze ideeën met dogma's en van bovenop gelegde waarden?
[1713.40 --> 1716.08]  Dus laten we die toch maar eventjes gewoon parkeren.
[1716.54 --> 1722.54]  Maar ik wil nog wel even het punt maken dat ik denk dat deze algoritme filterbubbel discussie...
[1722.54 --> 1726.14]  die misschien voor sommige luisteraars oud is of veel te bekend.
[1726.56 --> 1728.88]  Ik denk dat de nuance die wij nu maken is...
[1728.88 --> 1730.94]  dat de content waar we het steeds over hebben...
[1730.94 --> 1733.08]  dat lekkere lege YouTube woord denk ik altijd.
[1733.08 --> 1734.34]  Ik ben een content creator.
[1734.60 --> 1735.42]  Ja, oké, prima.
[1735.62 --> 1738.16]  Volgens mij maak je gewoon dingen en ga ik er bijna naar kijken.
[1738.54 --> 1740.90]  Alexander en ik zijn ook content creators.
[1741.08 --> 1745.64]  Maar goed, het is gewoon de hoogste abstractie volgens mij die je kan hebben voor iets wat inhoud zou kunnen hebben.
[1746.94 --> 1749.34]  Dat de content zelf ook gepersonaliseerd wordt.
[1749.74 --> 1753.24]  Dus het is niet alleen maar meer wat je ziet in welke volgorde wanneer...
[1753.24 --> 1754.52]  maar letterlijk de inhoud.
[1754.78 --> 1756.12]  Die is ook voor jou.
[1756.90 --> 1758.80]  Ik denk dat dat wel substantieel verschil maakt.
[1758.92 --> 1763.24]  En de oude dynamieken die al lang geïdentificeerd zijn op het gebied van filterbubbels, subjectificering...
[1763.88 --> 1765.24]  en weet ik wat voor moeilijke dingen allemaal...
[1766.06 --> 1767.80]  dat dat nu allemaal nog meer hyper gaat.
[1767.96 --> 1770.16]  Dat we het nog meer gaan merken mogelijk.
[1770.16 --> 1770.56]  Ja.
[1771.50 --> 1779.00]  Een ander bedrijf wat me opviel bij het maken van 3D beweegbare objecten is Luma Labs.
[1779.22 --> 1780.20]  Daar heb je wel van gehoord, toch?
[1780.26 --> 1780.42]  Ja.
[1780.42 --> 1783.42]  Dus Luma Labs maakt een app waar ze...
[1784.26 --> 1785.42]  Hoe moet ik dat nou zeggen?
[1785.86 --> 1786.36]  Een soort van...
[1786.36 --> 1788.48]  Je maakt een aantal foto's van een object.
[1788.80 --> 1790.78]  Dat doe je met een best wel vette interface.
[1790.92 --> 1793.72]  Dus stel je voor je zit aan een tafel en je zet een cactus op die tafel.
[1794.38 --> 1801.36]  Dan word je in de UI geïnstrueerd om die tafel heen te gaan wandelen met je iPhone in je hand.
[1801.54 --> 1802.86]  Zodat je helemaal rondloopt.
[1802.96 --> 1805.54]  Zodat je die cactus van boven een rondje maakt.
[1805.60 --> 1806.86]  Dan van de zijkant een rondje maakt.
[1806.94 --> 1807.42]  En dan van een onderaanzicht.
[1808.22 --> 1809.24]  Dus je maakt drie rondjes.
[1809.24 --> 1812.24]  Dan heb je die cactus vanuit alle hoeken gefotografeerd.
[1812.86 --> 1813.76]  Dan druk je op renderen.
[1813.92 --> 1814.62]  Dat duurt een minuutje.
[1815.10 --> 1817.54]  En vanaf dat moment heb je een 3D versie van die cactus.
[1817.90 --> 1821.98]  Waarbij je de scène, dus wat je net hebt gedaan, dat wandelen.
[1822.74 --> 1825.24]  Waarbij je hem kan manipuleren met je vinger.
[1825.42 --> 1829.44]  Dus je kan, zoals in Google Street View je door de wereld kan bewegen.
[1829.56 --> 1832.56]  Kun je opeens die cactus op je scherm van alle kanten bekijken.
[1832.70 --> 1834.48]  Nou, dat is iets waar ze...
[1834.48 --> 1836.60]  Dat was een jaar geleden of zo.
[1836.64 --> 1837.24]  Was dat heel vet.
[1837.24 --> 1840.60]  Toen kwamen ze met een versie waarbij je dit kan doen, maar dan met je huis.
[1840.72 --> 1841.44]  Dus een hele scène.
[1841.58 --> 1845.52]  Niet een gefixeerd object wat je fotografeert.
[1845.94 --> 1848.84]  Maar een hele ruimte.
[1849.30 --> 1853.02]  Waarbij je dus dat ding je weer instruueert om op een bepaalde manier door de ruimte te lopen.
[1853.50 --> 1854.34]  En dat kan van alles zijn.
[1854.34 --> 1862.04]  En dan maakt hij dus een soort van 3D representatie van die scène op basis van inputfoto's.
[1862.10 --> 1863.44]  Eigenlijk maakt hij gewoon heel veel foto's.
[1863.92 --> 1864.34]  En dan vanuit...
[1864.34 --> 1866.08]  Als ik mag toevoegen aan je verhaal.
[1866.30 --> 1867.44]  Volgens mij heet dit NERF.
[1867.54 --> 1867.68]  Ja.
[1867.88 --> 1869.44]  Neural Radiance Field.
[1869.70 --> 1869.88]  Ja.
[1870.50 --> 1870.94]  Nee, klopt.
[1871.12 --> 1871.58]  Dat is wat we doen.
[1871.58 --> 1871.66]  NERF.
[1871.66 --> 1871.76]  Ja.
[1872.42 --> 1875.64]  En dat zijn dus twee dingetjes die ze gedaan hebben.
[1875.72 --> 1876.92]  En dat laatste is echt heel vet.
[1877.00 --> 1881.96]  Want dan kun je dus zelfs een soort van drone shot laten genereren van je kamer.
[1882.28 --> 1887.58]  Het onbeperkt aantal drone shots kunnen genereren op basis van die beelden die je net gemaakt hebt.
[1887.66 --> 1888.96]  En ik probeer me dus de hele tijd voor te stellen.
[1889.06 --> 1892.56]  Wat gaat dit betekenen voor foto albums van de toekomst?
[1892.68 --> 1895.62]  Zeg maar als je dan de verjaardag van je kinderen wil fotograferen.
[1895.62 --> 1899.10]  Kun je dan gewoon een scène vastgesteld in de tijd.
[1899.10 --> 1906.04]  Wat ik me dan voorzie is dat je dus niet alleen gewoon onbeperkt drone shots kan maken van je kind die een verjaardagstaart uitblaast.
[1906.30 --> 1910.26]  Maar dat dan ook zo'n tool als runway dan die kaarsjes mooi animeert.
[1910.46 --> 1910.82]  Of weet je.
[1910.94 --> 1913.62]  Dus dat er nog wel iets van beweging in die scène is.
[1913.64 --> 1916.64]  Ik voel meteen een soort Harry Potter krantachtige sfeer.
[1916.80 --> 1919.32]  Met ontschilderijen die tegen je pratenachtige sfeertje.
[1919.52 --> 1919.84]  Precies.
[1919.84 --> 1927.48]  Dit is ook een beetje wat aansluit op die soort perfect storm van VR, AR, Vision Pro en AI in één.
[1927.48 --> 1929.70]  Ik zit even alle hype termen van de tijd bij elkaar te plakken.
[1929.70 --> 1930.76]  Maar het komt bij elkaar nu.
[1930.80 --> 1931.70]  Het komt gewoon bij elkaar.
[1931.92 --> 1935.36]  Want die generfde kamers en die momenten.
[1935.42 --> 1939.80]  Die kan je natuurlijk helemaal tot je nemen in zo'n ski bril van Apple.
[1939.96 --> 1941.34]  Dus dat gaat natuurlijk alle kanten op.
[1941.40 --> 1944.76]  Nou en dat is het volgende ding wat zij aankondigen.
[1944.76 --> 1949.50]  Dat was het nieuwste wat zij volgens mij een paar weken geleden nu hebben aangekondigd.
[1949.58 --> 1952.82]  Luma Labs heeft een tool gemaakt die heet Genie.
[1952.82 --> 1957.26]  En Genie is een text to 3D engine.
[1957.94 --> 1962.64]  En wat zij hebben gedaan is zij hebben al die beelden die wij in het afgelopen jaar met die app hebben gemaakt.
[1962.78 --> 1965.08]  Met wij bedoel ik app gebruikers van Luma Labs.
[1965.44 --> 1967.78]  Die dus objecten zoals Kakti.
[1968.26 --> 1972.22]  En andere objecten die mensen in 3D gefotografeerd hebben.
[1972.90 --> 1977.80]  Wat ik niet door had was dat dat allemaal een soort van opmaat was naar dit ding.
[1977.80 --> 1981.54]  Namelijk dat ze genoeg gebruikers hadden die foto's gefotografeerd hebben en die getagd hebben.
[1981.62 --> 1984.48]  Of misschien een soort van image recognition.
[1984.86 --> 1988.52]  En daarmee kunnen ze dus, kun je dus nu text to 3D doen.
[1988.60 --> 1991.38]  Dus als jij zegt doe mij een plaatje van een hamburger.
[1991.56 --> 1992.46]  Krijg je een 3D hamburger.
[1992.64 --> 1994.92]  Maar je kan dus ook zeggen doe mij een hamburger.
[1995.14 --> 1997.66]  Maar dan in plaats van broodjes moet het Oreo cookies zijn.
[1997.90 --> 1999.84]  En dan krijg je dus een hamburger gemaakt van Oreo cookies.
[1999.96 --> 2005.02]  Oftewel hij kan zoals je in Mid Journey je fantasie kunt gebruiken om alles te genereren.
[2005.02 --> 2008.18]  We hebben dus nu foto's gehad genereren.
[2008.28 --> 2010.24]  We hebben nu net video's besproken genereren.
[2010.36 --> 2012.36]  En dit is dus 3D beelden genereren.
[2012.92 --> 2016.36]  En dat is dan weer, ik zag iemand die het heeft gemaakt voor Oculus.
[2017.00 --> 2019.34]  Om het rondje maar compleet te maken.
[2019.92 --> 2025.02]  Waar dus iemand die 3D objecten die dus gegenereerd worden uit stof.
[2026.06 --> 2030.06]  Of nou ja, beelden die mensen eerder hebben toegevoegd aan die app.
[2030.12 --> 2032.38]  En blijkbaar hun auteursrechten daarop hebben weggetekend.
[2032.38 --> 2036.26]  Dat in een 3D wereld zetten.
[2037.20 --> 2037.90]  Nou dan ben je rond.
[2038.96 --> 2044.82]  Ik zit te denken wij hebben ooit jaren geleden in een 3D VR experience op zombies geschoten.
[2045.02 --> 2045.88]  Ik weet niet, waren het zombies?
[2045.88 --> 2046.98]  Ik weet het niet meer.
[2047.04 --> 2047.56]  Weet je nog?
[2047.86 --> 2049.72]  In een VR arcade of zoiets.
[2049.86 --> 2051.04]  Nou dat is het wel geleden.
[2051.04 --> 2051.54]  Ja.
[2052.46 --> 2058.94]  En nu had ik ineens door jou verhaal dat ik dacht nu ja, maar als het dan zo, ja hoe zeg je dat, schaalbaar is.
[2059.08 --> 2062.90]  Dus je kunt echt met een druk op de knop hele 3D werelden creëren ook.
[2063.34 --> 2068.12]  En dan zou je misschien kunnen zeggen dat in die game, om het even ook te koppelen met het verhaal hiervoor.
[2068.12 --> 2070.02]  Over de consensus realiteit.
[2070.74 --> 2079.88]  Dat ik daar in een soort Zelda Breath of the Wild, dus dat is een gezellige wereld vol gras, zon en gezelligheid ben.
[2080.20 --> 2083.36]  En jij bent echt in een hel dystopian zombie wereld.
[2083.72 --> 2085.24]  Ik zou zeggen super Mario land.
[2085.24 --> 2095.48]  Ja, ik ben in Mario teletubilant en jij staat gewoon echt, ja wat is het, Black Mirror incarnated in een VR game.
[2095.66 --> 2095.80]  Ja.
[2095.80 --> 2096.26]  Vreselijk.
[2096.54 --> 2098.90]  En dan, maar we zijn in dezelfde fysieke ruimte.
[2099.10 --> 2099.24]  Ja precies.
[2099.26 --> 2099.90]  In dezelfde.
[2100.28 --> 2101.40]  En jij begint op mijn...
[2101.40 --> 2101.82]  Precies is hetzelfde.
[2102.30 --> 2104.54]  Ja, jij schiet terwijl ik brandjes aan het blussen ben.
[2104.72 --> 2105.16]  Ja, ja, ja.
[2105.16 --> 2105.58]  Ik noem maar wat.
[2106.58 --> 2114.62]  Maar ik denk dat, dit is even mijn beeld om aan te geven wat er gebeurt als je dus nog wel in dezelfde fysieke realiteit met elkaar bent.
[2114.62 --> 2116.20]  Zoals wij dat nu al zijn, zeg maar.
[2116.32 --> 2118.78]  Ik bedoel, ja, als je iemand tegenkomt op straat.
[2119.28 --> 2122.42]  Maar op diezelfde manier zijn we nooit hetzelfde geweest.
[2123.02 --> 2134.90]  Maar de dial, zeg maar, het draaiknopje nu, de slider van die consensus, die slider van hoe groot het gat gaat worden tussen onze beleving van in essentie dezelfde fysieke situatie.
[2135.56 --> 2138.34]  Ja, dat ding is aan het schuiven nu door deze technologie.
[2139.14 --> 2141.90]  En ja, interessante tijden.
[2142.74 --> 2144.36]  Goed, laten we naar iets anders doorgaan.
[2144.62 --> 2147.24]  Even voor de foodies en drukke levensgenieters onder ons.
[2147.46 --> 2148.14]  Je kent het wel.
[2148.26 --> 2151.84]  Je hebt net een drukke dag achter de rug en je hebt eigenlijk geen zin om uitgebreid te koken.
[2152.26 --> 2154.64]  Wel nu, Lazy Vegan heeft daar de oplossing voor.
[2155.04 --> 2157.26]  We hebben het over echt lekkere roerbakmaaltijden.
[2157.60 --> 2162.48]  Voedzaam, vol met knapperige groenten en het beste van alles klaar in slechts 8 minuten.
[2163.06 --> 2163.86]  Je hoeft niet ver te zoeken.
[2164.04 --> 2167.78]  Deze kleurrijke zakken liggen gewoon in de vriezer bij verschillende supermarkten en flitsbezorgers.
[2168.18 --> 2171.10]  En voor de prijs hoef je het ook niet te laten vanaf slechts 4,99 euro.
[2171.10 --> 2174.80]  Er is nu een nieuwe smaak gelanceerd bij Albert Heijn, Korean Noodles.
[2175.42 --> 2177.82]  Je kunt de knallende roze verpakking niet missen.
[2178.36 --> 2182.12]  Voor meer informatie en het compleet assortiment, check lazyvegan.com.
[2182.78 --> 2184.92]  En ja, de link vind je ook in de show notes.
[2184.92 --> 2192.42]  Ik las in de Volkskrant dat het ministerie van Economische Zaken 13,5 miljoen euro uittrekt voor de bouw van GPT-NL.
[2193.10 --> 2196.06]  Niet echt een fantasievolle rij, een fantasievolle naam.
[2196.56 --> 2200.06]  Een AI-taalmodel dat als basis kan dienen voor chatbots.
[2200.42 --> 2203.92]  En dat wordt gemaakt door onder andere TNO, door de NFI en SURF.
[2203.92 --> 2207.66]  En zij gaan dus een eigen taalmodel trainen.
[2207.80 --> 2213.84]  Waarbij het ongemak wat nu bestaat met taalmodellen uit Amerika van Big Tech.
[2214.22 --> 2217.20]  Die getraind zijn op allerlei trainingsdaten waarvan we helemaal niet weten.
[2217.88 --> 2218.36]  Ja, één.
[2218.44 --> 2221.24]  We weten niet wat voor data in zit, wat voor biases daarin zitten.
[2221.24 --> 2227.10]  Dat is een probleem als je AI serieus wil gaan gebruiken voor bijvoorbeeld overheidsdoelstellingen.
[2227.16 --> 2230.24]  Dan wil je niet gebruik maken van zo'n taalmodel waarbij je dat niet kan herleiden.
[2231.08 --> 2233.00]  Helemaal niet als je automatische beslissingen gaat nemen.
[2233.38 --> 2234.40]  Wat dan waarschijnlijk gaat gebeuren.
[2235.84 --> 2239.36]  Daarnaast is helemaal niet duidelijk of die rechten wel geregeld zijn.
[2239.98 --> 2245.28]  Dus hij is al ongetwijfeld in het OpenAI heeft allerlei data naar binnen geslurpt.
[2245.36 --> 2246.98]  Waar ze helemaal de auteursrechten niet over hebben.
[2247.50 --> 2249.20]  Dat willen ze in Nederland dus beter gaan doen.
[2249.20 --> 2253.94]  En het idee is dan ook dat dit gaat helpen om AI talent in Nederland te houden.
[2254.10 --> 2258.92]  In plaats van dat die naar grote Amerikaanse of Chinese techbedrijven gaan.
[2260.72 --> 2266.20]  En dat zal niet in eerste instantie denk ik beter worden dan JetGPT.
[2266.96 --> 2272.00]  Dus het zal een beetje tegen gaan vallen waarschijnlijk als ze eenmaal dit getraind hebben met 13,5 miljoen.
[2272.68 --> 2274.58]  Wat de functionaliteit daarvan is.
[2275.84 --> 2276.16]  Maar...
[2276.16 --> 2278.76]  Misschien al genoeg geld om alleen maar dat cluster te moeten huren.
[2278.76 --> 2279.62]  Om het te trainen.
[2280.98 --> 2281.72]  Energiegebruik bedoel je.
[2281.90 --> 2285.82]  Ja, het schijnt echt gigantisch te zijn om zo iets te doen.
[2286.50 --> 2288.14]  Maar goed, dat is moeilijk in te schatten.
[2288.36 --> 2290.54]  Ik ben wel meteen benieuwd en ik kan dat niet vinden.
[2291.44 --> 2298.36]  Of, kijk, als het is chat.reisoverheid.nl en inloggen met je digi-daze.
[2298.44 --> 2301.58]  Dat je kan praten met een model gemaakt met geld van de overheid.
[2301.96 --> 2302.72]  Ik hoop het niet.
[2302.90 --> 2304.22]  Ik wil dit ding op Hugging Face.
[2304.22 --> 2306.50]  Dat ik hem kan downloaden, toch?
[2306.50 --> 2308.10]  Ja, maar ik denk wel dat ze dat gaan doen, hoor.
[2308.84 --> 2309.90]  Ja, want dan is het echt vet.
[2309.96 --> 2314.42]  Want als je hem dan ook commercieel mag gebruiken, dan kan je hem ook gewoon als bedrijf inzetten.
[2314.56 --> 2317.18]  En zeggen, joh, binnen onze...
[2317.18 --> 2319.68]  Ja, als we met taalmodellen werken, werken we met dit model.
[2319.68 --> 2322.02]  Ja, ik ga me verbazen, hoor, als dat niet zo is.
[2322.10 --> 2323.42]  Want dit is natuurlijk de gedachte erachter.
[2323.70 --> 2324.58]  Dat het...
[2324.58 --> 2326.60]  Ja, het is ook non-profit, overheidsgeld.
[2326.74 --> 2331.72]  Dus je gaat ervan uit dat dat hele model dan gewoon netjes in de ranking op Hugging Face terechtkomt.
[2331.88 --> 2335.46]  Maar ik heb dus wel eens gelezen dat wij echt best wel veel supercomputerkracht hebben.
[2335.74 --> 2336.28]  Dat Nederland.
[2337.08 --> 2343.14]  Dat wij gewoon best wel veel van die videokaartjes ergens hebben staan.
[2343.14 --> 2346.42]  Ja, maar misschien is dan mijn opmerking wel heel erg naïef.
[2346.90 --> 2350.30]  Misschien is dat helemaal niet meegenomen in het budget, want dat hoeft niet, want het is al betaald.
[2350.78 --> 2352.78]  Die dingen staan nu niks te doen, dat denk ik niet, hoor.
[2352.90 --> 2354.86]  Maar die staan weer modellen te berekenen.
[2355.06 --> 2355.94]  Ik heb echt geen idee.
[2356.08 --> 2358.18]  Nee, maar het is een ander budget waarschijnlijk van het NFI.
[2358.84 --> 2360.92]  Ja, die betaalde überhaupt al stroomkosten.
[2360.92 --> 2364.54]  Niet dat dat opeens van dit budget af moet.
[2364.62 --> 2366.18]  Nee, waarschijnlijk gaat dit dan naar mensen.
[2367.32 --> 2367.40]  Ja.
[2367.88 --> 2370.68]  Kun je in de wereld dan weer tien mensen voor aannemen?
[2370.86 --> 2371.50]  Nou, gefeliciteerd.
[2371.50 --> 2373.94]  Ja, ja.
[2375.22 --> 2376.68]  Een jaartje bezig houden.
[2377.44 --> 2380.42]  Er zijn meer landen in Europa die ermee bezig zijn, toch?
[2380.50 --> 2381.64]  Zweden, begreep ik.
[2382.10 --> 2383.78]  En de Duitsers zijn er ook mee bezig, volgens mij.
[2384.10 --> 2387.42]  Ja, en het idee dan dat dit één groot Europees taalmodel gaat worden.
[2387.48 --> 2388.20]  Hoe werkt zoiets?
[2388.20 --> 2391.62]  Hoe kun je uit verschillende taalmodellen één taalmodel maken?
[2392.80 --> 2394.06]  Ja, je hebt die...
[2394.06 --> 2397.20]  Sowieso kan je ze natuurlijk koppelen door ze simpelweg...
[2397.20 --> 2400.98]  Of simpelweg, dat is helemaal niet simpel, maar je kunt ze laten samenwerken in groepsverband.
[2401.50 --> 2406.88]  Net als de EU zelf, dat je een soort federatie van modellen maakt...
[2406.88 --> 2408.52]  waarbij je met een metamodel praat.
[2408.66 --> 2411.38]  En niet het metabedrijf, maar metat hangt erboven, zeg maar.
[2411.48 --> 2412.06]  Oké.
[2412.06 --> 2414.34]  En die zijn allemaal gespecialiseerd in één ding of zo?
[2414.34 --> 2414.46]  Ja.
[2415.28 --> 2416.62]  Ja, en dan misschien dat die...
[2416.62 --> 2419.62]  Je wil eigenlijk misschien een soort proxy model hebben die zegt...
[2419.62 --> 2424.00]  Ik merk dat dit heel erg gaat over een context waar ik van weet dat model A...
[2424.00 --> 2425.00]  Ja, precies.
[2425.00 --> 2428.42]  Het is het GPTNL misschien geschikter is om dit antwoord te geven.
[2428.70 --> 2430.38]  Ik mis dat sowieso nog een beetje in...
[2430.38 --> 2434.00]  Ik weet dat sommige mensen het zelf bouwen, zeg maar, met open source modellen.
[2434.88 --> 2436.84]  Ja, meer een soort groepsproces.
[2436.96 --> 2440.24]  Dus dan creëer je een aantal entiteiten die je ook namen geeft.
[2440.78 --> 2442.10]  Je eigen persona, zeg maar.
[2442.52 --> 2444.30]  De wiskundige, de geschiedkundige.
[2444.48 --> 2446.40]  Dat doe je allemaal met taalmodellen op je eigen machine.
[2446.96 --> 2448.90]  En die groep laat je met elkaar discussiëren.
[2449.00 --> 2450.26]  En jij leest het dialoog mee.
[2450.40 --> 2453.38]  Of je leest het transcript mee van een discussie tussen de verschillende...
[2453.38 --> 2457.70]  semi-entiteiten die je hebt gecreëerd.
[2457.80 --> 2460.78]  En ik kan me voorstellen, dat is dus eigenlijk binnen één taalmodel...
[2460.78 --> 2462.84]  Je laat je een soort toneelstuk opvoeren.
[2462.94 --> 2465.74]  Zodat jij het kan volgen als mens hoe iets tot een conclusie komt.
[2466.28 --> 2470.74]  Maar in dit geval zou je die persona's echt kunnen koppelen aan onderliggende verschillende modellen.
[2470.88 --> 2471.92]  Kan ik me zo voorstellen.
[2473.06 --> 2476.36]  Het idee voor het Nederlands model is dus ook dat er verschillende...
[2476.36 --> 2480.92]  Dat het transparanter is in de zin van hoe het tot tekst komt.
[2480.94 --> 2482.16]  Ja, misschien is dat nog wel belangrijker.
[2482.16 --> 2482.88]  Ja.
[2483.16 --> 2484.78]  Toch? Wat erin ging aan is.
[2485.94 --> 2487.86]  Ja, precies. Ik moet eraan denken door wat je zegt hoor.
[2488.58 --> 2497.54]  Dat het een punt op zich wordt dat het herleidbaar is hoe hij tot tekst komt.
[2497.90 --> 2500.64]  Om biases te vermijden.
[2501.04 --> 2505.88]  En in het geval van overheidsbeslissingen wil je gewoon dat dat transparant is...
[2505.88 --> 2507.14]  voordat je AI gaat gebruiken.
[2507.28 --> 2509.94]  Je wil niet dat het een black box wordt en dat we alleen maar de output krijgen.
[2509.94 --> 2513.48]  Ja, en ik begrijp ook dat er best wel wat is.
[2513.54 --> 2516.56]  Want ik weet dat in Adobe Firefly waar we het wel eens over hebben gehad...
[2516.56 --> 2523.18]  in ieder geval vanaf het begin met het trainen van hun afbeelding, generatie, generating, algoritme...
[2523.18 --> 2527.74]  dat daar in ieder geval gezorgd is dat ze kunnen herleiden uit hun stockdataset van foto's.
[2527.78 --> 2531.60]  Joh, het is een foto van Alexander geweest die heeft bijgedragen aan dit eindproduct.
[2531.78 --> 2532.98]  Dus Alexander krijgt een credit.
[2533.22 --> 2534.08]  Ik denk het maar even simpel.
[2534.08 --> 2538.62]  Maar ja, dus er is royalties worden betaald, ook al zijn het maar dubbeltjes.
[2539.72 --> 2542.70]  Dat je mogelijk ook als je dit vanaf het begin goed opzet, zo'n model...
[2543.52 --> 2547.00]  veel sterker en beter kunt herleiden van...
[2547.00 --> 2550.46]  hé, we hebben hier nu een eindproduct.
[2550.62 --> 2551.56]  Noem het een appeltaart.
[2551.74 --> 2554.42]  Maar we kunnen ook kijken waar de appels vandaan komen die erin zitten.
[2554.62 --> 2554.80]  Juist.
[2554.88 --> 2555.70]  Je hebt uiteindelijk...
[2555.70 --> 2558.38]  En de mail komt uit die en die en die hoek.
[2559.08 --> 2560.08]  Ik weet niet precies...
[2560.08 --> 2561.44]  Ik weet dat er veel onderzoek mee is.
[2561.44 --> 2563.26]  Ik heb geen idee hoe dit dan uiteindelijk werkt.
[2563.34 --> 2567.12]  Maar ik kan me voorstellen dat als je dit vanaf het begin als een van je kernwaarden hebt...
[2567.12 --> 2569.90]  dus we willen een herleidbaar model...
[2569.90 --> 2576.20]  dat dat een heel ander iets oplevert dan wat tot nu toe Antropic en OpenAI hebben gedaan.
[2576.36 --> 2581.68]  Want het cynisme wat ik een beetje hoor in de verschillende media die ik tot me neem...
[2581.68 --> 2587.14]  is volgens mij vindt OpenAI het wel lekker dat ze niet kunnen herleiden waar het te gaan komt.
[2587.34 --> 2589.74]  Dat is voor hun juridisch eigenlijk gunstiger.
[2589.74 --> 2591.40]  En dit klinkt als een conspiretie.
[2591.50 --> 2595.20]  Maar goed, dat mag je dan in het midden laten hoe erg conspiretie het is.
[2595.32 --> 2600.06]  Maar het is voor hun eigenlijk niet zo gunstig om hun eigen modellen helemaal te gaan openmaken...
[2600.06 --> 2600.92]  als een soort uilenballetje.
[2600.98 --> 2602.24]  Want het geeft alleen maar problemen.
[2602.36 --> 2603.54]  Ja, juridische problemen.
[2603.72 --> 2603.90]  Ja.
[2603.90 --> 2607.90]  En het zou dus kunnen dat dit Nederlands model...
[2608.42 --> 2611.84]  We weten dit niet, maar dit zou zo'n specialisatieslag kunnen zijn.
[2611.90 --> 2614.60]  En ik kan me dus ook voorstellen dat er talent voor te werven is.
[2614.68 --> 2615.90]  Die dan denken, oké...
[2617.10 --> 2619.54]  Dus mensen die veel gevraagd zijn in deze wereld...
[2619.54 --> 2621.90]  dat ze dan ook denken, oké, dit is een manier om bij te dragen.
[2621.98 --> 2625.18]  Want dit is heel belangrijk, een soort van accountability in AI.
[2625.18 --> 2625.98]  Oké.
[2626.48 --> 2629.84]  Ik ben dan wel heel benieuwd wat de brondata is...
[2629.84 --> 2631.52]  waar erop getraind gaat worden.
[2632.06 --> 2635.24]  Want bij OpenAI, die trekken gewoon alles leeg...
[2635.24 --> 2638.74]  waar geen no-robot op zit.
[2639.24 --> 2641.46]  Kan je wel een goed model maken als je eerlijk bent.
[2641.62 --> 2642.22]  Ja, precies.
[2642.62 --> 2644.24]  Maar dat doet mij dus denken...
[2644.24 --> 2648.18]  zou het dan niet nu een vorm van maatschappelijk verantwoord ondernemer zijn voor bedrijven...
[2648.18 --> 2651.34]  om hun content te doneren aan dit model.
[2651.42 --> 2652.78]  Voor volk en vaderland.
[2653.20 --> 2656.50]  Dus dat jij DPG bent, of je bent MediaHuis...
[2656.50 --> 2659.04]  of je bent, nou wat hebben we nog meer als Nederland, NPO.
[2659.58 --> 2662.70]  Gewoon alle content van de NPO getranscribeerd.
[2663.48 --> 2666.00]  Of, nou wat hebben we, we hebben beeld en geluid.
[2666.10 --> 2667.66]  Maar goed, zijn we allemaal andere rechthebbenden.
[2668.60 --> 2671.18]  Eigenlijk vind ik dus dat gewoon alle...
[2672.10 --> 2675.18]  We moeten nu dan iedereen de handen op elkaar krijgen...
[2675.18 --> 2679.64]  om dit Nederlandse alternatief wat open source is...
[2679.64 --> 2680.88]  en weet je, voor de mensen.
[2681.76 --> 2685.48]  En niet met een winstoogmerk en allerlei andere belangen...
[2685.48 --> 2686.80]  als die Amerikaanse bedrijven hebben.
[2687.12 --> 2689.26]  Moeten we dit nu ook wel echt gaan omarmen met z'n allen.
[2689.34 --> 2690.90]  En dan vind ik ook dat die bedrijven moeten laten zien...
[2690.90 --> 2694.48]  we geven het niet aan Google, we geven het niet aan OPI.
[2694.98 --> 2698.10]  We geven het wel aan dit sympathieke initiatief.
[2698.40 --> 2701.18]  Nou, ik denk dat gezien het feit dat we er langzaam achter komen...
[2701.18 --> 2707.30]  wat er allemaal in de GPT-modellen van OpenAI terechtgekomen is...
[2707.30 --> 2710.94]  aan content die gescreept is van Nederlandse websites, onder andere.
[2711.68 --> 2715.18]  Zou je natuurlijk kunnen zeggen, dat gaan we er of uit laten halen...
[2716.46 --> 2717.54]  Ja, dat kan niet denk ik.
[2717.96 --> 2719.18]  Of, we gaan sowieso...
[2719.70 --> 2720.78]  Kan waarschijnlijk niet eens, nee.
[2720.84 --> 2722.02]  Kan niet, want het zit te diep.
[2723.38 --> 2725.68]  Daarmee, ik geef meteen maar twee opties.
[2726.46 --> 2728.18]  De tweede optie is, oké, alles wat daarin zit...
[2728.18 --> 2730.14]  moet ook in dit model kunnen komen.
[2730.74 --> 2732.18]  Het is heel raar als je nu zegt...
[2733.14 --> 2735.18]  mijn content zit al in een model van OpenAI...
[2736.02 --> 2738.14]  maar ik wil niet dat het terechtkomt in het model...
[2738.14 --> 2740.78]  wat de Nederlandse overheid met allemaal instanties probeert te maken.
[2741.00 --> 2742.32]  Daar zit al een soort consent in.
[2742.44 --> 2744.94]  Wat zij al hebben gejat van je, kan je dan net zo goed ook laten jatten.
[2745.52 --> 2746.86]  Dat is stap één van mijn...
[2746.86 --> 2750.02]  Ik vind het een bijzonder Robin Hood-achtige juridische redenering.
[2750.02 --> 2753.18]  Nee, maar dan kom ik langs en dan zeg je van...
[2753.18 --> 2754.18]  Ja, ik had per ongeveer...
[2754.18 --> 2756.18]  Zij doen het ook, argument.
[2756.18 --> 2759.68]  Nou ja, het is een beetje een laf argument.
[2759.86 --> 2763.18]  Maar ik vind dan, als jij je robots TXT niet goed had staan...
[2763.96 --> 2766.66]  of je had hem wel goed staan en OpenAI heeft het gejat...
[2766.66 --> 2768.44]  dan moet je of achter je content aan...
[2768.44 --> 2770.28]  of ook toestaan dat het hier gebruikt mag worden.
[2770.72 --> 2773.40]  En stap drie, die jij nu beschrijft...
[2773.40 --> 2775.22]  Ik weet niet waar die drie ineens vandaan komt...
[2775.22 --> 2776.64]  maar dat is niet...
[2776.64 --> 2779.18]  Een volgende stap na die twee opties is...
[2779.92 --> 2781.66]  ik ga ook content beschikbaar stellen...
[2781.66 --> 2783.24]  waar OpenAI niet bij kon.
[2783.24 --> 2785.00]  En ik ga het bewust opengooien.
[2785.04 --> 2786.04]  Ja, het bedrijf dat dit...
[2786.04 --> 2788.04]  Het media bedrijf of ander content bedrijf...
[2788.04 --> 2790.44]  het Nederlands, die dit als eerste gaat doen...
[2790.44 --> 2791.94]  die verdient wel echt een standbeeldje.
[2792.26 --> 2793.36]  Een AI-standbeeldje.
[2794.20 --> 2795.36]  Die gaan wij dan oprichten.
[2795.68 --> 2797.36]  Kopen we een stuk grond in een weiland...
[2797.36 --> 2798.52]  en dan zetten we een standbeeld neer...
[2798.52 --> 2800.14]  in de vorm van het logo van dit bedrijf.
[2800.20 --> 2800.64]  Zullen we dat doen?
[2801.12 --> 2804.02]  Ja, als dat standbeeldje dan ook generft kan worden daarna...
[2804.02 --> 2804.70]  Ja, zeker!
[2804.70 --> 2809.14]  Ik wil dat standbeeldje vanuit alle oogpunten...
[2809.14 --> 2810.78]  en hoekpunten en belichtingen kunnen zien.
[2810.96 --> 2812.14]  Dat kan, dat kan.
[2812.90 --> 2814.24]  Waar wil je het verder over hebben, Witsen?
[2814.68 --> 2815.70]  Nou, ik had...
[2816.42 --> 2820.70]  Ik werk veel met GPT dagelijks eigenlijk...
[2821.56 --> 2824.52]  om de scriptjes te schrijven die ik nodig heb.
[2824.66 --> 2826.02]  Dat heb ik er wel eens eerder over verteld.
[2826.18 --> 2827.94]  Ik zie het niet als een hele goede programmeur.
[2828.42 --> 2830.76]  Maar A is B naar C naar D...
[2830.76 --> 2832.26]  in een soort lineair verhaaltje maken...
[2832.26 --> 2832.70]  van een soort kettingreactie...
[2832.70 --> 2834.62]  die kettingreactie kan niet echt supergoed.
[2836.12 --> 2839.02]  En ik test natuurlijk ondertussen ook andere dingen.
[2839.14 --> 2840.38]  Want wat je nu een beetje merkt...
[2840.38 --> 2842.76]  is dat GPT-4 is ontzettend...
[2842.76 --> 2845.72]  in wat we nu weten en kunnen vergelijken gewoon goed.
[2845.98 --> 2847.84]  Ik test van alles de hele tijd...
[2847.84 --> 2849.44]  en iedere keer ga ik weer terug naar GPT-4...
[2849.44 --> 2850.84]  omdat ik denk, ja, sorry man...
[2850.84 --> 2852.90]  dit is gewoon nu wat ik merk...
[2852.90 --> 2854.52]  voor mij het meeste waarde biedt...
[2854.52 --> 2856.96]  voor mijn vragen en opdrachten die ik geef.
[2857.68 --> 2861.52]  Maar er wordt natuurlijk heel veel specifieker getraind.
[2861.52 --> 2867.52]  Want de taalmodellen van OpenAI zijn heel erg breed...
[2868.52 --> 2870.44]  en meer dan niet oppervlakkig.
[2870.54 --> 2872.18]  Soms wel hoor, qua inhoud.
[2872.28 --> 2873.32]  Het is geen feitenmachine.
[2873.50 --> 2874.90]  Dat weet iedereen inmiddels, denk ik wel.
[2875.02 --> 2876.66]  Het is niet op zoek gaan naar feiten.
[2876.74 --> 2879.02]  Het is op zoek gaan naar theorieën en patronen...
[2879.02 --> 2880.90]  en dan de feiten gaan zoeken in de boeken...
[2880.90 --> 2881.80]  waar het uiteindelijk uitkomt.
[2881.88 --> 2883.96]  Want je weet nooit wat er gehallucineerd wordt.
[2884.46 --> 2886.30]  Vragen wanneer jij geboren bent of zo.
[2886.70 --> 2888.12]  Waarschijnlijk kan die het een beetje gokken.
[2888.12 --> 2892.28]  Maar ja, dan moet je de feature die er nu in zit...
[2892.28 --> 2894.20]  zag ik search with Bing aanzetten.
[2894.30 --> 2896.16]  En dan kan die het gaan opzoeken op jouw website.
[2896.32 --> 2896.96]  Dat is wat anders.
[2897.46 --> 2901.52]  Maar goed, je ziet dus nu meer gespecialiseerde...
[2901.52 --> 2903.12]  noem het expertmodellen ontstaan.
[2903.86 --> 2905.12]  Waarbij bedrijven zeggen...
[2905.12 --> 2905.92]  oké, wacht even.
[2906.06 --> 2909.12]  Het gaat ons niet lukken om zoiets groots en moois neer te zetten...
[2909.12 --> 2910.16]  als wat OpenAI heeft gedaan.
[2910.28 --> 2912.56]  Want we hebben de serverinstructuur niet.
[2912.90 --> 2913.64]  De mensen niet.
[2915.20 --> 2916.84]  En we hebben nog X en Y niet.
[2917.24 --> 2918.54]  Maar wat we wel kunnen doen...
[2918.54 --> 2921.12]  is op hele specifieke datasets met een doel...
[2921.88 --> 2924.86]  een model maken die heel erg goed is in A of B of C.
[2925.34 --> 2927.62]  Een voorbeeld daarvan is find.com.
[2927.70 --> 2930.62]  En dat schrijf je met ph zoals je foto schrijft in het Engels.
[2931.16 --> 2933.42]  Dus niet find, maar phind.com.
[2933.42 --> 2935.20]  En wat die al een hele tijd doen...
[2935.20 --> 2937.18]  is trainen op Stack Overflow.
[2937.46 --> 2939.42]  Omdat, dit is een beetje een grapje onder veel ontwikkelaars...
[2940.04 --> 2942.58]  maar de copy-paste ontwikkelaars...
[2942.58 --> 2943.52]  ben ik er ook een van.
[2943.74 --> 2945.42]  Die zoeken vaak gewoon via Google...
[2945.42 --> 2948.28]  en dan uitroepteken stackoverflow.com als het ware URL.
[2949.28 --> 2950.42]  Antwoorden op hun programmeervragen.
[2951.24 --> 2952.42]  Ja, Stack Overflow is een soort forum...
[2953.12 --> 2954.22]  waar programmeurs heen kunnen gaan.
[2954.32 --> 2955.98]  Als ze vastlopen, kunnen ze vragen...
[2955.98 --> 2956.70]  hoe moet ik dit oplossen?
[2956.80 --> 2958.20]  En dan zijn er andere mensen in die community...
[2958.20 --> 2960.16]  die dan dat probleem soms voor ze oplossen.
[2960.22 --> 2961.76]  Ja, het is een soort QOra...
[2961.76 --> 2964.22]  alleen dan minder spammy en inhoudelijk gewoon beter.
[2964.46 --> 2965.90]  Nou, heel fijn deze metafor...
[2965.90 --> 2967.10]  nu begraat iedereen het wie het zet.
[2967.12 --> 2967.98]  QOra, toch jongens?
[2969.98 --> 2972.36]  Het oude Fokforum, alleen dan voor technologie.
[2972.36 --> 2972.92]  Fokforum, ja.
[2973.06 --> 2973.98]  Fokforum voor technologie.
[2974.42 --> 2975.98]  Eigenlijk is het een soort nos.nl...
[2976.64 --> 2977.64]  maar dan voor programmeurs.
[2978.00 --> 2978.18]  Ja.
[2978.96 --> 2980.68]  Het is een soort lokale bakkerij...
[2980.68 --> 2981.98]  maar dan gewoon geen bakkerij...
[2981.98 --> 2982.82]  maar dan gewoon een website.
[2984.04 --> 2984.48]  Oké, hou op.
[2984.48 --> 2984.74]  Sorry.
[2986.36 --> 2987.98]  Wat kan je doen op find.com?
[2987.98 --> 2988.98]  Er staat gewoon een tekstinputveld...
[2989.58 --> 2990.36]  zoals je dat gewend bent.
[2990.36 --> 2992.20]  Daar typ je jou je programmeervraag in...
[2992.20 --> 2993.70]  maar die kan je intypen op een manier...
[2993.70 --> 2995.90]  zoals je dat normaal naar een taalmodel doet.
[2995.98 --> 2996.28]  Ja, ja.
[2996.62 --> 2998.42]  En dan krijg je antwoord...
[2998.42 --> 3000.56]  en die antwoorden zijn echt beter geworden...
[3000.56 --> 3003.34]  want ze hebben het model een week of anderhalf week geleden weer geüpdate.
[3003.98 --> 3006.82]  Dus getraind op alles wat ze openbaar kunnen vinden...
[3006.82 --> 3010.98]  rondom het thema programmeren en technologie.
[3011.78 --> 3014.26]  Maar ja, veel meer toegespitst van...
[3014.26 --> 3016.32]  wat voor antwoorden wil een programmeur eigenlijk?
[3016.56 --> 3018.82]  Ja, vaak wil je die stukjes scripten bijvoorbeeld...
[3018.82 --> 3020.22]  gewoon altijd netjes in beeld...
[3020.22 --> 3021.30]  dat je ze kunt kopiëren...
[3021.30 --> 3023.18]  en je wil misschien alleen maar het functietje wat verandert.
[3023.36 --> 3024.44]  En ik moet zeggen...
[3024.44 --> 3025.82]  ik gebruik ze nu door elkaar.
[3025.96 --> 3028.50]  De ene keer krijg ik een beter antwoord van GPT-4...
[3028.50 --> 3030.84]  maar de andere keer krijg ik een beter antwoord van Find.
[3031.32 --> 3032.42]  En dat vind ik interessant.
[3032.42 --> 3036.06]  meer als een soort opmerking van...
[3036.06 --> 3039.92]  oké, natuurlijk krijgen die ontwikkelaars als eerst hun expertmodellen.
[3040.32 --> 3040.78]  Natuurlijk.
[3041.44 --> 3043.32]  Want het is heel vaak in de IT...
[3043.32 --> 3046.74]  dat al dit soort automatisering en de eerste start-up...
[3046.74 --> 3048.52]  zijn voor mede-techies.
[3049.20 --> 3053.56]  Dus, maar Define.com kan je natuurlijk ook maken...
[3053.56 --> 3054.80]  op veel specifiekere zaken.
[3054.94 --> 3058.30]  Nou, noem het geestelijke gezondheid, onderwijs.
[3058.30 --> 3059.74]  En ik denk dus...
[3059.74 --> 3062.74]  Eigenlijk zei je dat een beetje in het vorige onderwerp al...
[3062.74 --> 3064.96]  van wat nou als we databanken gaan opengooien...
[3064.96 --> 3065.94]  die niet publiek waren.
[3066.94 --> 3068.72]  Dat je natuurlijk kan gaan kijken van...
[3068.72 --> 3071.08]  oké, we hebben hier misschien wel...
[3071.08 --> 3074.98]  50 jaar discours rondom onderwerp I...
[3074.98 --> 3077.22]  wat we in een hele mooie database hebben zitten al...
[3077.22 --> 3078.56]  gedigitaliseerd, want dat moest.
[3079.12 --> 3081.46]  Zullen we eens een model gaan trainen op dat onderwerp...
[3081.46 --> 3083.28]  en dan helemaal freaken daarop.
[3083.62 --> 3084.74]  En dat je bijvoorbeeld zegt...
[3085.28 --> 3087.10]  de data die erin zit is dus...
[3087.10 --> 3089.46]  de kwaliteit is hoger, er is minder ruis omheen...
[3089.46 --> 3092.46]  want het wordt dus een model dat...
[3092.46 --> 3095.68]  niet zo intelligent is...
[3095.68 --> 3097.76]  op het merendeel van de onderwerpen...
[3097.76 --> 3099.52]  maar op één of twee onderwerpen...
[3099.52 --> 3100.24]  een expert is.
[3100.50 --> 3102.32]  Maar wat ik niet begrijp hieraan is...
[3102.32 --> 3104.46]  hoe kan dit beter zijn dan...
[3105.64 --> 3107.52]  Kijk, dat het beter kan zijn dan...
[3107.52 --> 3109.30]  JGPT begrijp ik ergens nog wel...
[3109.30 --> 3111.16]  ondanks dat allebei waarschijnlijk getraind zijn...
[3111.16 --> 3112.10]  op dezelfde dataset...
[3112.10 --> 3114.10]  want het lijkt me dat OpenAI ook gewoon...
[3114.10 --> 3115.48]  Stack Overflow heeft leeggetrokken.
[3116.18 --> 3118.32]  Maar vooral als ik denk aan...
[3119.32 --> 3120.70]  co-pilot van...
[3120.70 --> 3121.02]  wat is het?
[3121.06 --> 3121.30]  GitHub.
[3121.98 --> 3122.80]  Dan zou ik zeggen...
[3122.80 --> 3124.20]  daar zit ook nog eens een keer...
[3124.20 --> 3125.70]  een feedback loop in.
[3125.82 --> 3127.32]  Want daar zoek je op...
[3128.10 --> 3129.32]  allerlei AI-code...
[3129.32 --> 3131.32]  en GitHub weet...
[3131.98 --> 3133.04]  of je dan vervolgens die code...
[3133.04 --> 3134.22]  implementeert ja of nee.
[3134.72 --> 3136.98]  Dus als het antwoord ja is...
[3136.98 --> 3139.16]  dan zou je dat als signaal kunnen interpreteren...
[3139.16 --> 3141.06]  dat dat dus een goede suggestie was...
[3141.06 --> 3142.32]  en dat is een soort feedbackmechanisme...
[3143.20 --> 3147.50]  om die AI relevanter te maken.
[3147.60 --> 3149.26]  Lijkt mij, zeg ik, als week.
[3150.24 --> 3151.74]  Maar hoe kan het dan...
[3151.74 --> 3152.32]  dat zo'n systeem...
[3153.32 --> 3154.54]  dat dan alleen maar zoeken is...
[3154.54 --> 3156.22]  en verder niet weet hoe je het implementeert...
[3156.22 --> 3157.66]  hoe kan dat ooit beter zijn...
[3157.66 --> 3159.08]  dan zo'n geïntegreerde...
[3159.08 --> 3160.04]  Nou, ik denk dat...
[3160.04 --> 3160.88]  dat één ding al is...
[3160.88 --> 3161.74]  dat je natuurlijk kan zeggen...
[3161.74 --> 3162.82]  in het geval van Find...
[3162.82 --> 3163.90]  ik weet het niet precies hoor...
[3163.90 --> 3165.86]  maar we gaan iedere twee weken updaten.
[3166.36 --> 3168.02]  We gaan iedere twee weken opnieuw trainen.
[3168.08 --> 3169.02]  Dat is veel te duur.
[3170.96 --> 3171.52]  Meer data?
[3172.32 --> 3174.00]  Nee, dus het is dezelfde data...
[3174.00 --> 3174.92]  alleen het is...
[3174.92 --> 3176.92]  in het geval van programmeervragen...
[3176.92 --> 3177.84]  wil je natuurlijk eigenlijk...
[3177.84 --> 3178.84]  de data van vandaag.
[3180.08 --> 3180.92]  Zeg maar de OpenAI...
[3181.50 --> 3182.44]  traint nu volgens mij...
[3182.44 --> 3183.28]  iedere kwartaal.
[3183.46 --> 3185.22]  Wat bedoel je met de data van vandaag?
[3185.22 --> 3188.18]  Antwoorden op vragen...
[3188.18 --> 3189.10]  rondom programmeren...
[3189.10 --> 3190.60]  die gisteren zijn gegeven door iemand...
[3190.60 --> 3191.62]  of vanochtend om tien uur.
[3191.74 --> 3192.66]  Die staan niet in...
[3192.66 --> 3194.60]  Dat forum blijft ook de hele tijd...
[3194.60 --> 3195.68]  Ja, dat is een gegevense fire.
[3195.68 --> 3196.74]  Er zijn nog mensen...
[3196.74 --> 3198.00]  die naar dat forum gaan...
[3198.00 --> 3199.16]  om daar vragen en antwoorden te geven.
[3199.18 --> 3200.62]  Ja, ja, dat is echt een firehose.
[3200.78 --> 3202.68]  Het is niet zo snel als Twitter, denk ik.
[3202.92 --> 3204.22]  Maar dit is sowieso...
[3205.98 --> 3207.22]  op het moment dat jij...
[3207.22 --> 3208.92]  keuzes gaat maken...
[3208.92 --> 3210.86]  wat je wel of niet in je model gaat stoppen...
[3210.86 --> 3211.22]  om te trainen...
[3211.88 --> 3212.44]  of in ieder geval...
[3212.44 --> 3214.22]  in je trainingsdataset gaat stoppen...
[3214.22 --> 3216.46]  om je uiteindelijke model te trainen...
[3216.46 --> 3217.78]  dan kan je vaker trainen...
[3217.78 --> 3218.96]  want een klein model trainen...
[3218.96 --> 3220.58]  is goedkoper dan een groot model trainen.
[3220.92 --> 3221.60]  Dus daar heb je al...
[3221.60 --> 3222.82]  voordeel nummer één is...
[3222.82 --> 3224.34]  dat je wekelijks of tweewekelijks...
[3224.34 --> 3225.12]  kunt gaan updaten.
[3225.82 --> 3227.14]  Want dat is nu...
[3227.14 --> 3228.64]  Jij zei tegen mij net van...
[3228.64 --> 3230.18]  ik wilde iets vinden op Reddit...
[3230.18 --> 3230.72]  en toen zei je...
[3230.72 --> 3231.54]  ja, je moet inloggen...
[3231.54 --> 3232.88]  want die gasten hebben alles dichtgegooid...
[3232.88 --> 3234.16]  net als Twitter nu heeft gedaan.
[3234.56 --> 3236.64]  Voor het tegen het scrapen van...
[3236.64 --> 3238.46]  Ja, dat zorgt er natuurlijk voor dat...
[3238.46 --> 3240.88]  want ik merk ook bij mezelf heel vaak...
[3240.88 --> 3242.20]  dat ik vergeet...
[3242.20 --> 3243.20]  hoe...
[3243.20 --> 3245.04]  niet up-to-date...
[3245.04 --> 3246.24]  chat GPT is.
[3246.48 --> 3248.50]  Ja, daar zit gewoon een knowledge cut-off op.
[3248.92 --> 3249.98]  Want het is ontzettend duur...
[3249.98 --> 3251.32]  om dat ding weer helemaal te trainen.
[3251.38 --> 3251.90]  Miljoenen.
[3252.24 --> 3253.88]  Er schiet ineens iets in mijn hoofd.
[3253.96 --> 3254.52]  Namelijk...
[3254.52 --> 3255.78]  ik heb ooit een verhaal gehoord...
[3255.78 --> 3257.82]  van iemand die zei...
[3257.82 --> 3259.10]  het is heel vervelend...
[3259.10 --> 3260.14]  als een goede medewerker...
[3260.14 --> 3261.38]  bij een bedrijf weggaat...
[3261.38 --> 3262.38]  want dan...
[3262.38 --> 3265.56]  dan verlies je veel kennis.
[3266.00 --> 3267.76]  Dus wat als je van iedere medewerker...
[3267.76 --> 3268.54]  een botje maakt...
[3268.54 --> 3269.50]  dus je traint...
[3269.50 --> 3271.84]  de e-mails die die persoon heeft gestuurd...
[3271.84 --> 3273.98]  en de chatberichten die die persoon heeft gestuurd...
[3273.98 --> 3276.36]  en alle files waar die persoon in geëdit heeft...
[3276.36 --> 3279.20]  en dat maak je helemaal tot een model...
[3279.20 --> 3280.38]  waardoor je tot in oneindigheid...
[3280.38 --> 3281.92]  kan blijven praten met die collega.
[3282.10 --> 3282.84]  Dus Henk is weg...
[3282.84 --> 3284.54]  maar je hebt Henk nog wel in je Slack.
[3285.70 --> 3287.02]  De botversie van Henk.
[3287.02 --> 3290.02]  En toen constateerde die...
[3290.02 --> 3291.96]  bij het testen hiervan...
[3291.96 --> 3293.82]  dat er nog wel fouten in zitten...
[3293.82 --> 3296.24]  dus dat robot Henk niet altijd goede antwoorden geeft...
[3296.24 --> 3298.62]  dus dat je dan de echte Henk...
[3298.62 --> 3299.96]  kan vragen om nog een half jaar...
[3299.96 --> 3301.40]  op een consultancy contract te zitten...
[3301.40 --> 3304.22]  zodat als de feedback van de medewerkers is negatief...
[3304.22 --> 3306.88]  namelijk dit is niet wat de echte Henk zou zeggen...
[3306.88 --> 3308.30]  op deze vraag...
[3308.30 --> 3310.50]  dat dan de vraag doorgespeeld wordt naar echte Henk...
[3310.50 --> 3312.08]  echte Henk geeft dan het echte antwoord...
[3312.08 --> 3313.56]  dat wordt weer gebruikt als basis...
[3313.56 --> 3314.76]  om dat model verder te trainen...
[3314.76 --> 3316.38]  waardoor Henk dan na zes maanden...
[3316.38 --> 3318.26]  daadwerkelijk met 80% of zo...
[3318.26 --> 3319.02]  overbodig is geworden.
[3319.02 --> 3320.70]  Nou, als je dat aanneemt...
[3320.70 --> 3324.42]  dat dit een soort van feedbackloop wordt...
[3324.42 --> 3328.50]  voor allerlei AI's...
[3328.50 --> 3330.30]  die gespecialiseerd zijn op één thema...
[3330.30 --> 3331.22]  jij noemde net...
[3331.22 --> 3331.60]  wat is het?
[3331.68 --> 3332.16]  Medies.
[3332.30 --> 3333.68]  En we hebben het dus nu over programmeurs.
[3334.40 --> 3336.78]  Waarom is het eigenlijk nog niet zo...
[3336.78 --> 3340.02]  dat de beste programmeurs...
[3340.90 --> 3343.70]  werken om modellen beter te maken...
[3343.70 --> 3345.02]  in plaats van...
[3345.02 --> 3346.70]  of nee...
[3346.70 --> 3347.88]  niet alleen programmeurs...
[3347.88 --> 3349.62]  ik bedoel eigenlijk de beste experts...
[3349.62 --> 3350.42]  op ieder onderwerp...
[3351.14 --> 3353.26]  dat die niet aan het werk zijn...
[3353.26 --> 3354.50]  om het model te maken...
[3354.50 --> 3355.72]  waar veel grotere hefboom...
[3355.72 --> 3357.60]  op hun minuten tijd zit...
[3357.60 --> 3359.06]  soort van de return on investment...
[3359.06 --> 3361.02]  is veel groter dan waarschijnlijk...
[3361.02 --> 3363.42]  het specifieke ding waar ze nu aan werken...
[3363.42 --> 3364.42]  maar dat je gewoon een soort van...
[3364.42 --> 3366.50]  toplaag krijgt in de bevolking...
[3366.50 --> 3368.76]  in de wereldbevolking...
[3368.76 --> 3369.52]  van mensen die...
[3369.52 --> 3370.46]  als je dan echt...
[3370.46 --> 3372.06]  het allerbeste bent in je vakgebied...
[3372.06 --> 3373.36]  dan ga je niet meer...
[3373.36 --> 3374.22]  gewoon je werk doen...
[3374.22 --> 3374.56]  weet ik veel...
[3374.56 --> 3375.60]  als je een dokter bent...
[3375.60 --> 3377.10]  opereren of onderzoek doen...
[3377.10 --> 3378.00]  of wat dan ook...
[3378.00 --> 3380.00]  maar alleen maar...
[3380.00 --> 3381.10]  een model...
[3381.10 --> 3383.06]  helpen fine-tune...
[3383.06 --> 3384.60]  en dat dat dan het onderscheidende...
[3385.82 --> 3386.50]  ding is.
[3386.78 --> 3388.36]  Dit is volgens mij een beetje wat...
[3388.36 --> 3389.12]  in de geschiedenis...
[3389.12 --> 3390.56]  dan heb je een protégé, toch?
[3390.60 --> 3391.92]  Dan heb je een student...
[3391.92 --> 3393.34]  en dan ga je dan één iemand...
[3393.34 --> 3393.92]  in dat geval...
[3393.92 --> 3396.02]  Dus jij bent echt een fantastische...
[3396.02 --> 3396.92]  hartchirurg...
[3396.92 --> 3398.12]  je gaat richting je pensioen...
[3398.12 --> 3399.10]  misschien gaan je handen zelfs...
[3399.10 --> 3399.92]  al een beetje trillen...
[3399.92 --> 3400.92]  en voel je dat...
[3400.92 --> 3401.44]  en dan denk je...
[3401.44 --> 3403.18]  ik moet nu die volgende generatie...
[3403.18 --> 3403.86]  gaan opleiden...
[3403.86 --> 3405.12]  want dit is gewoon...
[3405.12 --> 3407.34]  Dit is het nieuwe protégé, bedoel je?
[3407.62 --> 3408.26]  Ja, toch?
[3409.22 --> 3411.00]  Ja, want die dynamiek...
[3411.00 --> 3411.54]  hadden we al...
[3411.54 --> 3412.92]  maar ik denk wel...
[3413.50 --> 3414.96]  ik ga het toch weer een beetje cynisch doen...
[3414.96 --> 3415.52]  ik denk wel...
[3415.52 --> 3416.26]  ik ben benieuwd, hè...
[3416.26 --> 3417.92]  want wat ik een beetje zie...
[3418.74 --> 3419.92]  als dynamiek...
[3420.44 --> 3421.92]  is dat je bijvoorbeeld ziet dat...
[3422.64 --> 3422.92]  DJ's...
[3422.92 --> 3423.92]  niet allemaal...
[3424.60 --> 3426.92]  maar dat DJ's en andere artiesten...
[3427.52 --> 3430.20]  vaak nieuwe artiesten gaan scouten...
[3430.20 --> 3433.06]  zoals Dr. Dre ooit Eminem heeft gevonden...
[3433.06 --> 3433.48]  als het ware.
[3434.22 --> 3436.22]  En dan heb je eigenlijk...
[3436.22 --> 3439.10]  een aantal dynamieken die ik een aantal keer heb gezien...
[3439.10 --> 3440.70]  als ik dat een beetje zo zit te lezen...
[3440.70 --> 3442.42]  in biografieën en zo...
[3442.42 --> 3442.92]  is...
[3443.76 --> 3447.28]  je kunt iemand in jouw eigen stal toevoegen...
[3447.28 --> 3448.10]  zeg maar...
[3448.10 --> 3450.62]  omdat je die als protégé ziet en denkt...
[3450.62 --> 3452.34]  oké, mijn tijd is geweest...
[3452.34 --> 3453.76]  ik ben niet meer relevant...
[3453.76 --> 3454.40]  of...
[3454.40 --> 3456.08]  ik wil graag de nieuwe generatie helpen...
[3456.08 --> 3458.74]  en dan ga je die persoon coachen...
[3458.74 --> 3460.32]  en met de juiste mensen in contact brengen...
[3460.32 --> 3463.18]  en dan ontstaat daar misschien wel een artiest...
[3463.18 --> 3464.02]  groter dan jijzelf.
[3464.58 --> 3467.90]  En als je dat echt op een beetje zuivere...
[3467.90 --> 3470.48]  wat meer egoloze manier kan doen...
[3470.48 --> 3471.66]  dan vind je het ook niet erg...
[3471.66 --> 3473.48]  dat daar iemand groter ontstaat dan jijzelf.
[3473.60 --> 3475.76]  Dit is de positieve lezing van een aantal verhalen...
[3475.76 --> 3479.12]  verhalen die inmiddels zijn gebeurd met artiesten.
[3479.18 --> 3480.88]  Die zijn als het ware gevonden...
[3480.88 --> 3483.22]  erkend, geholpen, in het zadel geholpen...
[3483.22 --> 3485.14]  en voorbij aan hun eigen meesters gegroeid.
[3485.68 --> 3487.36]  Maar wat je ook wel eens ziet...
[3487.36 --> 3489.18]  is dat mensen juist op zoek gaan...
[3489.18 --> 3490.52]  naar hun volgende...
[3490.52 --> 3492.62]  en die juist altijd een beetje klein zullen houden...
[3492.62 --> 3493.24]  omdat ze denken...
[3493.24 --> 3494.80]  ja, ik kan maar beter...
[3494.80 --> 3496.62]  keep your friends close...
[3496.62 --> 3498.98]  but keep your future competitors closer.
[3499.50 --> 3501.52]  Ik ga zorgen dat ik ze in mijn stal hou...
[3501.52 --> 3503.74]  maar nooit helemaal op de podia laat draaien...
[3503.74 --> 3505.48]  waar ik zelf zou willen staan.
[3505.62 --> 3507.24]  Dat is de andere kant van die dynamiek.
[3507.82 --> 3509.72]  Dus ik vraag me af...
[3509.72 --> 3512.64]  of in dezezelfde dynamiek...
[3512.64 --> 3513.88]  zijn er misschien ook mensen die zeggen...
[3513.88 --> 3515.74]  ja, ik wil best één protege opleiden...
[3516.80 --> 3517.74]  maar niet...
[3517.74 --> 3519.74]  de laatste protege ooit opleiden.
[3520.56 --> 3521.74]  Namelijk een model...
[3522.46 --> 3523.74]  die daarna...
[3524.32 --> 3527.08]  zolang die harde schijf netjes gebackupt wordt...
[3527.08 --> 3528.74]  voor altijd kan blijven doorleven.
[3529.98 --> 3533.12]  Dus je moet wel een beetje over je eigen ego heen...
[3533.12 --> 3535.78]  om jezelf op te laten slurpen door een algoritme.
[3536.26 --> 3537.42]  Alles heeft toch een prijs.
[3537.64 --> 3538.96]  En ik zie...
[3538.96 --> 3540.86]  op Veense vormen zitten dat de beste...
[3540.86 --> 3541.52]  Stel je voor...
[3541.52 --> 3545.00]  je ziet de Zuidas voor je...
[3545.00 --> 3547.20]  en waar dan allemaal van die advocatenkantoren zitten...
[3547.20 --> 3548.72]  als je dan echt een goede advocaat bent...
[3548.72 --> 3550.88]  dan word je partner bij De Brauw...
[3550.88 --> 3551.48]  of weet ik veel...
[3551.48 --> 3552.90]  een van die advocatenkantoren.
[3553.32 --> 3554.68]  En dat in plaats van dat die mensen...
[3554.68 --> 3556.20]  die zitten nu op de bovenste verdieping...
[3556.20 --> 3557.12]  achter een marmeren bureau's...
[3557.12 --> 3559.12]  en die zijn al die mieren...
[3559.68 --> 3561.46]  in die verdiepingen er beneden...
[3561.46 --> 3564.06]  aan het vertellen wat ze allemaal moeten doen.
[3564.52 --> 3565.72]  Dan opeens zie ik vormen...
[3565.72 --> 3568.24]  hoe juist die bovenste verdieping wordt omgeturnd...
[3568.24 --> 3570.46]  in een soort van cubicle farm.
[3570.82 --> 3572.12]  En dat iedereen alleen maar...
[3573.78 --> 3575.50]  door een robot gestelde...
[3575.50 --> 3576.12]  hypothetische...
[3577.58 --> 3578.12]  juridische vraagstukken...
[3579.28 --> 3581.24]  met hun beste kennis...
[3581.24 --> 3583.98]  alleen maar hypothetische vraagstukken aan het behandelen zijn...
[3583.98 --> 3585.86]  tegen een AI om het model te trainen.
[3585.86 --> 3588.28]  En dat ze daar gewoon goed betaald voor worden, Wietse.
[3588.78 --> 3589.72]  Nou ja, ik denk dus dat...
[3589.72 --> 3591.92]  Ik kan me voorstellen dat als je nu...
[3591.92 --> 3593.78]  je bent...
[3593.78 --> 3594.96]  advocaat...
[3594.96 --> 3596.10]  je draait al een tijd mee...
[3596.10 --> 3599.22]  je zit met je advocatenvrienden van verschillende firms...
[3599.22 --> 3601.46]  een keer wat gezellig te eten...
[3601.46 --> 3603.08]  en dan gooit iemand het balletje op...
[3603.08 --> 3604.82]  zullen we met elkaar een model gaan trainen?
[3604.92 --> 3605.12]  Ja.
[3605.72 --> 3608.32]  Je kan iets installeren als een menulut in je Mac...
[3608.32 --> 3609.90]  en in Windows is het in de andere hoek...
[3609.90 --> 3611.54]  een klein notificatie-icoontje...
[3611.54 --> 3613.52]  die draaien we de komende twaalf maanden mee...
[3613.52 --> 3614.90]  die werkt een beetje als...
[3614.90 --> 3615.34]  hoe heet dat?
[3615.34 --> 3616.80]  dat rewind.ai.
[3616.88 --> 3617.26]  Ja, ja.
[3617.86 --> 3619.14]  Dan geef je consent voor...
[3619.14 --> 3621.20]  want met elkaar teken je daar ook nog documenten over...
[3621.20 --> 3623.20]  en na twaalf maanden komt er iemand...
[3623.20 --> 3624.10]  die zegt...
[3624.10 --> 3625.14]  jongens, kom maar hier.
[3625.26 --> 3626.34]  Dat wordt allemaal geüpload.
[3626.76 --> 3628.02]  Daar komt een model uit...
[3628.02 --> 3629.52]  en dan is dat model...
[3629.52 --> 3631.64]  want dat is dan een beetje mijn voorgevoel...
[3631.64 --> 3632.20]  in het bezit...
[3632.98 --> 3634.10]  van die groep mensen.
[3634.10 --> 3634.26]  Ja, zeker.
[3634.26 --> 3634.70]  die zes.
[3635.06 --> 3636.04]  En dan gaan zij dus...
[3636.04 --> 3636.78]  maar...
[3636.78 --> 3639.44]  dat zit hem dus wat minder in die...
[3639.44 --> 3641.06]  maatschappelijk verantwoord...
[3641.06 --> 3641.68]  maar meer...
[3641.68 --> 3644.02]  ik ga zorgen dat ik net als dat ik een...
[3644.02 --> 3646.12]  bezit heb in de vorm van grond...
[3646.12 --> 3647.12]  een huis of een stuk bos...
[3647.12 --> 3648.68]  heb ik bezit in de vorm van een model.
[3648.84 --> 3649.56]  Ik bedoel dit...
[3649.56 --> 3650.40]  geen zins...
[3650.40 --> 3652.06]  als een maatschappelijk verantwoord...
[3652.06 --> 3652.44]  initiatief.
[3652.44 --> 3653.06]  Nee, dat blijkt.
[3653.06 --> 3655.06]  Ik bedoel dit als een hypercapitalistische...
[3655.06 --> 3657.66]  Ja, hypercapitalistische ontwikkeling.
[3657.76 --> 3659.66]  En ik denk dat dit soort juridische modellen bestaan...
[3659.66 --> 3662.34]  en die zijn dan vooral getraind op de bestaande data...
[3662.34 --> 3663.70]  van grote advocatencentoren.
[3664.40 --> 3666.22]  Dus zo werkt dit nu al.
[3666.34 --> 3668.06]  Er zijn drie, geloof ik, van die grote...
[3668.96 --> 3670.94]  En een uitgever zal luisteren en zeggen...
[3670.94 --> 3672.12]  hallo, ik ben O'Reilly.
[3672.28 --> 3673.70]  Ik maak boeken over Python.
[3673.86 --> 3674.02]  Precies.
[3674.22 --> 3676.08]  Daar betrek ik zeven schrijvers bij.
[3676.18 --> 3677.04]  Die krijgen royalties.
[3677.16 --> 3678.20]  En dat boek verkoop ik.
[3678.26 --> 3679.34]  Is dat anders dan een model?
[3679.80 --> 3680.52]  Ik bedoel, ik hoor je.
[3680.52 --> 3682.98]  Maar het verschil wat ik probeer aan te maken...
[3682.98 --> 3685.26]  en dat is subtiel, dat geef ik toe...
[3685.26 --> 3687.36]  maar is dus in plaats van gewoon volgen...
[3687.36 --> 3688.86]  wat bestaande zaken zijn...
[3688.86 --> 3692.70]  en dan volgen hoe partners op die zaken reageren...
[3692.70 --> 3695.16]  of weet ik veel, gewoon learning by doing...
[3695.16 --> 3696.22]  dat is eigenlijk wat jij zegt...
[3696.22 --> 3698.58]  soort van door gewoon te volgen...
[3698.58 --> 3699.94]  en dat dan om te zetten in een model...
[3699.94 --> 3701.52]  doe ik er nog een stapje bovenop.
[3701.94 --> 3703.44]  Namelijk de AI laten bepalen...
[3703.44 --> 3704.80]  waar de feedback het nuttigst is.
[3705.38 --> 3706.68]  Dus de AI laten bepalen...
[3706.68 --> 3709.22]  welke elementen missen in de feedback loop...
[3709.22 --> 3711.68]  en daar een mens, een heel goed mens...
[3712.24 --> 3713.96]  het model beter te laten maken.
[3714.06 --> 3715.68]  Ik zie opeens soort van typeaapjes vormen...
[3716.54 --> 3720.32]  die dan, weet je, om betere modellen te maken...
[3720.32 --> 3723.68]  waardoor de hefboom dus gigantisch is op hun werk...
[3724.36 --> 3729.64]  opeens een model beter gaan maken.
[3729.74 --> 3731.92]  En dit is dus wat ik eigenlijk voor me zie bij die programmeurs.
[3732.06 --> 3733.30]  Een soort van de beste programmeurs...
[3733.30 --> 3735.76]  moeten feedback loops gaan doen...
[3735.76 --> 3736.76]  in plaats van...
[3736.76 --> 3739.14]  ja, gewoon hun werk delen.
[3739.18 --> 3741.16]  Ik denk dat mijn zorg daarbij is...
[3741.16 --> 3743.62]  van dan moet er goed gekeken worden naar die structuur...
[3743.62 --> 3748.76]  waarin je anders de 700 topprogrammeurs in...
[3748.76 --> 3752.52]  de 700 topprogrammeurs uit Silicon Valley...
[3752.52 --> 3754.32]  de keer tien of keer honderd developers...
[3754.32 --> 3755.22]  zoals ze dat vaak noemen...
[3755.76 --> 3757.48]  die spreken met elkaar een bondje af...
[3757.48 --> 3758.62]  om zo'n ding te gaan maken.
[3759.50 --> 3761.36]  En dat daarna...
[3761.36 --> 3764.08]  de niet zo topprogrammeurs zoals ik...
[3764.08 --> 3765.56]  die mogen daar dan gebruik van maken...
[3765.56 --> 3766.54]  voor een tientje per maand.
[3766.54 --> 3770.04]  Ja, ik vraag me dan af...
[3770.04 --> 3772.00]  wat zo gaaf is...
[3772.00 --> 3773.76]  aan het idee in Nederland...
[3773.76 --> 3775.22]  dat iedereen in essentie...
[3775.22 --> 3776.30]  zeker niet in deze tijd...
[3776.30 --> 3777.68]  maar in de basis, in de theorie...
[3777.68 --> 3779.88]  een huizen zou kunnen bezitten...
[3779.88 --> 3782.36]  kunnen wij dan ook allemaal een model bezitten?
[3782.50 --> 3783.68]  Of is dat echt weggelegd...
[3783.68 --> 3785.76]  voor de 700 superdevelopers?
[3785.90 --> 3788.10]  Daar zit nog even mijn vraagteken...
[3788.10 --> 3789.02]  wat ik altijd zou maken.
[3789.16 --> 3792.32]  Net zoals kapitaal bij een select groepje beschikbaar is.
[3792.46 --> 3793.46]  Ja, maar dan...
[3793.46 --> 3794.52]  en dan...
[3794.52 --> 3795.74]  ik vraag me dus...
[3795.74 --> 3797.06]  ik ben nieuwsgierig...
[3797.06 --> 3798.46]  dat bedoel ik niet cynisch...
[3798.46 --> 3799.60]  over die mix...
[3799.60 --> 3801.56]  van dat groot kapitaal...
[3801.56 --> 3802.38]  met deze...
[3802.38 --> 3803.26]  Of dit beter zou kunnen.
[3803.54 --> 3805.56]  Ja, omdat je het echt met hele kleine groepjes kan nu.
[3805.68 --> 3807.46]  Vroeger had je nog echt mensen nodig steeds.
[3807.74 --> 3808.72]  Die irritante mensen.
[3809.32 --> 3811.46]  En nu heb je ze misschien niet eens meer nodig.
[3811.70 --> 3813.24]  En dat sluit misschien nog aan...
[3813.24 --> 3815.58]  om een soort strik om onze aflevering heen te doen.
[3816.00 --> 3817.26]  Is...
[3817.26 --> 3819.46]  het Nederlandse model...
[3820.72 --> 3821.46]  GPT-NL...
[3821.46 --> 3824.08]  daar zitten we natuurlijk ook een beetje te zoeken.
[3824.24 --> 3825.16]  Jij zei al...
[3825.16 --> 3827.04]  ik zet een beeldje neer op een grasveld...
[3827.04 --> 3828.78]  met een drone eromheen laten vliegen.
[3829.16 --> 3831.26]  Als er iemand eigenlijk dus zegt...
[3831.26 --> 3833.78]  we gaan niet ons eigen model intern trainen...
[3833.78 --> 3835.96]  maar we gaan een deel van die data geven...
[3835.96 --> 3836.52]  aan het...
[3836.52 --> 3837.12]  noem het...
[3837.12 --> 3838.28]  de public good.
[3838.58 --> 3840.72]  Het grote publieke model...
[3840.72 --> 3842.58]  voor Nederland als samenleving.
[3842.70 --> 3843.78]  En uiteindelijk zelfs...
[3843.78 --> 3845.86]  zelfs een Europees model.
[3846.04 --> 3847.38]  Volgens mij zitten we daar nu tussen...
[3847.38 --> 3849.52]  het is eigenlijk weer een publiek-privaat discussie...
[3849.52 --> 3852.20]  als het onder de meeste discussies stiekem zit.
[3852.70 --> 3853.40]  En dat is interessant.
[3853.84 --> 3855.22]  Want ik denk wel...
[3855.22 --> 3857.10]  dat in het geval van find.com...
[3857.10 --> 3859.26]  waar ik nu een beetje enthousiast over aan het doen ben...
[3859.26 --> 3860.96]  dat is waarschijnlijk geen NGO.
[3861.72 --> 3862.72]  Nee, vermoed ik ook niet.
[3862.74 --> 3864.30]  Dat is waarschijnlijk ook een...
[3864.30 --> 3865.84]  ik weet niet hoeveel ontwikkelaars daar werken.
[3866.34 --> 3867.68]  Nou, wat een heerlijk hoopvol einde.
[3868.22 --> 3868.58]  Allemaal.
[3869.34 --> 3870.86]  Voor de burgers van ons land.
[3871.86 --> 3872.60]  Voor de mensen.
[3873.86 --> 3874.48]  Dit was Palky.
[3874.64 --> 3875.22]  Tot volgende week.
[3875.22 --> 3876.10]  Doeg.
[3876.10 --> 3890.78]  En, ben je er al achter...
[3890.78 --> 3892.24]  of Eneco dynamisch bij je past?
[3892.80 --> 3893.64]  Of nog niet?
[3894.28 --> 3895.98]  Doe de test op eneco.nl
[3895.98 --> 3896.70]  slash test.
[3897.80 --> 3899.78]  Mensen helpen een bewuste keuze te maken.
[3900.68 --> 3901.28]  We doen het nu.
[3901.76 --> 3902.26]  Eneco.
[3902.26 --> 3902.30]  Eneco.

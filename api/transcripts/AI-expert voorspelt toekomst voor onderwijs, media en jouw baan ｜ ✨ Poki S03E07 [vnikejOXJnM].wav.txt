Video title: AI-expert voorspelt toekomst voor onderwijs, media en jouw baan ｜ ✨ Poki S03E07
Youtube video code: vnikejOXJnM
Last modified time: 2024-10-17 09:44:03

------------------ 

[0.00 --> 2.36]  Misschien stap jij nu wel net in je auto.
[3.18 --> 3.88]  Auto aan.
[4.96 --> 6.44]  Jouw favoriete podcast aan.
[6.64 --> 8.34]  Welkom bij een nieuwe podcast.
[8.46 --> 9.64]  En gaan.
[10.34 --> 12.44]  Op weg naar waar jij maar heen wilt.
[13.10 --> 14.40]  Jij kiest voor gaan.
[14.86 --> 17.92]  Met je FBTO autoverzekering weet je dat je goed zit.
[18.32 --> 19.82]  Check fbto.nl.
[20.70 --> 22.24]  Jij kiest FBTO.
[24.18 --> 28.08]  Als ondernemer weet je dat tijd jouw meest waardevolle bezit is.
[28.08 --> 30.22]  Weet je ook waar je altijd te weinig van hebt?
[30.56 --> 31.56]  Inderdaad, tijd.
[32.18 --> 33.98]  Maar daar komt vandaag verandering in.
[34.88 --> 36.82]  Stap nu over op Teamleader Focus.
[37.48 --> 45.84]  De gebruiksvriendelijke bedrijfssoftware die CRM, facturen, offertes en projectmanagement samenbrengt op één overzichtelijke plek.
[46.46 --> 49.02]  Zodat jij meer tijd hebt om te doen waar je echt goed in bent.
[49.60 --> 50.54]  Tijd om te ondernemen.
[51.06 --> 51.60]  Teamleader.
[52.00 --> 54.24]  Kijk op focusopjebedrijf.nl
[54.24 --> 59.18]  Welkom bij Palky, de Nederlandse podcast over kunstmatige intelligentie.
[59.28 --> 64.12]  Waar we uitzoeken welke invloed AI gaat hebben op ons werk, ons leven en de samenleving.
[64.76 --> 66.10]  Bij mij is Wietsehagen.
[66.44 --> 67.28]  Ik ben Alexander Klubping.
[67.44 --> 72.58]  En in deze aflevering hoor je een exclusief interview met een van z'n werelds meest vooraanstaande experts op het gebied van AI.
[72.78 --> 74.40]  Wordt een professor Ethan Mollek.
[74.96 --> 77.38]  Omdat ik zijn nieuwste boek uitgeef, had ik de eer om te interviewen.
[77.38 --> 80.18]  Tussen zijn gesprekken door met wereldleiders en CEO's.
[80.32 --> 85.40]  Mensen die, net als ik, van hem willen weten hoe ziet de wereld er straks uit met AI.
[86.00 --> 88.72]  En Tesla had vorige week haar Re-Robot event.
[89.30 --> 90.96]  Aangekondigd werden de Cyber Cab.
[91.04 --> 94.42]  Een volledig autonomische robot taxi zonder stuur of pedalen.
[94.78 --> 95.66]  Een robovan.
[95.84 --> 100.82]  Eigenlijk een soort van uit het kluitgelassen bus, maar dan in stijl van een prachtige ouderwetse stijl.
[100.94 --> 101.72]  Daar gaan we het over hebben.
[102.18 --> 104.26]  En een update van de Humanoid robot.
[104.76 --> 107.16]  Die je in kan zetten om de afwas te doen thuis.
[107.92 --> 108.78]  Piets is erin gedoken.
[109.00 --> 110.06]  Veel plezier met Poki.
[120.60 --> 123.68]  Is het geworden wat je had verwacht ervan, Piets?
[123.86 --> 126.00]  Van het event van Elon Musk?
[127.36 --> 130.10]  Ja, ik denk dat het aardig in de buurt kwam van wat ik had gedacht.
[130.84 --> 132.40]  Ik moet zeggen, ik zat naar het ding te kijken.
[132.56 --> 135.62]  Ik dacht dat hij Piets toch knap zit te bespiegelen in zijn glazen bol.
[135.62 --> 136.36]  Nou, thanks.
[136.46 --> 142.04]  Maar ik had me echt wel ingelezen en lopen graven in de krochten van leaks en dingen.
[142.72 --> 150.08]  En het ligt redelijk in de lijn der verwachting gezien de staat van andere robotica bedrijven en de andere zelfrijdende bedrijven.
[150.46 --> 156.36]  Het is trouwens interessant dat Waymo, waar we het over gehad hebben toen jij vroeg, wie zitten nu aan de frontlines?
[156.82 --> 158.90]  Daar is weer 5 miljard ingegaan vanuit Google.
[158.90 --> 164.38]  Of dat toevallig nu aangekondigd is of dat zij even zouden zeggen, hey, wij zijn er ook nog en we gaan hartstikke goed.
[164.72 --> 165.44]  Vind ik wel interessant.
[167.00 --> 168.24]  Zullen we er maar eens in duiken?
[168.40 --> 172.20]  Wat is misschien te beginnen bij de cybercap?
[172.38 --> 177.24]  Een soort van cybertruck, maar dan zonder pedalen en zonder stuur.
[177.54 --> 181.20]  Waarbij het idee is dat je hem kan summonen en dan kun je erin gaan rijden.
[181.50 --> 183.28]  Wat heb je gezien?
[183.28 --> 185.20]  Hij is klein.
[185.72 --> 186.68]  Het was al een beetje verwacht.
[186.78 --> 189.16]  Er zit eigenlijk alleen maar een bankje voorin.
[189.82 --> 191.50]  Het is een two-person cab.
[192.44 --> 196.22]  Ze hebben onderzoek gedaan dat de meeste ritjes blijkbaar met twee of één persoon is.
[196.60 --> 200.46]  Er is natuurlijk geen chauffeur, dus als er twee stoelen zijn, kunnen er twee mensen in.
[201.60 --> 207.16]  En omdat die wat kleiner is, het idee is echt dat het hele goedkope, in verhouding goedkope autootjes zijn,
[207.16 --> 210.18]  die je in massa kan produceren en in die steden uit kan rollen.
[210.26 --> 212.26]  Ja, 30.000 dollar moeten ze gaan kosten.
[212.26 --> 214.96]  Ja, relatief goedkoop zou ik wel zeggen.
[215.26 --> 217.30]  Uiteindelijk kom je naar een soort 10.000 dollar toe.
[217.30 --> 219.52]  Voor een zelfrijdende auto, ik schrik er niet echt van.
[220.42 --> 224.86]  Nee, en waarschijnlijk als je het deelt door het aantal ritjes dat hij moet rijden om zichzelf terug te betalen, is het prima.
[225.70 --> 231.04]  Dan mag hij ook wel een miljoen kilometer kunnen rijden zonder grote ingrepen.
[231.48 --> 233.26]  Maar oké, klein, wat viel je verder nog op?
[234.52 --> 236.90]  Nou, het is wel interessant, want ik zat te kijken.
[237.02 --> 240.62]  Ik heb die video ook een paar keer teruggespoeld, dat ik dacht, wat zit er nou op die display?
[240.62 --> 250.06]  Ik kon dat sowieso een beetje bijzonder dat er, wanneer Apple bijvoorbeeld een event doet, en Tesla eigenlijk ook in het verleden, is er daarna best wel een geavanceerde website beschikbaar.
[250.24 --> 254.66]  Waarop je gewoon even lekker kan scrollen, om even door die mooie 3D renders te gaan.
[254.94 --> 256.46]  Dat is er nu eigenlijk niet.
[256.46 --> 260.90]  Die WeRobot website ging een soort van uren later pas live, na de livestream.
[261.46 --> 263.62]  En uiteindelijk hebben we drie foto's, niet eens een video.
[263.84 --> 267.52]  Dus ik ben nog een beetje verward, dus ik ben vooral die livestream aan het teruggespoelen.
[267.88 --> 269.84]  Niet de hele dag, maar ik heb dat een paar keer gedaan.
[270.20 --> 273.08]  Om te kijken, ja, maar hoe ziet dat dashboard er dan uit?
[273.16 --> 274.06]  Wat zit er op dat scherm?
[274.14 --> 279.96]  Maar het lijkt toch, dat geeft op zich ook niet, wel een soort model free, waar ze zoveel mogelijk hebben weggehaald.
[279.96 --> 285.02]  Om daar een soort heel klein, zelfrijdend ding van te maken, met een groot tv'tje in het midden.
[285.14 --> 286.74]  En daar moet het allemaal op gaan gebeuren, zeg maar.
[286.92 --> 287.72]  Zest wel minimalistisch.
[287.94 --> 289.06]  En hoe gaat dit nou werken?
[289.14 --> 295.16]  Dan heb je een Tesla app waarin je er een kan bestellen en dan zoals een Uber ritje er een kan aanvragen.
[295.34 --> 297.64]  Of hoe gaan we dit als consument gebruiken?
[298.72 --> 301.36]  Volgens mij hebben zij bedacht dat het vanuit de Tesla app gaat.
[301.36 --> 305.84]  Want ik begreep niet deeltjes met de andere grote taxibedrijven.
[306.06 --> 309.46]  Of de neo-taxibedrijven dan, zoals Uber.
[309.96 --> 312.62]  Dit moet een Tesla ding worden, dus in de Tesla app.
[313.08 --> 314.62]  Maar goed, ook dit, het zijn goede vragen.
[314.90 --> 316.08]  Het is allemaal nog erg vaag.
[316.16 --> 317.24]  Het is niet heel erg concreet.
[317.38 --> 321.92]  We hebben wel, er zitten in de, ze doen natuurlijk quarterly reports.
[322.32 --> 324.76]  Omdat ze een publiek bedrijf zijn.
[325.14 --> 329.34]  Daar zitten dan pdf's bij met wat gave, een beetje een fancy deck.
[329.34 --> 331.72]  Met wat mooie 3D renders erin.
[332.08 --> 337.34]  Daar hebben ooit wel eens screenshots ingezeten van hoe die taxi interface er dan uit zou zien.
[337.46 --> 339.74]  Om een soort van investeerders blij te maken.
[339.96 --> 343.34]  Dat is een soort donkere Uber app.
[343.84 --> 345.20]  Ja, ja, ja.
[345.60 --> 347.86]  Oké, nou dat busje hoeven we niet lang bij stil te staan.
[348.00 --> 353.60]  Het ziet eruit als iets wat je bij Tuschinski mooi zou kunnen voorrijden in Amsterdam.
[353.88 --> 356.80]  Zo'n prachtige Art Deco bioscoop.
[356.80 --> 359.12]  Het is een beetje een Art Deco versie van een bus.
[359.42 --> 360.80]  Waar twintig man in kunnen.
[361.32 --> 362.18]  En waar je...
[362.72 --> 365.52]  Maar is die niet juist, is dat niet juist zo raar?
[365.70 --> 373.56]  Want ik bedoel, er wordt, een van de commentaar op al die zelfrijdende auto's is van, kunnen we niet gewoon beter openbaar vervoer hebben?
[373.56 --> 380.40]  In plaats van allemaal autootjes die dan op de snelweg aan elkaar gekoppeld worden als een soort asfalttrein.
[380.40 --> 380.46]  Treintje.
[380.46 --> 381.60]  Ja, treintje.
[381.92 --> 386.46]  Maar met die bus heb ik eigenlijk zoiets van, maar nu ben je gewoon een bus aan het maken.
[387.46 --> 392.50]  Ja, misschien moet je die dan gewoon aanbieden aan de connections van Nederland of zo.
[392.80 --> 393.50]  Of Q-bus.
[393.50 --> 395.20]  Die vond ik nog een beetje maf.
[395.32 --> 400.32]  Dat voelt voor mij heel erg als een ding dat mensen binnen San Francisco van hun huis naar hun kantoor brengt.
[400.46 --> 401.76]  En dat is het dan.
[402.18 --> 406.14]  Maar je hebt het de hele tijd over andere taxibedrijven en andere busbedrijven.
[406.22 --> 410.04]  Maar ik ga er toch van uit dat Tesla de hele keten gaat willen beheersen, toch?
[410.04 --> 415.26]  Het idee is neem ik aan niet dat je dat connection een robovan kan kopen, wel?
[415.82 --> 416.86]  Nou ja, nee.
[417.08 --> 421.32]  Maar tegelijkertijd de Semi, dat is hun vrachtwagen.
[421.32 --> 425.08]  Die is dan weer helemaal wel gebrand door andere transportbedrijven.
[425.26 --> 427.90]  Dus het is niet alsof ze dat soort deals nooit maken.
[428.46 --> 435.46]  En je hebt ook UFO, een taxi of een auto huurbedrijf die helemaal Tesla branded is.
[435.58 --> 437.86]  Maar dan met een andere logo op Tesla's.
[437.92 --> 441.36]  Dus helemaal niet toestaan van een rebrand van hun eigen product.
[441.52 --> 443.92]  Nee, maar ik denk precies wat jij zegt.
[444.18 --> 449.12]  Het idee is dat Tesla dan ook de rol van Uber gaat spelen en niet de leverancier van Uber wordt.
[449.28 --> 450.24]  Dat is jouw punt, denk ik.
[450.24 --> 451.66]  En dat is volgens mij waar.
[451.94 --> 453.40]  Maar goed, laten we naar de kern gaan.
[453.52 --> 456.18]  En dat is hun humanoid robot.
[456.38 --> 460.26]  Ik zag filmpjes tegenover elkaar van de versie die ze drie jaar geleden lieten zien.
[460.36 --> 464.26]  Dat was iemand in een pak die dansjes aan het doen was.
[464.70 --> 468.58]  Dat toch een vrij verontrustende en verwarrende ervaring was van drie jaar geleden.
[469.12 --> 474.52]  Deze robot aangekondigd zien worden en dan daadwerkelijk een mens in die dingen te laten zitten.
[474.52 --> 481.84]  Deze keer hadden ze een variant gemaakt waarin er daadwerkelijke robots waren die dankjes aan het maken waren.
[482.16 --> 486.48]  Waar je selfies mee kon nemen en waar je ook mee in gesprek kon gaan.
[486.48 --> 492.22]  En het werd gepresenteerd als in straks hebben we allemaal zo'n robot en die gaat de vaatwas voor je doen.
[492.50 --> 495.54]  En die kan de boodschappen uit de achterkant van de auto halen.
[495.76 --> 498.04]  En die kan je in fabrieken inzetten om te werken.
[498.16 --> 503.56]  Eigenlijk het verhaal wat jij al een tijdje lang in deze podcast aan het vertellen bent.
[503.70 --> 505.20]  Dat werd hier in beeld gebracht.
[505.20 --> 508.38]  Met een beetje fantasie zeg ik erbij.
[509.52 --> 520.06]  Hoe keek je naar dat shot waar er twintig robots in een soort van Noord-Koreaanse formatie dat Hollywood terrein opgelopen kwamen?
[521.44 --> 525.58]  Nou ja, het deed me heel erg denken aan de term visioneering.
[525.96 --> 528.16]  Dat is niet per se pessimistisch.
[528.26 --> 530.64]  Visioneering, dus engineering en vision.
[530.64 --> 536.82]  Dat is een idee dat je zegt je bent aan het engineered maar je gaat ook eigenlijk een visie neerzetten.
[536.98 --> 539.28]  Dus daar mag dan wat smoke en mirrors bij zijn.
[539.90 --> 546.00]  Dus een deel van wat je laat zien is om het publiek te laten wennen, enthousiast te krijgen.
[546.40 --> 549.58]  Cynisch investeerders geldt maar meer optimistisch.
[549.74 --> 551.52]  Die wereld moet erop voorbereid worden.
[552.08 --> 554.62]  We kunnen dan best wel in een ideale situatie.
[554.76 --> 558.84]  Oftewel een Hollywood studio in een stadje gemaakt van karton.
[558.84 --> 563.68]  Robots laten rondlopen die eigenlijk op afstand bestuurd worden door mensen met een VR headset op.
[563.76 --> 565.40]  Want dat blijkt nu de realiteit te zijn.
[566.10 --> 573.04]  Maar dan nog, want voor de luisteraars wat Alexander en ik hebben gezien in die live event is denk ik 16 of 20 robots echt veel.
[573.46 --> 579.08]  Die zonder tether, dus zonder kabel, zonder mensen eromheen om te zorgen dat ze niet omvielen.
[579.68 --> 582.04]  Eigenlijk best wel serieus.
[582.18 --> 583.14]  Ik vond het wel impressive.
[583.14 --> 588.42]  Ik had ze allemaal alleen gezien in filmpjes en zo, maar als het dan daar in een livestream rondloopt op straat.
[588.76 --> 590.04]  Dat is echt wel anders.
[590.56 --> 591.32]  Tussen de mensen.
[591.56 --> 596.40]  Het was ook wel, het is niet alleen maar de show, maar het was ook achteraf bij de borrel dat ze allerlei taakjes deden.
[596.58 --> 601.84]  Dus er is heel veel beeldmateriaal uitgekomen van mensen die gewoon met die dingen interacteren.
[601.92 --> 603.14]  Dat is toch best indrukwekkend.
[603.14 --> 609.74]  Ja, maar daar kregen we wel weer een beetje dat Rabbit R1 en Humane Pin effect.
[609.92 --> 610.76]  Wat bedoel ik daarmee?
[611.20 --> 616.94]  Is dat dan in het testen, dus we zagen veel video's waarin, kijk ik heb een gesprek met zo'n robot.
[617.26 --> 622.54]  Want ik denk, los of dat het een AI is, dat vind ik wat minder interessant.
[622.76 --> 624.34]  Ja, ik kan ook bij mijn telefoon praten.
[624.52 --> 625.88]  Stop die speaker in een robot.
[625.88 --> 629.60]  Dus daar kijk ik niet voor.
[629.72 --> 634.64]  Ik kijk voor, blijft dat ding staan tussen de mensen en kan die inderdaad een biertje tappen?
[635.08 --> 637.18]  Ik vind dat, dat vind ik dan het interessante eraan.
[638.10 --> 640.92]  Maar er is een video, iemand zegt, moet je kijken hoe menselijk die klinkt.
[641.02 --> 644.44]  Ik denk, ja goed, dat wist ik al wat Alexander en ik hebben dat live in de uitzending gedaan.
[644.60 --> 646.20]  Stop dat op twee poten en je hebt het ook.
[646.60 --> 648.54]  Dat bleek nou net het punt te zijn wat dus niet was.
[648.54 --> 649.64]  Waar ze gefaked hebben.
[650.16 --> 652.44]  Ja, dat waren toch teleoperated robots.
[652.44 --> 656.94]  Waar ik van denk, en daar ben ik niet het enige in, zeg dat gewoon, want dat is nog steeds super bizar.
[657.64 --> 666.76]  Het feit dat er ergens nu in ruimtes mensen met VR-brillen opstonden en koptelefoons om die apparaten te besturen, dat is op zichzelf toch al bizar.
[668.14 --> 671.66]  Ja, fake dan niet een soort van chat GPT daarin waar je mee kan praten.
[671.66 --> 678.12]  Het is zo'n bedrijf, want dit is eigenlijk, dit is eigenlijk een beetje de vibe die rondom Mus komt te hangen.
[678.22 --> 684.60]  Als ik even de, de toch wel vette, zijn er een vette raketten, zijn rakettenhandeltje even buiten beschouwing laat.
[684.72 --> 689.00]  Want dat is, ja, op, dat is veel indrukwekkender, zou ik zeggen.
[689.00 --> 693.20]  Zou ik zeggen, daar is ook smoke, maar weinig mirrors.
[693.82 --> 702.24]  En in het geval van die zelfrijdende auto, taxi, moeten we maar zien of die productiedatum van 2026 gehaald gaat worden.
[702.38 --> 703.44]  Of die prijs gehaald gaat worden.
[703.58 --> 706.00]  Of überhaupt het gaat lukken om full-cell driving te doen.
[706.46 --> 707.58]  Het zijn allemaal vraagtekens.
[707.66 --> 709.58]  En met die robots ook.
[709.58 --> 716.40]  En dan zitten we, moeten wij als soort van, nou, toch mensen die het beste voor hebben, zou ik zeggen, met Tesla.
[716.50 --> 723.68]  Die echt open staan voor de innovatie en voor de technische, ja, knappe dingen die ze doen.
[724.46 --> 729.02]  Dat wij echt bijna, we moeten bijna een soort van, je krijgt allemaal vragen van mensen om je heen.
[729.14 --> 731.46]  Een soort van, ja, ze hebben het gefaked, want iedereen hoort dat dan.
[731.56 --> 733.14]  Dat is dan wat blijft hangen bij mensen.
[733.14 --> 737.48]  En dan worden wij in een situatie gedwongen waarin wij moeten zeggen, ja, maar het is toch knap hoor.
[737.48 --> 745.16]  Want ja, ondanks dat ome Elon gefaked heeft dat die dingen praten, ja, is het toch wel knap dat die robot op zichzelf kan staan.
[745.62 --> 748.96]  En weet je, dat er geen draadje aan hoeft te zitten, et cetera, et cetera.
[749.18 --> 751.98]  Moet je kijken hoe knap motorisch die vingers bewegen.
[752.72 --> 761.40]  Ja, het is moeilijk om het op te nemen voor de heer Musk.
[761.40 --> 769.42]  Ja, en ik denk dat in dat opzicht, ik ben misschien net als jij ook een beetje in de war nu, omdat er net weer gisteren een raket is gelandt.
[769.42 --> 772.66]  Die met twee stokjes uit de lucht gepakt is, het waren dat ding zo groot als een flatgebouw.
[773.06 --> 773.88]  Ik zat weer helemaal hyped.
[773.90 --> 776.58]  Dit wordt op maandag opgenomen, dus als je weer terugluistert.
[776.64 --> 777.90]  Dat was afgelopen weekend, ja.
[778.28 --> 782.62]  Ja, ik was helemaal hyped zaterdag, dat ik dacht, wat is er nou weer gebeurd?
[782.62 --> 788.00]  Ik ging het weer even allemaal geloven, dat ik dacht, ja, dit is toch wel heel gaaf, weet je al zo.
[788.64 --> 789.72]  Maar goed, dan heb je dat event.
[789.94 --> 793.76]  Ik wil nog wel even terugkomen op wat je net zei, met die robots en die smoke and mirrors.
[794.46 --> 801.58]  Wat ik wel had verwacht, wat er eigenlijk niet was, is om ook dat laatste puzzelstukje te leggen.
[802.24 --> 808.04]  En desnoods was het een render geweest, vind ik prima, dus een volledige 3D-gegenereerde video.
[808.66 --> 812.08]  Had even laten zien hoe die robots een deel van die taxis in elkaar zetten.
[812.08 --> 814.24]  Had laten zien hoe ze de stoelen plaatsen.
[814.54 --> 819.12]  Had laten zien hoe die robots hem uiteindelijk schoonmaken met een paar doekjes voordat hij de fabriek uitrijdt.
[819.48 --> 823.32]  Had dat ook laten zien om die koppeling te maken.
[823.44 --> 829.18]  Maar wacht even, die humanoid robot kan ook en wordt ook onderdeel van de productielijn.
[829.60 --> 834.50]  Maar als onderdeel van de visionering, want dit is niet hoe die taxis nu gemaakt worden natuurlijk.
[834.90 --> 836.88]  Dat wordt gewoon door mensen in elkaar geschroefd.
[837.28 --> 841.52]  Als je dan een bluetooth speaker op twee benen zet, kan je net zo goed ook even dat filmpje laten zien.
[841.52 --> 845.48]  Ik bedoel meer om die visie te laten zien.
[845.90 --> 847.00]  Maar wacht even.
[847.36 --> 853.54]  En ook misschien zelfs Optimus in een andere form factor, waar we het over hadden voordat het event was.
[853.98 --> 859.58]  Had een andere robot op twee wielen met hele grote armen in een fulfillment centrum laten zien.
[859.72 --> 862.90]  En dan had ik je gezegd het is de Optimus Fulfillment 2 ofzo.
[862.90 --> 866.80]  Want die, dat vind ik het ingewikkeld.
[866.86 --> 870.02]  Dat ik denk nou, als Tesla daar straks een goed model in handen heeft.
[870.16 --> 872.90]  Dus dan heb ik het over het AI model dat al die apparaten aanstuurt.
[873.86 --> 879.06]  Die in verschillende form factors van auto's tot robots tot grasmaaiers iets kan doen.
[879.46 --> 881.12]  Dan is dat best wel substantieel.
[881.12 --> 883.24]  Als zij daar een beetje frontrunner in zijn.
[883.36 --> 889.14]  Dus ik had dat denk ik iets meer daarop gezet dan wat er nu getoond is.
[890.08 --> 891.12]  Het was een visueel spektakel.
[892.12 --> 899.14]  Maar als dat zich moet vertalen tot publiekelijk enthousiasme of investeerders enthousiasme.
[899.66 --> 903.12]  Dan zou ik zeggen dat dat medium gelukt is voor de hoeveelheid werk.
[904.08 --> 905.50]  Eigenlijk in de technologie is gestoken.
[905.62 --> 907.12]  Is in de aandacht in dit event.
[907.76 --> 908.42]  Of zie je dat anders?
[908.42 --> 911.88]  Misschien heb ik het gemist hoor.
[911.96 --> 914.36]  Maar ik kwam het niet echt tegen op NOS.
[914.64 --> 916.66]  Dat is toch een beetje de plek waar ik dan even doorheen ga scrollen.
[916.94 --> 918.08]  Ik heb nu even gezocht.
[918.16 --> 919.32]  Ze hebben er wel wat over geschreven.
[919.40 --> 920.76]  Maar het is geen frontpage geweest.
[920.96 --> 921.40]  Nee zeker.
[921.74 --> 923.48]  Ik denk dat mensen, dat het in.
[925.84 --> 929.06]  Het is enorm gepolitiseerd sinds dat hij Twitter heeft overgenomen.
[929.32 --> 932.56]  En ik denk echt dat dat invloed heeft op de manier waarop er over Tesla gepraat wordt.
[932.56 --> 934.60]  En zelfs hoe er over de SpaceX gepraat wordt.
[935.08 --> 936.56]  Dat is toch echt een soort smet.
[936.56 --> 939.52]  Smet voor een heel groot deel van de mensen.
[940.28 --> 941.58]  Dat zou niet uit moeten maken.
[942.06 --> 944.16]  Ik heb net een boek gelezen die dat stelt.
[944.28 --> 946.12]  Je moet dat kunnen decoppelen.
[946.34 --> 946.64]  Heet dat.
[946.96 --> 949.96]  Het uit elkaar trekken van de persoon.
[950.20 --> 951.98]  En dan wat de persoon doet.
[952.12 --> 954.40]  Je moet die twee los van elkaar kunnen bekijken.
[954.66 --> 957.62]  Om een soort van interessante meningen te kunnen vormen.
[958.08 --> 960.74]  Maar ik heb toch een beetje het idee dat dat wel door elkaar loopt.
[960.74 --> 961.66]  Bij de meeste journalisten.
[962.20 --> 962.40]  Nou ja.
[962.60 --> 963.06]  Dat zijn zo.
[963.06 --> 964.80]  Zijn er nog dingen die je hier aan wilt toevoegen?
[964.92 --> 967.80]  Of hebben we dit wel gedegen behandeld?
[969.22 --> 970.82]  Nou wat ik eraan toevoeg is.
[971.66 --> 975.68]  Er is dus niet echt een fijne marketing website om te kijken wat er allemaal is.
[976.30 --> 981.86]  Ik ben heel benieuwd wat er de komende maanden dan concreet naar buiten gaat komen.
[982.36 --> 983.42]  En wat rond gaat rijden.
[983.70 --> 987.14]  En of we video's gaan zien van Robo Taxis in the Wild.
[987.14 --> 991.94]  Dat gaat dan een beetje voor ons het antwoord geven op hoeveel smoking bearers was dit nou eigenlijk.
[992.44 --> 992.46]  Ja.
[992.46 --> 992.60]  Ja.
[992.80 --> 992.96]  Ja.
[993.08 --> 993.20]  Ja.
[993.34 --> 993.50]  Ja.
[993.58 --> 993.74]  Ja.
[993.82 --> 994.24]  Heel goed.
[994.46 --> 994.82]  Heel goed.
[995.56 --> 996.72]  Straks het hoofdonderwerp.
[996.80 --> 998.08]  Maar eerst een boodschap van onze sponsor.
[998.74 --> 1001.56]  Datamakelaars verzamelen en verkopen jouw persoonlijke gegevens.
[1001.70 --> 1003.58]  Die vervolgens in handen komen van de hoogste bieder.
[1003.80 --> 1006.10]  Van oplichters tot verzekeringsmaatschappijen.
[1006.64 --> 1009.28]  Maar gelukkig is daar Incogni jouw digitale bodyguard.
[1009.52 --> 1011.22]  En met hen blijft je data beschermd.
[1011.22 --> 1014.36]  Incogni beperkt publieke toegang tot je privégegevens.
[1014.82 --> 1018.34]  En zorgt ervoor dat jouw persoonlijke informatie niet zomaar kan worden doorverkocht.
[1018.60 --> 1021.02]  En daarmee verklein je het risico op identiteitsdiefstal.
[1021.40 --> 1023.22]  En houd je de controle over je eigen data.
[1023.62 --> 1026.04]  Wil jij nou ook de baas blijven over je persoonlijke informatie?
[1026.14 --> 1028.80]  Bezoek dan nu incogni.com.pokepod.
[1029.38 --> 1031.98]  En ontvang 60% korting op je Incogni-account.
[1031.98 --> 1033.98]  Dat is incogni.com.pokepod.
[1034.98 --> 1037.62]  Omdat jouw privacy het waard is om te beschermen.
[1038.16 --> 1038.92]  Terug naar de show.
[1039.10 --> 1040.08]  Het hoofdonderwerp.
[1040.08 --> 1041.70]  Ik zei het in de intro al.
[1041.78 --> 1046.46]  Deze week een exclusief interview met een van de meest verontstaande experts op het gebied van AI.
[1047.20 --> 1050.58]  En ja, dit klinkt een beetje wc-eend omdat ik zijn boek uitgeef.
[1051.06 --> 1052.58]  Maar deze man wordt...
[1052.58 --> 1054.50]  We geven het boek liever niks uit, zullen we maar zeggen.
[1054.58 --> 1059.30]  Hij wordt wereldwijd gezien als een belangrijke stem in het debat over AI.
[1060.44 --> 1062.90]  En ik heb een exclusief interview met hem gehad.
[1062.98 --> 1066.30]  Dat had ook heel wat voet in de aarde om met hem te mogen praten tussendoor.
[1066.30 --> 1072.62]  En dat is omdat de man onwaarschijnlijk druk is sinds zijn boek is uitgekomen.
[1072.76 --> 1075.06]  En het boek in heel veel talen vertaald wordt.
[1075.84 --> 1083.06]  Ik heb een gesprek met hem gehad waar ik geprobeerd heb zoveel mogelijk thema's die wij in deze podcast bespreken.
[1083.42 --> 1085.84]  Van wat AI voor onze banen betekent.
[1085.84 --> 1088.94]  De economie, het onderwijs, de manier waarop we media consumeren.
[1090.20 --> 1092.82]  Nou ja, langs te laten komen en zijn reflectie erop te krijgen.
[1093.48 --> 1098.00]  En Ethan vertelt ook wat hij hoort van medewerkers van grote AI bedrijven.
[1098.08 --> 1099.40]  Als hij ze privé spreekt.
[1099.54 --> 1104.42]  Over het algemeen krijg je dan andere dingen te horen dan dat je met de marketing afdeling spreekt.
[1104.54 --> 1105.98]  Van die grote AI bedrijven.
[1106.04 --> 1107.00]  Dus dat gaan we van hem horen.
[1107.00 --> 1110.80]  Welke cutting edge AI tools hij zelf gebruikt.
[1111.24 --> 1114.26]  En welke nieuwe golf aan AI systemen eraan zit te komen.
[1114.36 --> 1115.28]  En ik waarschuw je alvast.
[1115.36 --> 1116.92]  Hij praat ontzettend snel.
[1117.50 --> 1120.32]  Dus in een half uur krijg je veel informatie te verstouwen.
[1120.48 --> 1123.48]  Ik zou het niet erg vinden als je hem op 0.75 luistert.
[1123.54 --> 1124.52]  Maar dan moet je het lekker zelf weten.
[1125.06 --> 1125.40]  Ik zou zeggen.
[1125.56 --> 1127.64]  Veel plezier met dit interview met Ethan Mollek.
[1127.64 --> 1130.06]  Thanks for doing this, Ethan.
[1132.00 --> 1132.74]  Thank you for having me.
[1133.38 --> 1137.50]  You are in frequent contact with people in the AI industry.
[1138.50 --> 1140.30]  I guess my first question is.
[1140.52 --> 1142.84]  What are the topics that the people.
[1143.06 --> 1145.12]  The discussion that these people are having.
[1145.22 --> 1148.64]  That haven't yet reached mainstream media or public awareness.
[1149.52 --> 1152.64]  I'm interested in what they are talking about to you.
[1152.64 --> 1158.32]  What insights can you share about what engineers or professionals at AI companies.
[1158.42 --> 1162.36]  Are currently thinking about that haven't yet reached the public awareness.
[1163.02 --> 1164.36]  That's a terrific question.
[1164.88 --> 1166.64]  And I would say generally the answer is not.
[1166.84 --> 1168.22]  The things they're saying publicly.
[1168.68 --> 1171.46]  But I think people don't believe them when they say it publicly.
[1171.80 --> 1173.34]  So I'll emphasize privately.
[1173.80 --> 1175.96]  That they believe whether or not this is true or not.
[1176.00 --> 1179.48]  That they have found the key to scaling up to somewhere near human level.
[1179.48 --> 1182.70]  Or beyond intelligence just doing what they're doing now with more computers.
[1183.16 --> 1184.24]  They keep saying that to everybody.
[1184.32 --> 1184.70]  And everyone's like.
[1184.86 --> 1185.08]  Yeah.
[1185.16 --> 1186.04]  They might be making it up.
[1186.40 --> 1187.28]  They might be making it up.
[1187.30 --> 1188.22]  But they believe that.
[1188.56 --> 1189.70]  And the second thing is.
[1190.08 --> 1190.54]  You know.
[1190.64 --> 1192.18]  That they also think that agents.
[1192.36 --> 1194.76]  The idea of AIs that are autonomous and going to do stuff.
[1194.86 --> 1195.68]  Is right around the corner.
[1196.08 --> 1197.54]  So these are things that they firmly believe.
[1197.60 --> 1199.96]  Now they are telling everybody at every meeting about this.
[1200.00 --> 1200.72]  But I think everyone's like.
[1200.80 --> 1201.84]  Oh they're just trying to raise funds.
[1202.30 --> 1203.12]  They are true believers.
[1203.20 --> 1203.70]  Not everyone.
[1203.96 --> 1205.70]  But enough people that I talk to are true believers.
[1206.12 --> 1207.42]  That I take that as a signal.
[1207.42 --> 1208.78]  I don't necessarily think they're right.
[1208.78 --> 1210.80]  But I also don't necessarily think they're wrong.
[1211.38 --> 1214.08]  What should we take from this as a society?
[1214.24 --> 1216.70]  Because on one hand you can be cynical about it.
[1216.74 --> 1218.40]  And say they just say that for fundraising.
[1218.58 --> 1223.24]  But let's think for a second about what it would mean if they are right.
[1223.64 --> 1225.62]  What are we underappreciating here?
[1225.82 --> 1228.66]  When we just read accounts in the newspapers.
[1229.68 --> 1231.56]  Well I mean one of the major messages in my book.
[1231.64 --> 1232.86]  Is that you have to just use these systems.
[1232.86 --> 1236.24]  Like it's a really complicated way of basically trying to show you.
[1236.44 --> 1237.00]  It's not that hard.
[1237.10 --> 1237.78]  And it's kind of fun.
[1237.78 --> 1240.50]  And the implications will become clearer to you as you use them.
[1240.50 --> 1240.76]  Right?
[1241.02 --> 1242.66]  And this is a general purpose technology.
[1242.76 --> 1244.22]  So it's going to have very uneven effects.
[1244.32 --> 1246.60]  It's not going to change everything everywhere all at once.
[1246.62 --> 1248.28]  It's going to make changes in different places.
[1248.88 --> 1250.40]  It depends on how it interacts with humans.
[1250.88 --> 1258.92]  And so I think that too much of our anxiety is either around the idea of long-term science
[1258.92 --> 1259.48]  fictional harms.
[1259.48 --> 1260.54]  Which enough people are worried about.
[1260.60 --> 1264.26]  I mean the Nobel Prize winner is worried about AI becoming super intelligent and murdering us all.
[1264.26 --> 1266.26]  So we should obviously spend some worry about that.
[1266.32 --> 1269.96]  But I think that that tends to take away the focus from the fact that we're also living through,
[1270.10 --> 1273.80]  regardless of what happens in the long-term future, a short-term exponential change.
[1274.14 --> 1276.62]  And adjusting that requires getting involved in this.
[1276.70 --> 1278.76]  This is not a hard technology to use, weirdly.
[1279.08 --> 1280.24]  It is very easy to use.
[1280.24 --> 1284.74]  If you're a good teacher, a good parent, a good manager, a good interviewer, you're going
[1284.74 --> 1288.66]  to be able to figure out ways to use this technology because it's like kind of working with a person.
[1289.98 --> 1294.88]  And you talk about this in the book where you say the moment that this thought really hit
[1294.88 --> 1297.60]  you was the moment that you had three sleepless nights.
[1298.30 --> 1305.06]  Combining that idea and taking the doomerism aside for a moment, let's not do that.
[1305.06 --> 1312.40]  But if you combine the sort of sense that these people at AI companies share with you privately,
[1312.96 --> 1319.30]  combined with this idea, what do you envision for the next couple of years?
[1319.40 --> 1327.36]  What scenarios do you have in your mind that you really think a lot about and don't think
[1327.36 --> 1329.70]  is being talked about enough?
[1330.90 --> 1331.30]  Right.
[1331.44 --> 1332.88]  And I end the book with some scenarios.
[1332.88 --> 1337.18]  I would say just on the three sleepless nights issue, we're starting to get some empirical evidence
[1337.18 --> 1340.10]  that that's true, that people when they first use AI are kind of unhappy.
[1340.66 --> 1342.70]  And then after they've used it enough, they start to feel good.
[1342.76 --> 1345.72]  So there is this kind of like one-two punch of that.
[1347.20 --> 1350.28]  I would say, you know, there are four scenarios at the end of the book about what could happen in the future.
[1350.50 --> 1351.96]  One of them is a static future.
[1352.16 --> 1355.44]  The other is a linear improvement, exponential improvement, which is what we've been seeing.
[1355.92 --> 1359.38]  Or else, you know, AGI, machine smarter than a human in every task.
[1359.38 --> 1365.96]  And what I would say is, since I wrote the book, scenario one of a static world has actually become less likely.
[1366.60 --> 1368.72]  Part of the reason for that is two things.
[1368.84 --> 1372.04]  One is more evidence of what's called the scaling law.
[1372.12 --> 1372.82]  It's not really a law.
[1372.90 --> 1373.46]  It's an observation.
[1373.66 --> 1379.44]  But the idea that the more computing power and the more data you put into an AI, the smarter it gets.
[1379.52 --> 1380.32]  That seems to be holding.
[1380.32 --> 1388.10]  So that means for the next couple generations of AI systems, the next, you know, two to four years, we're going to keep seeing exponential improvement.
[1388.30 --> 1389.32]  And then there was this breakthrough.
[1390.50 --> 1392.34]  OpenAI announced a new model called O1.
[1392.66 --> 1398.96]  And what O1 did that's really interesting is they showed that if you just let the AI think for longer, it comes with better answers.
[1398.96 --> 1403.04]  So what that means is that you don't have to just scale this by throwing more data at it.
[1403.08 --> 1404.78]  You could also just give it more time to think.
[1405.00 --> 1409.98]  And that means that it's harder to see an endpoint to this technology in the near future, given that revelation.
[1411.08 --> 1416.00]  I know that in your book, you emphasize that we shouldn't look at AI progress with a doomsday mindset.
[1416.18 --> 1417.56]  So I tried to take that into account.
[1417.56 --> 1424.58]  But you say that it's more helpful to think about how it can assist us and how it can benefit us.
[1425.12 --> 1427.98]  But let's explore that perspective a little bit later.
[1428.20 --> 1433.70]  And for now, I think a lot of people on a lot of people's minds is concerns about the job market.
[1433.70 --> 1442.56]  And economists always note that past technological innovations leading to job loss were always compensated by the creation of new jobs.
[1443.26 --> 1446.42]  Do you believe that that pattern will continue this time?
[1446.42 --> 1453.94]  And do you feel that the economy is equipped to handle the growth of more advanced AI systems in the short term?
[1454.76 --> 1456.18]  So I don't think we know.
[1456.30 --> 1463.60]  I mean, I think there is a little bit of Pollyannish, like looking to the positive side of economists, that jobs will always be increased by the new technology.
[1463.96 --> 1465.92]  And that's been true for physical labor replacement.
[1466.16 --> 1468.88]  We don't know if it's true for intellectual labor replacement in the same way.
[1469.12 --> 1471.84]  It has been for small scale in a lot of cases, but not always.
[1471.84 --> 1481.14]  And the other thing I'd point out is living through the Industrial Revolution wasn't exactly whatever the appropriate expression is for a bag of chips for something great.
[1481.72 --> 1484.06]  Because there was massive job displacement.
[1484.16 --> 1485.16]  People moved towns.
[1485.52 --> 1486.48]  Cities rose and fell.
[1487.28 --> 1489.26]  Even if things are good, there can be disruption.
[1489.26 --> 1493.02]  I think part of the issue is that disruption is going to be very uneven here.
[1493.10 --> 1494.94]  It's going to happen across society in different ways.
[1495.74 --> 1496.80]  And I mean, let's be clear.
[1496.88 --> 1499.52]  The AI companies themselves have been very explicit.
[1499.66 --> 1501.66]  They think they'll be able to replace most human laborers.
[1501.68 --> 1502.90]  So we'll all be able to do other stuff.
[1502.94 --> 1505.70]  And the AI will make infinite bounty for all of us.
[1505.76 --> 1508.68]  Like, we don't really have a sense of what that science fiction future looks like.
[1508.78 --> 1512.80]  I think over the next three to five years, what we'll see is rolling change in industries.
[1512.80 --> 1516.72]  And a lot of that comes down to organizations choosing what to do.
[1517.16 --> 1526.32]  I often warn companies that if they think about this as technology for the last five or ten, you know, for a few decades has been about replacing labor.
[1526.76 --> 1529.62]  So if I get a, you know, higher efficiency, I fire people.
[1530.08 --> 1539.62]  I think that doesn't work when you're also on the edge of a technological boom, which is, you know, the image I like to give people is let's say that you were a Dutch beer manufacturer in the early 1800s.
[1539.62 --> 1542.18]  And along comes steam power.
[1542.38 --> 1543.04]  You have two choices.
[1543.16 --> 1547.80]  You can either fire most of your people and make more money per barrel of beer as in your local community.
[1548.14 --> 1553.08]  Or you can expand to go worldwide and hire 100,000 people and be the equivalent of Guinness or something.
[1553.08 --> 1560.58]  I think that companies that are more forward looking about what the future possibilities of this technology can do will gain a lot of benefit.
[1560.76 --> 1562.98]  But I don't know if that balances out job loss.
[1563.08 --> 1563.86]  Nobody really knows.
[1564.24 --> 1568.38]  It sounds like a coping mechanism in some way that we tell ourselves that.
[1568.38 --> 1574.54]  If we believe what the engineers are telling you, then it might be a little bit more dire.
[1574.74 --> 1579.30]  And you must take this into account when you're thinking about this at night.
[1579.82 --> 1580.88]  Yeah, I mean, I do.
[1581.28 --> 1587.60]  You know, part of this is I think that we're underestimating technology, but I think there's a lot.
[1587.68 --> 1591.24]  And one of the key findings in the book that I discuss is the idea that AI is jagged.
[1591.32 --> 1593.54]  It's really good at some stuff and really bad at other things.
[1594.04 --> 1597.94]  So I don't know if you've played with, and speaking of podcasts, have you played with Notebook LM yet?
[1597.94 --> 1598.30]  Sure.
[1599.10 --> 1599.36]  Right?
[1599.52 --> 1599.86]  I did.
[1599.96 --> 1603.94]  I fed my entire book into Notebook and did a really nice podcast based on it.
[1604.38 --> 1609.70]  But, you know, and as great as that is, though, there's still a lot of like detail missing that would have been useful or important.
[1609.82 --> 1611.52]  Will it fill in the detail just to get smarter?
[1611.58 --> 1613.26]  We don't know 100% at this point.
[1613.26 --> 1622.00]  So I think there's a jaggedness there that means not a total replacement for humans, but some fields that will beat us just like it does in chess right now.
[1622.24 --> 1625.44]  I think there is some coping and some hiding our head in the sand.
[1625.60 --> 1629.28]  But I also think that total doom feels unlikely as well.
[1629.46 --> 1629.62]  Sure.
[1629.62 --> 1630.98]  So we're going to live somewhere in the middle.
[1631.08 --> 1631.68]  We'll muddle through.
[1632.02 --> 1635.94]  I'd like people to muddle through in a positive way where they're showing positive impacts to this technology.
[1636.02 --> 1637.24]  It's going to have a lot of good and bad.
[1637.36 --> 1639.46]  Like we can't be close-eyed to the bad stuff.
[1639.84 --> 1642.08]  But there is agency on the good stuff as well.
[1642.08 --> 1647.72]  What skills and sort of competencies do you advise companies to gain in this?
[1648.28 --> 1654.22]  What are essential skills in this AI-dominated economy that we're looking at for the next couple of years?
[1655.28 --> 1664.64]  So I think the thing that gets overemphasized, I discuss in the book as well, is the idea of prompt crafting or prompt engineering, that you have to get really good at prompting the AI.
[1665.30 --> 1666.76]  I think that that has limited value.
[1666.88 --> 1669.24]  It will be decreasing in value as these systems get smarter.
[1669.24 --> 1671.32]  So they'll just be able to do things for you.
[1672.08 --> 1678.74]  I think that where we're going to see real value increase is actually people who are experts at their job.
[1678.82 --> 1679.88]  That's the skill right now.
[1679.94 --> 1684.50]  Because if you're an expert, you can work with the AI for a few seconds and really learn quickly.
[1685.00 --> 1686.16]  Is it any good at this?
[1686.22 --> 1687.08]  Is it bad at this?
[1687.16 --> 1688.50]  Where does it tend to lie to me?
[1688.68 --> 1690.94]  Where is it getting better?
[1691.06 --> 1692.04]  Where is it not getting better?
[1692.54 --> 1694.64]  And is it multiplier of your expertise?
[1695.58 --> 1699.32]  I think that people are too eager to try and find some secret to AI.
[1699.32 --> 1707.18]  The real secret, I've become very convinced since the book that one piece of advice I gave turned out to be exactly the right advice, which is just use the damn thing.
[1707.28 --> 1708.92]  10 hours is my minimum amount of time.
[1709.30 --> 1714.58]  And if you do that, if you can push through to 10 hours, you're as good an AI expert in your field as anyone's going to be right now.
[1714.96 --> 1720.12]  You look at it like it's sort of like a multiplier effect on your expertise.
[1720.12 --> 1721.90]  And a replacement.
[1722.18 --> 1727.78]  I mean, jobs, and we discuss in the book, jobs are bundles of tasks.
[1727.94 --> 1729.60]  You do more than one thing in your job, right?
[1729.88 --> 1732.50]  So, you know, you run a podcast.
[1732.94 --> 1734.08]  You run a publishing company.
[1734.28 --> 1736.08]  You do a lot of things all at once.
[1736.62 --> 1738.92]  Some of those job bundles will be replaced by AI.
[1739.08 --> 1740.32]  Some will be supplemented by AI.
[1740.98 --> 1744.74]  For right now, whatever you're best at, you're almost certainly better than AI at that.
[1744.74 --> 1747.32]  So does the AI do the tedious parts of your job, right?
[1747.64 --> 1754.52]  Would it have arranged for the two of us as we tried to find a time to talk when our two agents have just talked to each other and worked that all out without worrying about it, right?
[1754.96 --> 1758.00]  And, you know, would that have, that would have changed your job bundle and mine.
[1758.14 --> 1761.16]  But would that have been a good use of our, you know, of our time that we gave up?
[1761.40 --> 1763.30]  So I think there is change that's going to happen.
[1763.90 --> 1765.44]  And I think some of it's going to be more profound than others.
[1765.50 --> 1768.32]  Like, I am worried about some giant sectors of the economy.
[1768.32 --> 1774.84]  Customer service call senders are something to be concerned about because AI is pretty good at customer service right now.
[1774.88 --> 1775.98]  And there's a lot of money in it.
[1776.16 --> 1778.80]  And it's a low, you know, it's a low prestige feel for a lot of people.
[1778.88 --> 1781.16]  And there's a lot of, you know, effort to replace them.
[1781.52 --> 1784.04]  I think stock photography is another area that's going to take hits.
[1784.10 --> 1786.26]  So there's some areas that are going to take more damage than others.
[1786.72 --> 1789.16]  I think on the other hand, people are saying Hollywood's going to be destroyed.
[1789.56 --> 1793.22]  Don't really know how much effort goes into making a movie that tells a coherent story.
[1793.50 --> 1796.52]  And that when you're the top person in a field, it's not easy to do.
[1796.52 --> 1801.82]  So, I mean, I think that it is a co-intelligence really and a multiplier for people by and large.
[1802.78 --> 1811.58]  Sometimes I walk in this district in Amsterdam called the Zuytos and all the financial companies and lawyers and the banks are situated there.
[1812.20 --> 1816.28]  And sometimes I walk through these crowds and they're not customer service people.
[1816.56 --> 1818.28]  They're the high paid lawyers.
[1818.82 --> 1822.06]  And sometimes I think, like, are they still going to be here in five years?
[1822.06 --> 1829.30]  What do you think about those high paying jobs, consultants, that kind of stuff?
[1830.60 --> 1832.58]  It is a really interesting question, right?
[1832.74 --> 1834.32]  Like, there are things that...
[1834.32 --> 1838.50]  So, you know, in one of the studies we talk about in the book, we actually do this with a high-end consulting company.
[1838.60 --> 1842.28]  We find the AI is at the eighth percentile of doing consultant tasks and lots of tasks.
[1842.28 --> 1848.94]  However, there's also lots of glue and intuition and other pieces that may be more flexible than the AI requires.
[1849.08 --> 1850.50]  So, the job of consultants changes.
[1851.64 --> 1851.94]  Law...
[1851.94 --> 1858.10]  I don't know how law is done in your country, but in the U.S., they charge by the hour.
[1859.98 --> 1861.26]  That's how legal firms work.
[1861.28 --> 1862.76]  They have the tendency to do the hairs well.
[1862.76 --> 1866.90]  Yeah, that's going to have to change because it turns out, like, right now, all the clients...
[1866.90 --> 1869.12]  When they talk to a big law firm, all their clients are saying, don't use AI.
[1869.48 --> 1873.28]  That's going to switch very quickly to, if you're not using AI, do all the basic law stuff.
[1873.38 --> 1874.68]  Why are you charging us for this?
[1874.96 --> 1878.54]  But then the remaining work the AI doesn't do becomes very high-value work.
[1878.92 --> 1885.72]  So, you know, I'm more worried about the pipeline of new people into these jobs than mass unemployment from people currently in these jobs.
[1885.72 --> 1886.46]  Right?
[1886.58 --> 1895.32]  Because when I talk to companies, what's happening right now is, you know, how do you learn to be a lawyer or, you know, or a banker or a consultant?
[1895.52 --> 1898.86]  You go to school and we teach you to be a generalist.
[1898.90 --> 1903.92]  And then you go to that company and you end up becoming an apprentice, right?
[1903.92 --> 1907.60]  You do repeated work over and over again while people yell at you if you do things wrong.
[1908.00 --> 1908.86]  You gain the experience.
[1908.92 --> 1909.86]  And that's how you learn the job.
[1910.18 --> 1915.00]  Now, what's happening right now is everyone's delegating that intern work to AI because AI is already better than your intern.
[1915.00 --> 1918.18]  And your interns are also all using AI to give answers back to you.
[1918.68 --> 1920.46]  And it's completely severed the talent pipeline.
[1921.10 --> 1924.26]  That, to me, is the bigger concern that I have about the next generation of talent.
[1924.86 --> 1929.70]  Is this something that you think about a lot when you're teaching your students?
[1929.70 --> 1938.84]  How they, in times where they don't, where I guess they need the experience but feel that they don't need the experience because ChatGPT can do it for them.
[1939.22 --> 1944.28]  How do you get them to do the rough handiwork?
[1945.00 --> 1946.84]  Well, so school will solve the problem.
[1946.94 --> 1949.02]  It's a chaotic mess right now, right?
[1949.02 --> 1954.82]  So I don't have studies from the Netherlands, but we know in, like, Denmark and the U.S., basically, everybody's using AI.
[1954.96 --> 1956.38]  All the students are cheating, right?
[1956.58 --> 1957.62]  Cheating is not detectable.
[1957.82 --> 1958.58]  It's all falling apart.
[1959.72 --> 1960.78]  We'll get through that, right?
[1960.80 --> 1962.74]  My classes are 100% AI classes.
[1962.96 --> 1968.74]  Other classes, like if you need to learn a foreign language, you're going to end up doing a lot more in-class work and writing essays in class.
[1968.82 --> 1970.20]  Like, we'll solve the problem in class.
[1970.20 --> 1975.48]  The question is, what do we do about the on-the-job training piece that used to be apprenticeship?
[1975.66 --> 1976.70]  And I think it has to be two things.
[1976.84 --> 1982.84]  Companies either have to start to figure out how are they going to build an actual education system rather than pretending they have one, right?
[1983.08 --> 1984.24]  Or hoping it just happens.
[1984.60 --> 1986.52]  Or we'll have to start taking that into universities more.
[1986.64 --> 1988.84]  I don't know which two answers will follow.
[1988.84 --> 1993.52]  Don't you think there's a role for AI in sort of simulating these experiences?
[1994.16 --> 1994.48]  Absolutely.
[1994.76 --> 1997.78]  And I've got a few Google Moloch and simulation.
[1997.96 --> 1999.20]  There's a few you can play with right there.
[1999.28 --> 1999.82]  They're all free.
[2000.66 --> 2002.60]  Yeah, I mean, I think it can supercharge education.
[2003.10 --> 2005.66]  Once we're done destroying it, we'll be able to rebuild it much better.
[2005.82 --> 2006.86]  I mean, it is a good tutor.
[2006.94 --> 2007.58]  It's a good tool.
[2007.70 --> 2010.88]  But the problem is that students use it in a way that's lazy and hurts them.
[2011.24 --> 2012.58]  I mean, I speculate about this in the book.
[2012.62 --> 2013.28]  It turns out it's true.
[2013.28 --> 2024.08]  There's a nice study by my colleagues in Turkey that finds that students who are given AI to use and just use it naively to try and solve problems, you know, to help them do much better homework, but much worse on the test.
[2024.14 --> 2026.26]  Because the AI just does the work for them and they think they're learning.
[2026.90 --> 2033.34]  If you do this in a more direct way with a tutor, with an AI tutor, you end up doing better on the homework and don't take a hit on the test.
[2033.82 --> 2036.96]  So there is a future here, but it can't just be kind of naive use.
[2036.96 --> 2044.64]  You focus on universities and sort of education to prepare people for the job market.
[2045.14 --> 2056.32]  But one step before, maybe in high school or in elementary school, you're very certain that first education gets crushed and then gets built from the ashes again.
[2057.00 --> 2063.48]  How do you feel like a high school curriculum will look differently when we build it up again?
[2064.04 --> 2066.04]  I mean, so curriculum is harder than pedagogy.
[2066.04 --> 2068.26]  And by the way, I'm being a little overdramatic, right?
[2068.38 --> 2070.26]  Teaching changes very little over the centuries.
[2070.46 --> 2073.74]  Like most of us are just, most teachers are just going to ignore AI and hope it goes away.
[2074.14 --> 2078.20]  And you'll do badly the test if you use AI and it'll sort out the way it always sorts out, right?
[2078.80 --> 2084.24]  I do think that, you know, we've known long before AI, for example, that lectures are a bad way to teach, right?
[2084.58 --> 2086.14]  Or at least a bad main way to teach.
[2086.18 --> 2088.22]  And you do active learning, people who participate in learning.
[2088.22 --> 2094.26]  Like, so this will hopefully push us to doing more of that flipped classroom where you learn outside of class and apply inside of class.
[2094.34 --> 2097.80]  So I'm a little facetious, a little joking when I say destroy all of education.
[2097.80 --> 2100.68]  But it is undermining a lot of assignments and other kinds of things.
[2101.10 --> 2102.44]  Those are rebuildable, right?
[2102.70 --> 2104.62]  The curriculum question is a much bigger one.
[2105.08 --> 2111.22]  My argument is still that we need to keep learning what we've learned because you need a basis of expertise to build knowledge on top of.
[2111.22 --> 2115.64]  So you still need to know basic math and, you know, and basic literature and history.
[2116.04 --> 2122.26]  Those things might become even more important, especially in humanities, because it turns out the AI is trained at all of our cultural heritage.
[2122.60 --> 2125.60]  And if you know the cultural heritage, you can make the AI do much more interesting things.
[2127.34 --> 2131.36]  And how do you think you can motivate people to do that in these new times?
[2132.56 --> 2139.34]  I mean, I think that part of this is trying to get people to actually face what this thing is.
[2139.34 --> 2144.12]  I mean, not to loop back to the idea that I told you before, that you need your 10 hours with this, you need your crisis.
[2144.30 --> 2145.68]  But I think that's the key.
[2146.18 --> 2151.56]  Like, people are naturally averse to kind of, once you've seen the shape of the hyperobject, as they joke about online,
[2151.84 --> 2155.44]  once you've seen what this thing looks like, you start to realize, oof, things are going to change.
[2155.50 --> 2156.54]  And then you're motivated.
[2157.14 --> 2159.82]  But otherwise, I think we like to pretend that nothing is happening.
[2159.82 --> 2171.64]  Sometimes I feel like my job changed a lot because I can now dictate something and then let Claude rewrite the text like it's me.
[2171.86 --> 2178.36]  And I can sort of nudge it in some way so that something comes out that is quite presentable and, to me, looks good enough.
[2178.36 --> 2187.78]  But I have this built-in sense of experience, sort of being able to judge what the thing comes up with to say if it's okay, yes or no.
[2188.14 --> 2197.34]  And I'm thinking about kids that are 12 right now and grow up not having that sort of reference,
[2197.34 --> 2205.78]  the reference of knowing what is good or what isn't good, what is good context, what is, you know, not just the facts.
[2205.78 --> 2209.30]  It's not just the sort of basic curriculum that I'm worried about.
[2209.40 --> 2216.70]  I'm worried about sort of what their, how their judgment is going to form for the stuff that they do in school.
[2216.90 --> 2217.94]  How do you think about that?
[2218.78 --> 2222.04]  I mean, that's where I was talking about building that basis of expertise being so important.
[2222.44 --> 2223.90]  Because we do need those basics.
[2224.00 --> 2225.76]  You cannot learn to be good at your job.
[2225.76 --> 2231.52]  You would not be a good interviewer if you hadn't had a huge amount of background in reading material and understanding,
[2231.66 --> 2235.08]  talking to people, understanding what's interesting, what isn't, how to pose a question,
[2235.08 --> 2235.92]  how to learn.
[2236.46 --> 2238.78]  So that's, the great thing is we have students in school.
[2238.98 --> 2240.52]  We can get them to do this stuff.
[2240.62 --> 2241.88]  That's what school is supposed to do.
[2242.20 --> 2247.16]  It might mean taking that more seriously, but we do have to kind of grind people through those basics, right?
[2247.46 --> 2249.52]  The question is then what happens afterwards?
[2249.68 --> 2252.32]  How do you get that last while of skill of practical use?
[2252.32 --> 2254.48]  And that may be something to bring it to schools as well.
[2255.54 --> 2255.98]  Okay.
[2256.28 --> 2261.88]  I'm trying to make the AI sort of, this sort of thing that's happening to humanity now.
[2261.88 --> 2266.08]  I try to make it super concrete because it makes it possible to sort of visualize it.
[2266.14 --> 2271.72]  And education is one of those things where I think you help to visualize it by saying we can now flip the classroom
[2271.72 --> 2276.22]  and we can do these mentoring things that we always knew would be good for our kids
[2276.22 --> 2279.16]  and we couldn't because of the lack of people.
[2279.68 --> 2283.18]  And I try to zoom in on a couple of different parts of what society is.
[2283.22 --> 2287.30]  Now, media is also one of those parts that really shape us as a society.
[2287.30 --> 2291.60]  How do you envision media consumption changing?
[2291.76 --> 2297.54]  And do you sort of anticipate that synthetic media like the stuff being generated by Notebook LM
[2297.54 --> 2302.14]  will play a significant role in the consumption that we have?
[2302.24 --> 2304.62]  And in what way do you expect that to change?
[2305.70 --> 2309.76]  So I think that is a fairly profound question.
[2309.76 --> 2316.18]  I think that right now, right, it's not just Notebook LM, it's also all the fake information on there.
[2316.26 --> 2318.36]  Like I can create, you know, fake images of anybody.
[2318.92 --> 2323.10]  We're already living in a problematic, troubled information environment, right?
[2323.16 --> 2324.22]  Everyone's in their own bubbles.
[2324.34 --> 2325.44]  No one's interacting with each other.
[2326.26 --> 2329.50]  I am worried that this underlines that rather than anything else.
[2329.54 --> 2330.58]  You get like a source of truth.
[2330.70 --> 2335.30]  Now, on the other hand, the AI, there's a study that shows that the only way to,
[2335.30 --> 2340.78]  the only thing we actually found that robustly lowers belief in conspiracy theories is actually talking to the AI.
[2340.86 --> 2341.06]  Yeah.
[2342.02 --> 2343.78]  It lowers their beliefs three months later.
[2343.92 --> 2348.48]  And that also suggests it's really good persuasion for people with deeply held beliefs, which is also troubling.
[2348.60 --> 2350.94]  But like this is a good learning tool, right?
[2350.96 --> 2353.40]  It is a good tool for interrogating things.
[2353.46 --> 2354.92]  It's very good at pulling out information.
[2355.72 --> 2357.70]  You know, and this feels very cheesy to say,
[2357.70 --> 2361.20]  but I think that it really is about how we as humans use these systems.
[2361.62 --> 2364.38]  And that's what I'm pessimistic about in the information environment.
[2364.38 --> 2366.00]  Like, it would do a really good job.
[2366.08 --> 2369.24]  Explain the pros and cons of this politician's arguments.
[2369.58 --> 2370.80]  It will do a nice job with that.
[2371.10 --> 2374.60]  Help me think through why someone might support them or why they might not support them.
[2374.68 --> 2375.80]  Like, give me empathy.
[2376.12 --> 2377.38]  It's great at those kind of things.
[2377.52 --> 2378.42]  Are people going to ask?
[2378.50 --> 2379.42]  That's a separate question.
[2379.78 --> 2381.50]  Yeah, but that's an important question, though.
[2383.58 --> 2384.34]  What do you think?
[2384.34 --> 2384.64]  I agree.
[2384.78 --> 2386.92]  And that's a person question, right?
[2386.96 --> 2387.96]  Not an AI question.
[2388.10 --> 2388.52]  I don't know.
[2388.52 --> 2399.50]  People are part of a system with economic incentives and, you know, with companies, you know, optimizing for profit.
[2399.84 --> 2407.52]  And when you just take that into account and combine it with the technical sort of advances that we can now make,
[2407.52 --> 2415.72]  having a TikTok feed that's completely synthetically generated is not a weird thing to think about anymore.
[2415.72 --> 2418.26]  That is a bad thing that is going to happen, right?
[2419.26 --> 2423.98]  You know, but what I'm saying is we've already built the systems that are going to supercharge the bad effects.
[2424.18 --> 2424.48]  Exactly.
[2424.48 --> 2424.78]  Right.
[2425.04 --> 2429.62]  So the issue we have is that we have an engagement-based economy and a personalized-based economy,
[2429.62 --> 2433.88]  and we're about to set off a, you know, supercharge that with AI.
[2434.32 --> 2436.12]  And that is ominous.
[2436.84 --> 2437.24]  So?
[2438.44 --> 2440.48]  I guess my question is...
[2440.48 --> 2440.54]  So I can't...
[2440.54 --> 2442.88]  I'm not claiming I can solve...
[2442.88 --> 2445.34]  Like, I think some of these are longstanding sets of issues, right?
[2445.68 --> 2447.64]  And some of these are, like I said, social problems.
[2447.64 --> 2454.40]  So in this event, what, you know, the logical answer, and it was filled with a world of deep fakes and no common understanding,
[2454.58 --> 2459.88]  let's return to traditional media and, you know, listen to the people who were the authorities 20 years ago
[2459.88 --> 2462.32]  because they at least had a professional standard.
[2462.76 --> 2464.12]  I mean, that's one solution.
[2464.28 --> 2465.24]  Do I think that's going to happen?
[2465.42 --> 2466.30]  I don't know.
[2466.52 --> 2469.58]  You know, it doesn't feel like we're going to move back from that partisan.
[2469.72 --> 2475.58]  Maybe people just descend into complete bubbles, but that's so annoying that they end up dealing with each other in real life more
[2475.58 --> 2477.12]  and reject this AI world.
[2477.12 --> 2480.10]  Maybe, you know, like, I don't know, right?
[2480.16 --> 2480.78]  I don't...
[2480.78 --> 2483.32]  Like, what I'm saying is that there's a policy piece to this, too.
[2483.38 --> 2487.68]  There's a deeper set of questions about what this means and how we handle it.
[2489.80 --> 2496.26]  Like, I can point out the crisis and I can give pathways to solutions, but, like, this is where agency matters.
[2496.46 --> 2499.56]  I really worry about people viewing AI as something happening to them.
[2499.92 --> 2502.06]  And to some extent, no one asked for this technology.
[2502.26 --> 2503.14]  I totally get it.
[2503.44 --> 2504.88]  But we also get to shape what this is.
[2504.88 --> 2508.10]  The AI labs do not have a view of how this is going to affect media.
[2508.48 --> 2509.12]  They just haven't thought...
[2509.12 --> 2510.44]  They never thought about how it's going to affect education.
[2510.78 --> 2511.86]  This is all news to them.
[2512.24 --> 2513.50]  So who gets to shape this?
[2513.62 --> 2516.28]  Well, if we can start showing them how to do this stuff right...
[2516.28 --> 2517.10]  I mean, you know, it's funny.
[2517.16 --> 2518.52]  You're talking about Notebook LM, right?
[2518.52 --> 2521.68]  I've spoken to a couple radio hosts and interviewers like yourself.
[2522.04 --> 2526.10]  And most of them are like, this is pretty good, but it messes up a few things that I would definitely do differently.
[2527.02 --> 2530.76]  So, like, we should be choosing control of that and saying, okay, how do we build this kind of tool?
[2530.86 --> 2531.52]  It's not like...
[2531.52 --> 2532.52]  These are not...
[2532.52 --> 2534.52]  We have the ability to do these kind of things there.
[2534.62 --> 2536.16]  These technologies are not that hard to use.
[2536.26 --> 2540.06]  And what I deeply worry about is that we're abrogating our responsibility to shape that world.
[2540.06 --> 2543.04]  So you're asking me, who has nothing to do with media, right?
[2543.36 --> 2544.06]  What would I do?
[2544.14 --> 2545.00]  You know, what would I do?
[2545.08 --> 2547.66]  The real question is, okay, so you are a national government.
[2547.86 --> 2549.10]  You are a national broadcaster.
[2549.56 --> 2554.10]  You are, you know, you have the head of TikTok in front of your congressional committee.
[2554.64 --> 2556.04]  You have to make some...
[2556.04 --> 2556.74]  Take some agency.
[2557.02 --> 2560.84]  People in TikTok need to decide is this good or bad or decide if they're going to quit en masse.
[2561.12 --> 2562.70]  Like, we have to make some decisions.
[2563.06 --> 2564.84]  And I agree with you.
[2564.86 --> 2567.00]  It won't all work out for the best if we don't make choices.
[2570.06 --> 2571.00]  Okay, let's see.
[2571.20 --> 2574.66]  We have five minutes left, so I'm going to...
[2574.66 --> 2575.78]  These are great questions, by the way.
[2575.92 --> 2576.60]  Thank you, sir.
[2577.52 --> 2579.40]  So let's switch gears a bit.
[2579.64 --> 2580.82]  How do you...
[2580.82 --> 2585.74]  To end with a very practical way of looking at this, and maybe...
[2585.74 --> 2586.26]  Sorry.
[2587.74 --> 2588.28]  No worries.
[2588.42 --> 2589.06]  What is here?
[2589.86 --> 2590.02]  Okay.
[2590.46 --> 2591.24]  Bless you, sir.
[2591.74 --> 2592.06]  Thank you.
[2592.72 --> 2593.92]  Let's switch gears a bit.
[2593.92 --> 2598.28]  How do you personally integrate these tools in your daily work?
[2598.28 --> 2606.62]  What is the stuff that feels cutting edge to you that you toyed with, and it makes you really excited in the last couple of weeks?
[2607.54 --> 2611.36]  I mean, so, you know, we've talked about it, and, you know, you are an early adopter.
[2611.50 --> 2614.18]  So let's just go back and say the things we've already mentioned.
[2614.32 --> 2618.08]  The things you've mentioned that you're using already are going to blow people's minds.
[2618.16 --> 2621.26]  So if you have not used Notebook LM from Google, you should do that.
[2621.38 --> 2623.88]  It is a great tool where you just upload documents.
[2624.72 --> 2627.60]  Please, not an illegal copy of my book, but you can take anything else you want.
[2627.60 --> 2631.34]  And load that in, and then it will create a radio show for you.
[2631.40 --> 2633.04]  It will let you interact with those documents.
[2633.20 --> 2634.10]  Absolutely mind-blowing.
[2634.60 --> 2636.68]  One of those things that moves the needle for people.
[2637.24 --> 2640.30]  I think advanced voice mode for ChatGPT just came out.
[2640.38 --> 2641.42]  And I should make a point.
[2641.52 --> 2642.28]  I talked to all the AI labs.
[2642.32 --> 2643.10]  I'm not paid by any of them.
[2643.14 --> 2644.44]  So this is not endorsements, right?
[2644.48 --> 2645.56]  This is just my current feeling.
[2645.56 --> 2652.66]  If you haven't used advanced voice mode yet, which is if you get the premium access to ChatGPT, you can use it.
[2652.96 --> 2655.78]  That will, I think, probably have a big effect on you.
[2655.90 --> 2656.80]  How do you use that?
[2656.98 --> 2659.32]  How do you use the voice thing from ChatGPT?
[2659.32 --> 2661.64]  Right now, I mean, I talk to it when I'm hands off.
[2661.64 --> 2667.68]  I'm not the kind of person who builds a relationship with AI, so I'm not that interested in having deep, meaningful conversations with it.
[2667.68 --> 2671.98]  But I find it's a very fun way to get some quick interaction going on, right?
[2672.20 --> 2673.50]  And to ask questions and other stuff.
[2673.52 --> 2674.82]  It's like what you wanted Siri to be.
[2674.92 --> 2677.90]  Now, other people I know have very deep conversations with the AI.
[2678.16 --> 2683.56]  I've talked to a quantum physicist at Harvard who talks to the AI and says he gets all of his good ideas talking to him.
[2683.60 --> 2684.36]  I'm like, is it good at physics?
[2684.42 --> 2685.96]  He's like, no, but it's really good at asking questions.
[2686.04 --> 2686.38]  Really?
[2686.38 --> 2689.10]  So you'll have to figure out your own use cases, right?
[2689.36 --> 2699.02]  And then the third tool I would recommend to everybody is just try Claude Sonnet, because if you like words, you're going to be very impressed by how just clever it is, right?
[2699.24 --> 2704.76]  And by the way, we're on the cusp of a whole new range of AI tools being released, and we've got pretty heavy rumors going on.
[2705.14 --> 2708.80]  So one of the things about AI is that it's generational.
[2709.28 --> 2711.38]  The larger the AI systems, the stronger they are.
[2711.68 --> 2713.26]  It takes a while for a new generation to keep up.
[2713.26 --> 2716.22]  We have a whole bunch of what I call Generation 2 AI systems.
[2716.38 --> 2718.70]  Like GPT-4, like Claude Sonnet.
[2719.24 --> 2726.60]  And Generation 3 will be released soon, which will be like GPT-5, Claude 3.5, Opus, a bunch of other of these tools.
[2726.74 --> 2727.74]  What do you expect of these?
[2727.80 --> 2728.46]  What are the rumors?
[2730.04 --> 2730.86]  I mean, the rumors are...
[2730.86 --> 2731.64]  That's not rumors, Ethan.
[2733.14 --> 2736.56]  The rumors are exactly what you'd expect, which is that these are better systems.
[2736.68 --> 2741.32]  I mean, the interesting thing I'm running into, and we saw this with this O1 release that just came out, this high-end system.
[2742.46 --> 2744.52]  A lot of people are like, well, I don't know what to do with this.
[2744.52 --> 2748.28]  I don't have that many problems that require a PhD student to solve my problems for me.
[2748.78 --> 2760.66]  So like, you know, one of the world's best mathematicians, Terence Tao, who's ever lived, said that, you know, he views O1, the new model, as a mediocre but not incompetent grad student in math.
[2760.90 --> 2764.76]  And speaking as one of the best mathematicians who ever lived, this is not a meaningless statement, right?
[2764.76 --> 2767.72]  So like, part of the question I was like, okay, these systems get smarter and smarter.
[2768.74 --> 2772.34]  You know, does it change your life if you already find the model smart enough?
[2772.60 --> 2773.50]  That's a hard question.
[2773.82 --> 2784.00]  I guess the assumption is that then it can not make those mistakes anymore, that it can be so creative that we, you know, are actually surprised when a thing comes up with something.
[2784.34 --> 2785.78]  But that already is happening, right?
[2785.86 --> 2788.06]  I mean, first of all, mistakes are overrated as a concern.
[2788.22 --> 2789.84]  Like, hallucinations are a real issue.
[2789.84 --> 2793.38]  But if you're using it as a co-intelligence, it's less of an issue because you know when it's BSing or not.
[2793.44 --> 2795.40]  My standard has always been best available human.
[2795.54 --> 2798.70]  Is it smarter or is it more or less accurate than the best human you have access to?
[2799.02 --> 2800.86]  And that's something, again, you learn how to use.
[2801.26 --> 2807.52]  And I think the other kind of, you know, the other question about this stuff is like, okay, it's already beating humans like creativity tests.
[2807.58 --> 2809.28]  For most people, it's more creative than them already.
[2809.64 --> 2810.76]  How has that changed their lives?
[2810.82 --> 2812.02]  So that's what I think about a lot.
[2812.02 --> 2834.36]  Do you have also this tendency that you combine, you know, the progress that you see with Notebook LM, with the stuff that you see from the video generation, Pika, the Chinese models that we now see generating these videos, the amount of, you know, the limitation of the lag time in the way that we talk to AIs.
[2834.36 --> 2842.32]  It feels like we can see this path where there's different building blocks that are now in isolation progressing.
[2842.78 --> 2853.44]  But there's going to be this path where it combines and you combine with just real world applications with people doing jobs or leading their lives or being entertained or communicating with friends.
[2854.92 --> 2862.24]  What do you feel is sort of underappreciated when you combine these things, when people talk to you in interviews?
[2862.24 --> 2865.30]  What don't people get?
[2866.24 --> 2874.18]  I don't think they get how much on the cusp we are of AI doing meaningful, obviously meaningful work as a stand-in for people.
[2874.40 --> 2880.72]  Like one of the things that makes these, like people still think of this like software, where what it does is you need to write all these APIs and code around it.
[2881.06 --> 2888.58]  What the companies are building towards is something they can see and talk and interact with the world and click buttons on a mouse and follow directions.
[2888.58 --> 2892.16]  And as you said, I mean, and I want to point out, you are far more advanced.
[2892.26 --> 2893.94]  Anyone who's listening to this is like, what are you talking about?
[2894.34 --> 2896.50]  Please try the kind of tools we talked about for a while.
[2896.58 --> 2902.54]  You will start to get the picture that Alexander was bringing to the table because I think you're on the cutting edge here of what you're telling me.
[2902.90 --> 2904.88]  And I think that's the image I'm seeing too, right?
[2904.88 --> 2915.70]  Which is these pieces are starting to come together in a way that feels like working with an actual person as opposed to chatting with a chatbot and then feeding out a picture and then having an image generator work.
[2915.78 --> 2917.90]  It's all supposed to clearly become one thing soon.
[2918.60 --> 2923.10]  Is there something that you wish could have been in the book that hasn't been in the book?
[2923.10 --> 2927.42]  Is it already time for a second book, Ethan?
[2927.52 --> 2929.70]  Your book is selling incredibly well here in the Netherlands.
[2930.04 --> 2931.80]  What is missing from the book?
[2932.16 --> 2932.90]  That's wonderful to hear.
[2933.00 --> 2937.96]  I don't think, you know, so because we've been in generation two for a while, I don't think anything is fundamentally different.
[2938.12 --> 2940.22]  I think more evidence has come out supporting the points of the book.
[2940.32 --> 2945.56]  I haven't found anything that I have written that I would change because it's inaccurate.
[2945.56 --> 2954.28]  I think I did not do enough to see how much of a role agents will play in the future, autonomous AI systems that I mentioned those.
[2954.38 --> 2957.86]  But I think that I thought they were further away than they are right now.
[2957.92 --> 2959.68]  And I think that they're going to be closer than we think.
[2959.94 --> 2965.88]  I think the other thing that, as I mentioned earlier, is I thought the scaling law might run its course sooner than it has.
[2965.96 --> 2975.38]  Because when I talked to people a year and a half ago or whenever I was writing the book, they were more pessimistic about, it was more divided about whether the AI could, you could keep getting AI smarter by just adding more computing power.
[2975.56 --> 2978.42]  That question seems to have gone away and the answer seems to be yes.
[2978.56 --> 2983.90]  So I think that if anything, I think I underestimated the level of acceleration that we might be facing.
[2984.38 --> 2986.32]  You've been very generous with your time.
[2986.46 --> 2987.64]  Thank you so much for doing this, Nathan.
[2988.26 --> 2989.22]  Thank you very much.
[2990.96 --> 2992.78]  Dan nog een tip voor de luisteraars.
[2993.50 --> 2996.38]  Een essay met de titel Machines of Loving Grace.
[2996.44 --> 2996.80]  Oh ja.
[2997.74 --> 3000.60]  Van Dario Amodij.
[3000.96 --> 3002.50]  De co-founder van Anthropic.
[3002.50 --> 3003.62]  Yes.
[3003.96 --> 3008.74]  En dat is eigenlijk, de subtext is how AI could transform the world for the better.
[3009.38 --> 3013.28]  Een optimistisch verhaal over AI heb ik ook wel eens nodig.
[3013.78 --> 3016.94]  En ik werd wel uitgedaagd door deze essay en de discussies.
[3017.30 --> 3022.28]  Het is best wel, het heeft een discussie gecreëerd van oké, inderdaad, dit zijn ook allemaal mooie zaken.
[3022.44 --> 3027.12]  Het gaat wel medicijnen uitvinden en mensen uit stomme banen halen en dat soort dingen.
[3027.12 --> 3035.22]  Ja, ik tip hem even ter uitdaging voor de cynisten en ter bevestiging van de optimisten.
[3035.76 --> 3039.06]  Stuur dit door in je omgeving als er mensen bezig zijn rondom dit onderwerp.
[3039.12 --> 3044.48]  Het is een leuke lijst van mogelijke dingen die gaan gebeuren die wel heel erg goed en leuk zijn.
[3045.30 --> 3050.24]  Ja, nou ook helemaal in het verlengde van het gesprek wat ik met Ethan Mollek had.
[3050.78 --> 3052.58]  Dat artikel, waar kunnen mensen dat vinden?
[3052.58 --> 3060.58]  In de show notes, het staat ook op de website van Dario Modai zelf en dat is darioamodai.com.
[3061.70 --> 3064.56]  Oké, en het artikel heet, voor als mensen dat willen googlen?
[3065.00 --> 3071.80]  Machines of Loving Grace en het gedicht All Watched Over by Machines of Loving Grace.
[3072.08 --> 3073.76]  Ook een tip, kan je zelf even googlen.
[3074.38 --> 3079.28]  Nou, heel goed. Het boek Co-intelligentie kun je bestellen op co-intelligentie.nl.
[3079.28 --> 3082.78]  Dit was Poki. Bedankt aan Danny Vermeulen voor de edit.
[3084.04 --> 3087.28]  Vergeet je niet te abonneren op onze nieuwsbrief. Dat kan via AIReport.email.
[3088.84 --> 3092.14]  Het was hem weer, wietse. Volgende week zijn we weer met een gewone aflevering.
[3092.24 --> 3094.82]  Het was een beetje een rare aflevering, maar het kan ook wel een keer.
[3095.24 --> 3097.42]  Als we er maar gewoon iedere week zijn, daar gaat het om, toch?
[3097.48 --> 3100.12]  Nou, sorry hoor. Ik vind dat enorm veel waarde.
[3100.24 --> 3103.22]  Ik bedoel alleen maar te zeggen, we hebben geen nieuwsaflevering gedaan.
[3103.22 --> 3108.10]  Sorry. Ik heb dat interview helemaal nog niet gehoord.
[3108.22 --> 3109.82]  Ik ben het net zo fresh als de luisteraar.
[3110.04 --> 3112.04]  Dus het wordt vast een goed interview geweest.
[3112.24 --> 3114.40]  Ga je echt podcast terugluisteren, dan kun je hem ook horen.
[3115.22 --> 3116.12]  Oké. Dag.
[3116.84 --> 3117.12]  Dag.
[3117.12 --> 3117.50]  Tag.
[3117.74 --> 3118.28]  Dag.
[3118.28 --> 3118.34]  Dag.
[3119.26 --> 3121.34]  Dag.
[3121.54 --> 3121.92]  Dag.
[3121.92 --> 3122.14]  Dag.
[3122.14 --> 3122.44]  Dag.
[3122.44 --> 3122.62]  Dag.
[3126.40 --> 3134.54]  Dag.
[3134.54 --> 3135.78]  Panning.
[3136.60 --> 3138.68]  Dag.
[3140.68 --> 3140.76]  Dag.
[3143.98 --> 3144.56]  Dag.

Video title: 9 miljoen podcasts, speciaal voor jou
Youtube video code: T60khfcwdk4
Last modified time: 2024-01-22 10:06:02

------------------ 

[0.72 --> 4.44]  Zet jij je verwarming nog aan met zo'n ouderwetse thermostaatknop?
[5.12 --> 7.46]  Dan is Eneco Dynamics niks voor jou.
[8.04 --> 9.86]  Of bedien jij je thermostaat met een app?
[10.52 --> 12.76]  Dan is Eneco Dynamics misschien wel iets voor jou.
[13.50 --> 18.78]  Doe de test op eneco.nl slash test om te ontdekken of een dynamisch energiecontract bij jou past.
[19.36 --> 21.32]  Mensen helpen een bewuste keuze te maken.
[22.24 --> 23.82]  We doen het nu. Eneco.
[23.82 --> 31.26]  Als nu echt die vitale processen geraakt worden en we hebben echt te weinig cybercapaciteit bijvoorbeeld.
[31.64 --> 34.38]  Welke processen willen we dan kost wat kost in de lucht houden?
[34.84 --> 37.92]  En waar zetten we onze schaarse capaciteit op dat moment op in?
[38.28 --> 43.32]  In de nieuwe editie van Enter duiken we in Easydoor, de grootste cyberoefening van Nederland.
[43.90 --> 47.00]  Ontdek het belang van voorbereiden, oefenen en samenwerken.
[53.82 --> 56.98]  En dat we een bepaalde van ons kunnen kreeren kunstwerk om Rembrandt te veranderen.
[57.34 --> 58.78]  Met alleen een paar kliks.
[59.52 --> 64.76]  Als een artificiale intelligence kreert een stuk van kerk dat gewoon zoals een Basquiat original ziet,
[65.08 --> 65.62]  dan werkt het.
[67.38 --> 70.28]  Welkom bij POKI, een podcast over kunstmatige intelligentie.
[70.38 --> 74.66]  Waarin wij, Wietsehage en Alexander Klubbing, je bijpraten over kunstmatige intelligentie.
[74.94 --> 75.42]  Met vandaag.
[75.60 --> 79.22]  Politici in de Europese Unie willen graag laten zien dat ze het niet gaan laten gebeuren
[79.22 --> 81.92]  dat de negatieve impact van Ayaya ongestraft voorbij kan gaan.
[81.92 --> 85.34]  En het eerste onderzoek over de impact van Ayaya op jouw baan is uit.
[85.44 --> 86.44]  En daar is goed nieuws over.
[86.88 --> 90.98]  En Wietseh gaat uitleggen waarom het bedrijf met een emoji als logo belangrijk is.
[91.44 --> 94.60]  En we duiken dieper de wereld in van de kunst.
[95.06 --> 97.40]  Want die gaat door Ayaya flink op zijn kop staan.
[103.94 --> 104.96]  God, die tune is voorbij.
[105.06 --> 106.42]  We kunnen weer praten als normale mensen.
[107.06 --> 107.92]  Heb jij gisteren...
[107.92 --> 110.80]  Ja, we nemen dit op de dag nadat op 1 is uitgezonden.
[110.80 --> 112.30]  Heb je op 1 gekeken, Wietseh?
[112.32 --> 112.96]  Ik heb gekeken.
[113.08 --> 113.98]  En ben je opgebleven?
[114.42 --> 115.72]  Voor mijn achtste présence.
[116.16 --> 116.44]  Zeker.
[117.12 --> 118.14]  Ik ben opgebleven.
[118.24 --> 118.86]  Ik heb gekeken.
[119.26 --> 120.00]  Ik vond dat je het goed deed.
[120.26 --> 121.36]  Ik stuurde als grapje naar jou.
[121.52 --> 122.72]  Ik zat naar mijn laptop te roepen.
[122.92 --> 123.66]  Wat die gast zegt.
[124.92 --> 125.82]  Nee, ik vond het...
[125.82 --> 127.80]  Ik vond het nog best wel een boeiend...
[127.80 --> 128.44]  Je was het met me eens.
[128.54 --> 129.02]  Nou, dat is fijn.
[129.52 --> 130.42]  Ja, ik was het wel met je eens.
[130.54 --> 131.82]  Nou, dat hoeft niet altijd natuurlijk.
[131.92 --> 132.50]  Dat is ook saai.
[132.64 --> 133.10]  Maar nu wel.
[134.22 --> 134.50]  Zeker.
[135.84 --> 137.60]  Wat vond je van die mevrouw die erbij zat?
[137.84 --> 139.70]  Die een replica-avatar had?
[139.70 --> 140.86]  Nou, ik vond het wel bijzonder.
[140.96 --> 143.66]  Ik vond sowieso een hele leuke, amikale vrouw.
[143.76 --> 144.22]  Wat ben ik?
[144.32 --> 146.04]  Ik vond haar wel interessant om naar te luisteren ook.
[146.12 --> 148.38]  En ook een soort van wel zelfreflectie.
[148.54 --> 151.06]  Een soort zelfkennis over dat het eigenlijk ook wel bijzonder is.
[151.16 --> 152.14]  Maar ja, je moet er maar gaan zitten.
[152.38 --> 153.62]  We hebben het de vorige keer over gehad.
[153.70 --> 155.00]  Er zit dan een soort stigma op.
[155.32 --> 156.50]  Hebben wij het idee, of in ieder geval.
[156.60 --> 157.24]  Dat idee heb ik.
[157.94 --> 159.10]  Dus als het dan zo verteld wordt.
[159.22 --> 159.70]  En ik denk dat...
[160.70 --> 165.52]  Ik weet niet zo goed hoe oud die technologie van replica is.
[165.58 --> 166.36]  Dat dacht ik ook nog.
[166.36 --> 170.02]  Het voelt wel een beetje als een soort van vorige generatie AI.
[170.68 --> 171.88]  Waardoor ik ook dacht...
[171.88 --> 176.04]  Wat we daar beoordelen met elkaar is eigenlijk iets van gisteren.
[176.78 --> 177.32]  Ja, ja, ja.
[177.58 --> 179.94]  Dus ik zat ook wel te kijken met een soort van...
[179.94 --> 181.68]  En dat bracht jij op een gegeven moment ook in.
[181.82 --> 184.14]  Van ja, de technieken liggen eigenlijk al op tafel.
[184.24 --> 186.14]  Dat is een kwestie van die technieken aan elkaar koppelen.
[186.72 --> 188.90]  En dan zitten we hier ineens met een heel ander item met elkaar.
[189.60 --> 191.78]  Maar goed, ik denk dat dat zich gewoon nog moet bewijzen.
[191.78 --> 193.92]  Nou, meer dingen die zich nog moeten bewijzen is...
[193.92 --> 196.58]  De Economist heeft besloten dat ze onderzoek gaan doen...
[196.58 --> 199.12]  En wat de impact van AI op white-collar jobs is.
[199.68 --> 200.78]  Dus ze gaan een soort van...
[201.60 --> 203.98]  Elke paar maanden gaan ze bijhouden...
[203.98 --> 205.76]  Kunnen we al iets zien in het grafiekje?
[206.22 --> 210.76]  Er was enorme invloed op de instroom van white-collar jobs tijdens COVID.
[210.94 --> 213.20]  Want toen allerlei mensen die normaal serviceberoepen hadden...
[213.20 --> 214.52]  Gingen opeens white-collar jobs doen.
[214.70 --> 216.28]  Dus COVID was een enorme piek.
[216.66 --> 218.78]  Maar daarna stabiliseerde dat eigenlijk heel erg snel...
[219.52 --> 220.66]  Naar het niveau van daarvoor.
[220.66 --> 223.18]  En nu gaan ze dus proberen te kijken...
[223.18 --> 227.60]  De impact op generatieve AI zou zijn op white-collar jobs.
[227.74 --> 230.24]  Dus de banen waar mensen met informatie te maken hebben.
[231.04 --> 232.50]  Gaat dat dan ook echt gebeuren?
[232.88 --> 234.42]  En vooralsnog is de conclusie...
[234.42 --> 236.58]  Ze hebben dus nu hun eerste onderzoek gepresenteerd.
[237.04 --> 238.18]  Er is niets van merkbaar.
[238.42 --> 240.28]  Ze zeggen ook, het is misschien ook nog wat vroeg.
[240.44 --> 241.70]  Misschien zijn bedrijven langzaam...
[241.70 --> 245.48]  Maar zeker nog maar stapjes aan het nemen om hier iets mee te doen.
[245.96 --> 248.58]  Maar ze gaan dat in ieder geval de komende maanden in de gaten houden.
[248.58 --> 249.16]  Dus dat is fijn.
[249.28 --> 250.30]  Dus daar kunnen wij er ook naar kijken.
[250.66 --> 252.14]  Hoe vaak gaan ze dat checken?
[252.98 --> 254.46]  Ze zeggen iedere drie maanden.
[254.62 --> 255.24]  Oké, check.
[255.92 --> 256.32]  Interessant.
[256.84 --> 259.68]  Verder was er groot nieuws van de EU...
[259.68 --> 263.44]  Die hun AI Act heeft voorgesteld.
[264.22 --> 268.50]  En veel van die regelgeving lijkt te gaan...
[268.50 --> 270.20]  Of voorgestelde regelgeving, moet ik zeggen...
[270.20 --> 272.82]  Lijkt te gaan over gezichtsherkenning.
[272.82 --> 276.32]  En dat is denk ik wel heel vrij.
[276.32 --> 280.32]  Want heel veel poeha over AI gaat dus over banen...
[280.96 --> 283.44]  En gaat over of de mensheid gaat beëindigen.
[283.52 --> 286.48]  Maar er zijn ook hele concrete privacy-risico's...
[286.48 --> 288.94]  Die ons op hele korte termijn al kunnen gaan raken.
[289.36 --> 291.62]  En registreersherkenning is daar volgens mij absoluut één van.
[291.62 --> 297.58]  En eigenlijk gaan ze in dit voorstel verbieden...
[297.58 --> 300.54]  Dat dit soort technologie op straat ingezet mag worden.
[301.54 --> 304.42]  En ik las dus een fantastisch voorbeeld...
[304.42 --> 307.58]  Van hoe gezichtsherkenning als technologie misbruikt kan worden.
[307.68 --> 309.34]  Namelijk in Madison Square Garden.
[309.42 --> 310.96]  Dat is een beetje de Ziggo Dome van New York.
[311.54 --> 312.36]  Daar was een...
[312.36 --> 315.06]  Dat moederbedrijf heeft een rechtszaak tegen zich lopen.
[315.72 --> 317.08]  En dat vinden ze irritant.
[317.26 --> 318.62]  Dat er een rechtszaak tegen ze loopt.
[318.78 --> 322.44]  En daarom hebben ze de advocaten van de tegenpartij...
[322.44 --> 325.22]  De toegang ontzegd tot Madison Square Garden...
[325.22 --> 326.34]  Middels gezichtsherkenning.
[326.42 --> 328.60]  Dus zij hebben een database met alle gezichten...
[328.60 --> 330.36]  Waarschijnlijk hebben ze dat van LinkedIn getrokken of zo.
[330.84 --> 334.14]  Van alle advocaten van de tegenpartij.
[334.62 --> 336.22]  En die kunnen dus niet meer naar concerten.
[336.22 --> 338.94]  Dat is hun manier om die tegenpartij te zieken.
[339.42 --> 342.14]  En blijkbaar is dat iets wat mag.
[342.68 --> 343.92]  Wat ik al opvallend vind.
[343.92 --> 346.34]  En blijkbaar is dat ook iets wat gemakkelijk kan.
[346.58 --> 347.96]  Wat ik opvallend vind.
[348.44 --> 349.94]  Maar het geeft ook een klein inkijkje...
[349.94 --> 352.70]  Hoe dit helemaal uit de hand zou kunnen lopen...
[352.70 --> 356.70]  Als bedrijven dit soort dingen gaan toepassen voor hun eigen win.
[357.18 --> 360.38]  Maar dat is ook nog maar een schim...
[360.38 --> 361.94]  Met wat voor problemen dat zou kunnen hebben.
[362.24 --> 365.02]  Als je ongezien op straat zou willen lopen.
[365.02 --> 369.40]  Of in allerlei foto's en videobeelden te zien bent...
[369.40 --> 371.58]  Op momenten dat je gewoon anoniem wil zijn.
[371.58 --> 374.96]  Wat toch een groot goed is en waar je niet zo vaak aan denkt...
[374.96 --> 375.80]  Dat dat wel fijn is.
[375.88 --> 376.82]  Dat je dat nog steeds hebt.
[377.38 --> 379.44]  Dus dat is een voorstel.
[379.58 --> 381.20]  Daar gaan ze nog een paar jaar over bakken leien.
[381.36 --> 382.34]  Kunnen we dat ergens lezen?
[382.46 --> 383.24]  Dan zullen we zien...
[383.24 --> 384.82]  Ja, zeker kun je dat voorstel lezen.
[385.36 --> 387.32]  En dat wordt ook druk gelezen door lobbyisten...
[387.32 --> 388.52]  Van grote techbedrijven.
[388.56 --> 389.74]  Want die hebben er alle belang bij.
[390.04 --> 392.40]  Om die EU-regelgeving klein te houden.
[392.68 --> 394.58]  Met name de techbedrijven die een beetje voorlopen.
[395.96 --> 398.74]  Dus OpenAI bijvoorbeeld heeft er belang bij...
[398.74 --> 401.50]  Om dit soort wetgeving toch soepeler te krijgen.
[402.46 --> 405.60]  En het zijn gek genoeg de bedrijven die dan een beetje achterlopen.
[406.30 --> 409.60]  Die het veel makkelijker vinden dat dit soort dingen gereguleerd worden.
[409.66 --> 413.00]  Omdat ze daarmee ook weer hun achterstand een beetje...
[413.00 --> 416.06]  Nou ja, het speelveld wordt geniveleerd zomaar te zeggen.
[416.06 --> 419.22]  Dat is regulatory capture, heet dat volgens mij.
[420.60 --> 421.34]  Regulatory capture.
[421.34 --> 422.84]  We zullen zien over een paar jaar waar dit uitkomt.
[422.84 --> 425.18]  Het voordeel eruit doen dat het gereguleerd wordt.
[425.24 --> 426.60]  Dat wil je dan eigenlijk als speler.
[426.88 --> 427.58]  Interessant is dat.
[428.02 --> 430.08]  En die tekst, is dat leesbaar?
[430.22 --> 433.32]  Moet ik dat door Chattieptie heen halen om het Explain Like I'm Five te maken?
[433.90 --> 435.10]  Ik heb het geprobeerd.
[435.60 --> 437.06]  En je moet het door Chattieptie heen halen.
[437.10 --> 437.32]  Check.
[437.50 --> 438.62]  Tenminste, voor mij is dat nodig.
[439.00 --> 441.08]  Maar ik ben ook bijzonder gevoelig voor juridische taal.
[441.08 --> 444.58]  Dus dat is iets waar ik Chattieptie graag voor gebruik.
[445.94 --> 447.44]  Hugging Face, wilde jij het over hebben?
[447.56 --> 449.58]  Ja, ik dacht dat dit is wel iets voor de luisteraar...
[449.58 --> 450.88]  om sowieso bekend mee te zijn.
[451.56 --> 452.34]  Dat vind ik.
[453.08 --> 457.44]  En Hugging Face, ik ben eigenlijk Hugging Face best wel laat tegengekomen.
[458.40 --> 458.96]  Ja, ik weet niet.
[459.00 --> 460.98]  Ik heb altijd het idee dat ik er dan vroeg bij ben met dingen.
[461.06 --> 461.74]  Maar ik word ook ouder.
[461.86 --> 465.00]  Dus op een gegeven moment zit je er nu misschien niet meer helemaal top of the game.
[466.56 --> 469.64]  Ik kwam een paar jaar geleden Hugging Face tegen toen ik aan het kijken was...
[469.64 --> 472.64]  naar speech-to-text oplossingen.
[473.52 --> 476.18]  Om te kijken van, oké, kan ik dit ergens downloaden op GitHub of zo?
[476.26 --> 477.40]  Ik wil iets draaien.
[477.48 --> 479.12]  Ik wil gewoon van een mp3 naar tekst.
[479.12 --> 480.52]  Dat was wat ik voor doel had.
[480.60 --> 481.36]  En toen was ik een beetje aan het zoeken.
[481.50 --> 485.38]  En toen in een van die repositories op GitHub van open source code...
[485.38 --> 491.30]  stond een raar emoji met een soort blij poppetje die al bloosend naar je zwaait.
[491.92 --> 492.98]  Toen dacht ik, oké, wat is dit?
[493.10 --> 498.32]  Dit zal wel een of andere federated, rare community ding zijn of zo.
[498.40 --> 499.98]  Een of andere, ja, een beetje een hacky ding.
[500.14 --> 501.84]  Toen klikte ik erop, toen kwam ik op Hugging Face.
[501.84 --> 504.60]  En dat is eigenlijk een platform.
[504.80 --> 507.60]  Het ziet er heel vrolijk uit door die emoji en ook heel erg toegankelijk.
[507.70 --> 510.64]  Want je denkt dit is een soort van gezellig community project.
[511.06 --> 512.24]  Het is gewoon een bedrijf.
[512.36 --> 513.94]  Er zitten allemaal grote investeerders in.
[514.06 --> 515.16]  Dat wist ik eerst ook niet.
[515.24 --> 516.30]  Dat heb ik nog even uitgezocht.
[517.14 --> 518.38]  Maar waarom is het zo belangrijk?
[518.48 --> 522.10]  Omdat wat je ziet, en dat zag je ook een beetje in de begindagen van GitHub...
[522.10 --> 524.24]  inmiddels door Microsoft overgenomen...
[524.24 --> 526.82]  is dat dit soort platforms een soort speel worden...
[526.82 --> 530.20]  of een beetje een spin in het midden van een spinneweb van zo'n beweging.
[530.28 --> 534.92]  En deze beweging die ik dan nu beschrijf is de beweging van open source machine learning...
[534.92 --> 537.66]  of wat we vandaag de dag eigenlijk weer veel meer AI zijn gaan noemen.
[538.32 --> 542.10]  En concreet is het dus zo dat als bijvoorbeeld een bedrijf...
[542.76 --> 546.80]  of een onderzoeksteam bij een universiteit een nieuw onderzoek uitbrengt...
[546.80 --> 549.96]  het gaat vaak gepaard met een onderzoekspaper en daar zit dan code bij.
[550.12 --> 551.10]  Zo van hier heb je een model.
[551.10 --> 552.12]  We hebben hem alvast getraind.
[552.20 --> 553.52]  Dan kan je een beetje ermee spelen.
[553.62 --> 557.10]  Dat kan echt zijn van tumoren herkennen in MRI scans...
[557.76 --> 559.46]  tot en met speech-to-text.
[559.62 --> 563.56]  Eigenlijk alles wat je met machine learning-achtige oplossingen kunt bereiken.
[564.10 --> 565.96]  En die uploaden ze dan vaak naar Hugging Face.
[566.04 --> 570.74]  Want op Hugging Face zit eigenlijk een gigantische community van en mensen en data.
[571.36 --> 573.70]  Dus er staan voorgetrainde modellen op.
[574.32 --> 575.66]  Dus die kan je zo gebruiken, zeg maar.
[575.72 --> 578.70]  Dus dan kan je meteen van jouw mp3'tje naar text toe.
[578.70 --> 579.88]  Want dat model krijg je dan zo.
[579.94 --> 580.96]  Oeh, kan je zo downloaden.
[581.10 --> 582.90]  Dat is een voorbeeld van zo'n model.
[583.50 --> 584.62]  En dan bedriek je dan tekst.
[584.94 --> 586.06]  Ja, en bedriek je dan tekst.
[586.42 --> 591.12]  Maar vaak is het zo dat dat onderzoek wat uitgebracht wordt niet het getrainde model bevat.
[591.48 --> 593.32]  Zo zeggen ze, dit is hoe je hem kan trainen.
[593.44 --> 597.16]  Dus als jij zelf heel veel data hebt en je traint het op deze manier...
[597.16 --> 599.62]  dan komt daar een getraind model uit wat jij kan gebruiken.
[600.08 --> 603.24]  Maar waar ik, als ik ben meer toegepast, ik ben geen wetenschapper.
[603.24 --> 605.48]  Dus ik wil gewoon, geef me gewoon wat ik meteen kan.
[605.84 --> 606.76]  Een Python dingetje.
[607.16 --> 609.64]  Een stuk script dat ik gewoon meteen mijn mp3 erin kan knallen.
[609.96 --> 611.50]  En dan kom je op Hugging Face terecht.
[611.60 --> 618.70]  Dat gaat inmiddels zover dat ze op Hugging Face ook bij ieder model een klein interface aan de zijkant hebben gemaakt.
[618.80 --> 619.70]  Kan je meteen even testen.
[619.70 --> 621.14]  Dus dan draaien ze hem even voor je.
[621.52 --> 624.66]  Er zitten allemaal van die hele kleine lekkere toeltjes in.
[625.62 --> 628.46]  En waarom ik het wel relevant vond om het vandaag even te vertellen is...
[628.46 --> 632.30]  omdat zijn hele mooie, ja, een soort top 40 is het.
[632.38 --> 639.50]  Het zijn er geen 40, maar een rankinglijst bijhouden van de verschillende GPT-achtige modellen.
[639.64 --> 644.14]  Dus het zijn grote taalmodellen waar je mee kan praten, zoals we dat nu gewend zijn van ChatGPT.
[644.14 --> 650.48]  Alleen dan niet gemaakt door OpenAI, maar door of andere bedrijven, andere instellingen, andere universiteiten.
[651.08 --> 652.16]  En er is een test gemaakt.
[652.26 --> 657.42]  Er zijn verschillende tests, waardoor je eigenlijk toch wel een soort objectief kunt meten van hoe goed zijn deze nou.
[657.52 --> 659.08]  En dat vind ik persoonlijk heel interessant.
[659.24 --> 663.64]  Want je wilt natuurlijk weten hoe dicht zijn ze in ieder geval bij GPT-4 of bij GPT-3,5.
[664.36 --> 668.12]  En dus in de show notes linken we sowieso die top.
[668.38 --> 670.64]  En bovenaan die top staat nu het Falcon model.
[670.64 --> 675.70]  Falcon is een model, komt vanuit een commerciële partij, maar die heeft een open source gemaakt.
[675.94 --> 679.52]  Want daar moet je goed op letten als je dit ook gebruikt binnen, zeker binnen een bedrijf.
[679.58 --> 680.90]  Dan mag ik het eigenlijk wel gebruiken.
[681.00 --> 681.82]  Even goed kijken.
[682.46 --> 684.96]  Maar de lijst op deze is redelijk gunstig.
[685.08 --> 689.36]  En wat je eigenlijk heel concreet kan, is dus Falcon downloaden vanuit Hugging Face.
[689.46 --> 690.94]  En schrik niet, dit gaat echt om gigabytes.
[691.16 --> 692.58]  Het is hartstikke groot allemaal.
[693.40 --> 698.26]  En dan moet je die drie grafische kaarten aanschaffen waar ik het aan het einde van de aflevering over heb gehad de vorige keer.
[698.26 --> 699.26]  Dus drie grote videokaarten.
[699.26 --> 701.38]  En dan kan jij thuis...
[701.38 --> 703.76]  Is dat zo'n videokaart van 14.000 euro?
[704.42 --> 709.02]  Nou, eigenlijk wel als je ze nieuw koopt op bol, zeg maar.
[709.10 --> 714.82]  Maar als jij gewoon drie kaarten haalt, tweedehands en aan elkaar koppelt en wat knutselt, dan kan jij toch wel een ent komen thuis.
[715.04 --> 715.52]  Oh ja.
[715.58 --> 716.70]  Maar dan moet je je wel voorstellen...
[716.70 --> 717.50]  Ik zat naar die Nvidia...
[717.50 --> 718.02]  Wat is het?
[718.22 --> 720.28]  Nvidia A100 te kijken.
[720.28 --> 727.14]  Ja, de A100 serie en de A1000 en de A2, dat zijn goudstaven met een PCI connector erop.
[727.52 --> 728.44]  Dat is heel bizar.
[728.58 --> 732.70]  Maar kun je wel gewoon op tweakers pricewatchen vinden.
[732.70 --> 733.60]  Ik vind dat altijd grappig.
[733.60 --> 734.94]  18.000 euro.
[735.10 --> 738.54]  De prijs van een instapauto kan je gewoon een kaartje in je computer stoppen.
[740.44 --> 742.92]  En dan het mooie dus is dat je dan...
[742.92 --> 745.02]  Want ik denk dus waarom ik dat Hugging Face vertel.
[745.10 --> 746.80]  Het grappige is ook wel...
[746.80 --> 749.92]  Ze zijn ooit gestart als een soort Tamagotchi, dat heb ik begrepen.
[750.02 --> 752.86]  Dus het was eigenlijk een schattig start-upje die een Tamagotchi probeerde te maken.
[753.02 --> 754.62]  Ja, zeven, acht jaar geleden al.
[754.80 --> 756.80]  Want ze dachten dat dat moet weer bestaan op internet.
[757.44 --> 759.08]  En die Tamagotchi hebben ze geopen sourced.
[759.14 --> 761.18]  Dat open source project kreeg ineens heel veel aandacht.
[761.18 --> 763.78]  Toen dachten ze, mensen vinden machine learning blijkbaar cool.
[763.90 --> 767.22]  We praten echt over ver voor deze nieuwe hypegolf.
[767.90 --> 773.48]  En toen beseften ze zich, maar er is waarschijnlijk gewoon vraag naar een soort GitHub voor machine learning.
[773.74 --> 775.60]  En dan echt toegespitst op machine learning.
[775.60 --> 776.98]  Want je kan zeggen, GitHub kan dat al.
[777.02 --> 778.24]  Maar dat is heel algemeen.
[778.36 --> 779.94]  En dit is echt voor...
[779.94 --> 785.06]  Ja, ik wil spelen met hele kleine modelletjes waar ik dingen in kan op een robotje.
[785.42 --> 788.58]  Tot en met iets waar ik dus drie auto's aan videokaart voor moet kopen.
[788.58 --> 790.66]  Om OpenAI thuis na te kunnen doen.
[790.66 --> 792.52]  Daar zit mijn persoonlijke belang.
[792.62 --> 794.02]  Het lijkt mij gewoon ontzettend gaaf.
[794.44 --> 797.30]  Om een eigen large language model thuis te hebben draaien.
[797.46 --> 798.32]  Daar willen we toch naartoe.
[798.66 --> 802.62]  Er zijn dus heel veel van die mini-tooltjes die op Hugging Face draaien.
[802.88 --> 805.50]  Die zelfs simpele zielen als ik kunnen gebruiken.
[805.96 --> 812.14]  Dus een van de dingen die ik probeerde was inderdaad verschillende tekstmodellen tegelijkertijd iets laten uitvoeren.
[812.24 --> 813.12]  Dus je hebt één prompt.
[813.70 --> 815.32]  En dan zie je hoe Anthropic het doet.
[815.46 --> 816.82]  En je ziet hoe GPT-4 het doet.
[816.82 --> 818.74]  Maar je ziet inderdaad ook hoe die andere modellen.
[818.86 --> 823.06]  En ook van die Stanford en al die universiteiten hebben allemaal hun eigen modellen inmiddels.
[823.22 --> 824.16]  Dus je ziet dan...
[824.16 --> 827.68]  En dan zie je in een soort van splitscreens hoe je ziet hoe alles opgebouwd wordt.
[827.76 --> 829.16]  Nou, dat vond ik best wel grappig om te kijken.
[829.26 --> 831.68]  Maar ook wel cutting-edge dingen.
[831.68 --> 833.82]  Dus bijvoorbeeld video generatie door AI.
[833.98 --> 836.98]  Dus dat je een prompt maakt en dat die dan drie seconden video genereert.
[837.08 --> 838.38]  Daar zijn ook modellen van.
[838.94 --> 839.74]  En dan kun je gewoon...
[839.74 --> 840.86]  Ja, het is gewoon een tekstvak.
[841.06 --> 841.96]  En dan druk je op generate.
[842.06 --> 842.22]  Precies.
[842.22 --> 842.88]  En dan maakt hij het.
[842.96 --> 843.64]  Het is heel simpel.
[844.20 --> 847.68]  En allerlei dingen met...
[847.68 --> 851.00]  Bijvoorbeeld voor een podcast, als je die wilt transcriberen.
[851.06 --> 853.06]  Dat hij dan automatisch de sprekers uit elkaar trekt.
[853.12 --> 854.90]  Dat hij zegt dit zegt Wiet en dit zegt Alexander.
[855.32 --> 857.06]  En dat hij die stemmen uit elkaar trekt.
[857.16 --> 859.94]  Dat zijn allemaal kleine bouwsteentjes of zo.
[860.22 --> 861.24]  Waarvan je dan...
[861.24 --> 864.66]  Waar op de achtergrond waarschijnlijk allemaal heel ingewikkelde shit aan het draaien is.
[864.72 --> 866.36]  Maar waarbij je Huggingface niet zo heel veel van merkt.
[866.36 --> 872.92]  Dus zelfs voor Noob zoals ik is het wel grappig om die soort van meest gebruikte dingetjes door te gaan.
[873.34 --> 878.34]  Maar ik kan me voorstellen dat voor iets geavanceerdere mensen zoals jij en nog veel verder.
[879.66 --> 882.64]  Dat dat echt wel een bron is van al het nieuws.
[883.08 --> 886.58]  Dat zijn de ruwe bouwstenen op een makkelijk toegankelijke manier.
[887.10 --> 887.72]  Ja, en ik denk ook.
[887.98 --> 890.40]  Ik categorise mezelf best wel ook als Noob hoor.
[890.48 --> 891.58]  Want dit was voor mij...
[891.58 --> 893.34]  Ik kan wel een beetje programmeren en zo.
[893.34 --> 895.84]  Maar voor mij om echt het diepe...
[895.84 --> 897.70]  Het was heel moeilijk om deze dingen te testen.
[897.78 --> 899.56]  Ik ging dan toen er nog geen Huggingface was.
[899.64 --> 900.28]  Ik ging dan naar GitHub.
[900.38 --> 901.20]  Dan download die code.
[901.30 --> 902.26]  En dan kreeg ik de eerste error.
[902.36 --> 903.06]  Dan dacht ik echt zo.
[903.12 --> 903.82]  Deze terminologie.
[903.92 --> 904.62]  Ik snap hier niks van.
[904.78 --> 905.24]  Laat maar.
[905.36 --> 905.72]  Weet je al.
[906.04 --> 907.40]  Terwijl ik vaak gewoon even wilde kijken.
[907.88 --> 908.56]  Hoe goed is het?
[908.62 --> 909.78]  Ik wil er een JPG'tje in doen.
[909.86 --> 910.94]  En die vergroten keer drie.
[911.08 --> 912.40]  Of een ander poppetje inplakken.
[912.74 --> 913.88]  Gewoon even ermee spelen.
[914.26 --> 916.48]  Want ik ga dan vaak vanuit spelen naar installeren.
[916.86 --> 918.62]  Als ik heb gespeeld en ik ben onder de indruk.
[918.62 --> 920.32]  Dan wil ik nog wel een half uurtje erin stoppen.
[920.36 --> 921.34]  Om het op mijn laptop te zetten.
[921.86 --> 923.68]  En dat hebben ze ook wel heel erg goed gedaan.
[923.94 --> 925.68]  Dat eerste excitement van.
[925.76 --> 926.64]  Hé, dit doet iets.
[927.12 --> 928.52]  Ik ga toch nog even verder rommelen.
[929.20 --> 930.08]  En een beetje kijken.
[930.62 --> 932.06]  En er is een hele community met een forum.
[932.14 --> 933.62]  Met mensen die het ook al hebben geprobeerd.
[934.16 --> 934.86]  En het voelt echt.
[935.34 --> 936.32]  Het is bijzonder leuk.
[936.52 --> 937.62]  Leuk nu om te spelen.
[937.96 --> 940.16]  En wat jij net even tussen de neus en lippen door zei.
[940.96 --> 941.94]  Je hebt allemaal charts.
[942.06 --> 943.48]  Dus je hebt die charts waar ik net over had.
[943.66 --> 945.98]  Met het testen van die GPT-achtige modellen.
[945.98 --> 947.18]  Maar je kunt ook kijken.
[947.26 --> 949.38]  Welk model is er heel erg populair deze week?
[949.90 --> 950.70]  En daardoor kom ik er.
[950.80 --> 953.42]  Want ja, ik kan niet al die research in de gaten houden.
[953.52 --> 955.32]  Maar als ik een model omhoog zie schieten in die charts.
[955.38 --> 955.80]  Weet ik al.
[955.84 --> 956.58]  Daar is iets gebeurd.
[957.16 --> 958.52]  Dus dat is voor mij ook een hele leuke manier.
[958.62 --> 959.82]  Om een beetje op de hoogte te blijven.
[960.00 --> 960.30]  Ja, klopt.
[960.68 --> 962.18]  Nou, als je dingen wil gaan proberen.
[962.50 --> 964.00]  Met AI is Hugging Face.
[964.12 --> 965.22]  Echt een heel fijne plek.
[965.36 --> 967.26]  Dat kun je vinden op huggingface.co.
[967.70 --> 969.10]  Niet .com, maar .co.
[970.16 --> 972.86]  Wietje, wij gaan het vandaag hebben over kunst.
[972.86 --> 979.14]  Want dat is een sector die enorm in het licht is gekomen.
[979.28 --> 982.36]  Sinds de opkomst van al die plaatjesgeneratoren.
[983.38 --> 986.08]  En je zou je kunnen afvragen.
[986.24 --> 987.92]  Wat is de rol nog van de kunstenaar?
[988.64 --> 989.62]  Daar gaan we het vandaag over hebben.
[991.50 --> 993.06]  Waar ik over naast heb te denken.
[993.40 --> 995.68]  Ik hoor vaak als ik iets aan Mid Journey vraag.
[995.86 --> 997.82]  Of Dali om iets voor mij te tekenen.
[998.06 --> 998.92]  Dan vraag ik dat zelf.
[999.06 --> 1001.26]  Dat is de hele kunst van prompt engineering geworden.
[1001.26 --> 1006.60]  Dus het is nu niet meer de kunst om zelf helemaal het proces te doorlopen.
[1006.68 --> 1007.88]  Van het schilderen en het idee.
[1008.30 --> 1009.26]  Nee, je hebt het idee.
[1009.38 --> 1010.88]  Dat beschrijf je als een soort architect.
[1011.06 --> 1013.72]  Aan een generative AI.
[1014.18 --> 1016.30]  En die gaat het dan voor jou visueel creëren.
[1016.70 --> 1020.06]  Maar dus de creativiteit is verplaatst naar de prompt.
[1020.86 --> 1022.02]  Daar ben je nu creatief.
[1022.12 --> 1022.84]  Hoe je het vraagt.
[1022.92 --> 1023.44]  Wat je vraagt.
[1023.54 --> 1024.22]  Hoe je het combineert.
[1024.68 --> 1028.56]  Maar omdat die large language models ook kunnen prompten.
[1028.68 --> 1029.98]  Krijg je dan natuurlijk al het grapje.
[1029.98 --> 1033.08]  Daarom is Dali inmiddels ook een plugin in ChatGPT.
[1033.42 --> 1033.90]  Dat je zegt.
[1034.44 --> 1036.20]  Maak een interessante prompt voor mij.
[1036.38 --> 1037.76]  Zodat ik die kan doorgeven.
[1037.90 --> 1039.94]  Trouwens geef hem zelf maar door aan Dali.
[1040.40 --> 1043.70]  En dan schrijf je die creativiteit weer een stukje op.
[1043.90 --> 1044.26]  Namelijk.
[1044.96 --> 1047.00]  Wat vraag je om de prompt te maken.
[1047.24 --> 1048.02]  Om het ding te maken.
[1048.52 --> 1049.88]  Dan kom je op een soort van.
[1050.94 --> 1052.66]  First mover question in de filosofie.
[1052.76 --> 1053.68]  Daar schrap je altijd van.
[1053.82 --> 1054.36]  Oké.
[1054.44 --> 1055.40]  Je kan het de wereld.
[1055.64 --> 1057.14]  Het universum is gemaakt door God.
[1057.20 --> 1057.90]  Dan hou je God weg.
[1058.02 --> 1058.58]  Wordt de Big Bang.
[1058.58 --> 1059.86]  En dan is wie heeft de Big Bang gedaan.
[1060.00 --> 1061.12]  Dat wordt heel irritant.
[1061.20 --> 1062.44]  Een soort turtles all the way down.
[1062.50 --> 1063.20]  Achtig vraagstuk.
[1063.64 --> 1065.14]  Maar die prime mover.
[1065.22 --> 1065.78]  Zoals ze dat noemen.
[1065.86 --> 1066.68]  Dat eerste zetje.
[1066.74 --> 1067.34]  Daar kom je nooit.
[1067.78 --> 1069.26]  En het voelt voor mij ook een beetje zo.
[1069.72 --> 1070.28]  Met die.
[1070.70 --> 1072.28]  Waar zit dan de creativiteit nog?
[1072.34 --> 1073.38]  Want je schuift hem steeds op.
[1073.50 --> 1073.80]  Je gaat.
[1073.96 --> 1074.58]  Wat ik net zei.
[1074.68 --> 1075.90]  Je gaat op een gegeven moment vragen.
[1075.96 --> 1076.80]  Om proms te creëren.
[1076.88 --> 1078.20]  En is jouw vraag om proms te creëren.
[1078.28 --> 1079.18]  Weer jouw creativiteit.
[1079.18 --> 1082.66]  Waar wij gaan verdwijnen als mens.
[1082.74 --> 1084.50]  Als ik nog enigszins te volgen ben.
[1085.56 --> 1086.88]  Is in zo'n ketting.
[1087.00 --> 1089.70]  Als je eigenlijk dat draad aan allebei de kanten aan elkaar koppelt.
[1089.74 --> 1090.40]  In een soort loop.
[1090.80 --> 1091.66]  En dan echt zegt.
[1092.40 --> 1094.20]  Ga maar de komende jaren.
[1094.76 --> 1095.90]  Kunst creëren.
[1096.14 --> 1097.04]  Of wat dan ook creëren.
[1097.20 --> 1099.46]  Door als een soort supervisor aan andere modellen.
[1099.66 --> 1101.28]  Constant interessante proms te vragen.
[1101.34 --> 1102.74]  Die ze weer doorsturen naar Mid Journey.
[1103.26 --> 1104.14]  En weet je wat.
[1104.14 --> 1106.94]  Ik voeg er nog 5% randomness aan toe met onderwerpen.
[1107.04 --> 1108.32]  Die lijst mag je ook zelf verzinnen.
[1108.90 --> 1110.20]  En ik kom graag bij je terug.
[1110.84 --> 1113.08]  In jouw gallery in VR binnenkort.
[1113.26 --> 1114.88]  Waarin jij de komende twee jaar.
[1115.24 --> 1116.62]  Met een stukje randomness erin.
[1116.74 --> 1117.82]  Voor mij kunst gaat maken.
[1118.52 --> 1119.18]  Wat is dan.
[1119.26 --> 1119.74]  Dan kan je nog zeggen.
[1120.08 --> 1122.06]  Ja maar dan was jouw creativiteit toch weer.
[1122.18 --> 1123.46]  Om dit hele plan neer te zetten.
[1123.84 --> 1124.26]  Dat klopt.
[1124.34 --> 1124.92]  Dus daarom zeg ik.
[1125.00 --> 1126.64]  Volgens mij gaan we er niet uitkomen.
[1126.76 --> 1127.58]  Maar je kan wel denk ik.
[1127.58 --> 1128.76]  Op een gegeven moment iets aanzetten.
[1128.86 --> 1129.96]  Waarvan je wegloopt.
[1130.30 --> 1131.96]  En dat stukje waarin je zegt.
[1132.40 --> 1133.58]  Ik heb het zaadje geplant.
[1133.58 --> 1136.06]  Dat is ook in de theologie.
[1136.18 --> 1137.14]  Een van de ideeën van.
[1137.42 --> 1138.14]  God was er wel maar.
[1138.24 --> 1139.32]  En nu is hij weggegaan.
[1139.62 --> 1140.36]  Ja doei.
[1140.64 --> 1141.56]  Dat zit voor mij ook in.
[1141.60 --> 1142.74]  The Hitchhiker's Guide to the Galaxy.
[1142.98 --> 1143.28]  Van ja.
[1143.34 --> 1144.44]  God vond het op een gegeven moment niet meer interessant.
[1144.64 --> 1145.06]  Het universum.
[1145.22 --> 1146.32]  Saai is hier weggelopen.
[1146.76 --> 1148.04]  Nou dat voelt hier ook een beetje.
[1148.40 --> 1149.86]  Dat jouw creativiteit.
[1149.94 --> 1151.66]  Is de tik die je geeft.
[1151.76 --> 1153.24]  Aan die grote Ruby Goldberg machine.
[1153.36 --> 1154.82]  Waar ineens allemaal ballonnetjes en kaarsen.
[1154.82 --> 1155.64]  Gaan ontploffen.
[1155.84 --> 1156.80]  Dat jij zo poep.
[1156.90 --> 1157.54]  En dan loop je weg.
[1157.64 --> 1158.74]  En dan gaat dat zich als een soort.
[1160.20 --> 1161.30]  Generative algoritme.
[1161.48 --> 1162.78]  Helemaal laten groeien zeg maar.
[1162.78 --> 1163.30]  Maar.
[1163.30 --> 1165.00]  Je hebt het onderwerp aangesneden.
[1165.26 --> 1166.70]  Dus laten we het over het onderwerp gaan hebben.
[1166.86 --> 1167.12]  Inderdaad.
[1167.50 --> 1168.44]  Geloof je er inderdaad.
[1168.68 --> 1171.78]  Geloof je er in dat mensen AI kunst evenveel gaan waarderen.
[1172.10 --> 1175.66]  Als kunst gemaakt met een penseel.
[1176.00 --> 1176.72]  Nou ik denk dat.
[1178.26 --> 1179.14]  Het interessante.
[1179.60 --> 1181.44]  Er zijn een aantal vragen rondom kunst.
[1181.52 --> 1182.02]  Wat mij betreft.
[1182.10 --> 1183.16]  We kunnen het doen van wat is kunst.
[1183.16 --> 1183.80]  Wat is geen kunst.
[1183.86 --> 1184.98]  Vind ik even daarbuiten.
[1185.10 --> 1187.40]  Want dat is een eigen aflevering waard.
[1187.48 --> 1188.96]  En ik denk ook niet dat wij dat moeten doen.
[1188.96 --> 1189.24]  Nee.
[1189.38 --> 1190.92]  Dus we gaan heel even ervan uit.
[1190.98 --> 1192.26]  Dat we daar een soort van idee bij hebben.
[1192.30 --> 1193.26]  Wat we daar dan mee bedoelen.
[1193.62 --> 1195.70]  Nou ik kan wel uitleggen wat ik daarmee bedoel.
[1196.12 --> 1199.20]  Voor mij is een onderdeel van kunst.
[1199.36 --> 1200.58]  Niet alleen maar wat ik ga zien.
[1200.82 --> 1203.36]  En dat kan een performance zijn op een plein.
[1203.58 --> 1205.06]  Ergens tijdens Oerol op een eiland.
[1205.24 --> 1206.98]  Maar het kan ook iets zijn wat aan een muur hangt.
[1207.04 --> 1209.58]  Maar het kan ook iets zijn wat mij tot in mijn oren komt.
[1209.58 --> 1214.80]  Want het is een podcast waar iemand op een gegeven moment zoveel tijd en moeite en iets interessants in doet.
[1214.82 --> 1215.52]  Het kan ook kunst zijn.
[1215.56 --> 1215.66]  Ja.
[1215.86 --> 1217.70]  En ik zeg nu al ineens allemaal dingen.
[1217.84 --> 1218.16]  Tijd.
[1218.38 --> 1218.68]  Moeite.
[1218.82 --> 1218.96]  O.
[1219.02 --> 1220.64]  Is dat als er geen moeite is geen kunst?
[1220.76 --> 1222.36]  Dat is heel ingewikkeld in deze.
[1222.68 --> 1222.82]  Ja.
[1223.00 --> 1226.02]  Want volgens mij.
[1226.24 --> 1228.22]  En dat zie je ook een beetje met synthetische diamanten.
[1228.46 --> 1232.00]  Die diamanten zijn eigenlijk atomair volledig gelijk aan een diamant uit de grond.
[1232.40 --> 1234.36]  Maar daar heeft de aard niet heel lang op hoeven drukken.
[1234.66 --> 1238.90]  Dus voelt het toch alsof de moeite die in die diamant gegaan is niet helemaal klopt.
[1238.90 --> 1239.22]  Of zo.
[1239.48 --> 1242.14]  En daar kunnen we een soort van grappig over doen.
[1242.26 --> 1243.56]  Van wat een arbitrair verschil.
[1243.64 --> 1245.36]  Maar ik denk dat daar wel wezenlijk iets zit.
[1245.52 --> 1248.62]  Het is significant minder waard als je een synthetische diamanten hebt.
[1248.62 --> 1248.74]  Ja.
[1248.84 --> 1250.34]  Maar als je ze naast elkaar legt.
[1250.38 --> 1250.50]  Ja.
[1250.64 --> 1251.22]  Eindresultaat.
[1251.40 --> 1253.94]  Maar de weg ernaartoe schijnt dus ook waardevol te zijn.
[1254.00 --> 1254.34]  Dat kan.
[1254.98 --> 1256.28]  Dan gaan we dus eigenlijk zeggen.
[1256.86 --> 1258.52]  Als je een boterhammetje met pindakaas eet.
[1258.60 --> 1260.14]  Ook het smeren is onderdeel van het eten.
[1260.28 --> 1263.16]  We gaan gewoon het ding ervoor erbij trekken bij het hele proces.
[1263.70 --> 1265.16]  En dat mag volgens mij.
[1265.34 --> 1266.92]  En dan moet je er ook een verhaal bij vertellen.
[1267.04 --> 1267.62]  Misschien moet je het verhaal dan ook oplezen.
[1267.62 --> 1270.40]  Om er als mens waarde aan toe te kunnen kennen.
[1270.62 --> 1270.98]  Schijnbaar.
[1271.18 --> 1271.30]  Ja.
[1272.04 --> 1273.60]  Moest het verhaaltje moeten kloppen.
[1274.00 --> 1276.88]  Dat is een van de ideeën die ik persoonlijk.
[1277.24 --> 1278.60]  Dat ik dat intuïtief merk.
[1278.76 --> 1279.16]  Dat ik denk.
[1279.24 --> 1279.38]  Hé.
[1279.44 --> 1282.72]  Als daar een soort journey aan vast zit.
[1282.78 --> 1284.24]  Van hoe dat tot stand gekomen is.
[1284.36 --> 1286.44]  Dan vind ik het op een of andere manier meer waard.
[1286.52 --> 1287.78]  Of dat doet iets met waarde voor mij.
[1287.78 --> 1289.62]  En dat komt denk ik.
[1290.30 --> 1290.78]  Omdat.
[1291.50 --> 1293.98]  En dat is een beetje wat ik interessant vind.
[1295.76 --> 1296.26]  Als er.
[1297.20 --> 1300.72]  Dat zie je ook een beetje terug in het idee dat schaken.
[1301.10 --> 1301.88]  Is al heel lang.
[1302.54 --> 1304.14]  Zijn computers veel beter in schaken.
[1304.24 --> 1305.86]  Maar schaken is een enorme groeiende sport.
[1306.02 --> 1306.80]  Schaken tussen mensen.
[1307.80 --> 1310.36]  En terwijl computers kunnen het veel beter.
[1310.48 --> 1311.82]  Dus we zouden ook heel de dag op YouTube.
[1311.98 --> 1312.80]  Zee computers tegen elkaar.
[1312.98 --> 1313.78]  Maar toch doet dat ons.
[1313.78 --> 1314.50]  We gaan.
[1314.50 --> 1314.88]  Geen reet.
[1315.02 --> 1315.10]  Nee.
[1315.32 --> 1316.72]  Terwijl we zouden waarschijnlijk ook trouwens.
[1316.80 --> 1318.26]  Bijvoorbeeld een golf robot kunnen maken.
[1318.36 --> 1320.02]  Die constant Hoy & Once golft.
[1320.12 --> 1322.16]  Dit is gewoon technisch al lang mogelijk.
[1322.36 --> 1322.46]  Ja.
[1322.76 --> 1322.80]  Maar.
[1323.00 --> 1324.30]  Ga je toch geen livestream gaan kijken.
[1324.52 --> 1324.54]  Ja.
[1324.54 --> 1324.78]  Klop.
[1324.92 --> 1325.08]  Klop.
[1325.30 --> 1325.54]  Nou ja.
[1325.60 --> 1326.32]  Ik dacht toen wel.
[1326.44 --> 1327.02]  Want er is wel.
[1327.28 --> 1328.82]  Ik wou dus heel dit punt gaan maken.
[1328.90 --> 1329.66]  Zal ik te denken van.
[1330.22 --> 1330.38]  Ja.
[1330.44 --> 1331.02]  Zie je nou wel.
[1331.08 --> 1333.26]  Want ook bijvoorbeeld zelfrijdende Formule 1 auto's.
[1333.26 --> 1333.84]  Zou saai zijn.
[1333.94 --> 1334.88]  Ik had allemaal ideeën bedacht.
[1335.00 --> 1335.10]  Ja.
[1335.10 --> 1335.36]  Toen dacht ik.
[1335.40 --> 1335.50]  Ja.
[1335.56 --> 1336.48]  Maar ik kijk wel eens.
[1336.66 --> 1337.26]  Sporatisch hoor.
[1337.26 --> 1339.54]  Er is zo'n.
[1339.56 --> 1340.62]  Er komt nu een confession.
[1340.92 --> 1340.94]  Ja.
[1341.38 --> 1344.20]  Volgens mij heet het Jelles knikkerbaan.
[1344.28 --> 1345.88]  Maar er is een knikkerbaan op YouTube.
[1346.04 --> 1348.08]  Waarbij je een soort van Formule 1 race hebt tussen knikkers.
[1348.26 --> 1348.58]  Oké.
[1348.70 --> 1349.36]  En toen zat ik te denken.
[1349.42 --> 1349.52]  Ja.
[1349.58 --> 1349.84]  Maar die.
[1350.58 --> 1351.92]  Dat zijn geen mensen die knikkers.
[1352.94 --> 1353.16]  Nee.
[1353.24 --> 1353.56]  Precies.
[1354.20 --> 1355.34]  Waarom is dat dan wel leuk.
[1355.64 --> 1355.94]  Nou ja.
[1356.00 --> 1358.00]  Omdat je het verhaal van die Jelle kent inderdaad.
[1358.00 --> 1358.32]  En.
[1359.90 --> 1361.26]  Hij heeft die baan gemaakt.
[1361.40 --> 1362.32]  Dat helpt dan denk ik.
[1362.50 --> 1362.68]  Oh ja.
[1362.68 --> 1364.04]  Dat is wel wat je net zegt.
[1364.14 --> 1365.10]  Namelijk de stap ervoor.
[1365.28 --> 1366.32]  Is dan wat je nodig hebt.
[1366.32 --> 1367.90]  Het verhaal van de stap ervoor.
[1367.98 --> 1368.70]  Is wat je nodig hebt.
[1368.80 --> 1370.14]  Om het als geheel te kunnen waarderen.
[1370.64 --> 1371.30]  Nou misschien dus.
[1371.42 --> 1373.14]  En dan aansluit op wat jij nu zegt.
[1373.62 --> 1376.54]  Op het moment dat ik dan twee schaakcomputers tegen elkaar zie schaken.
[1376.74 --> 1376.84]  Ja.
[1376.84 --> 1378.00]  En de een is gemaakt door.
[1378.86 --> 1380.24]  Een jongen uit Noord-Holland.
[1380.60 --> 1380.72]  Ja.
[1380.72 --> 1381.98]  En de andere meisje in India.
[1382.14 --> 1382.72]  En we hebben twee.
[1383.02 --> 1385.90]  Door tieners gecreëerde algoritmes die tegen elkaar gaan schaken.
[1385.98 --> 1386.10]  Ja.
[1386.10 --> 1386.96]  Dan voel ik al ineens van.
[1386.96 --> 1387.14]  Ja.
[1387.14 --> 1388.12]  Ik zou het toch kunnen.
[1388.44 --> 1388.58]  Ja.
[1389.84 --> 1390.56]  Dat helpt.
[1390.62 --> 1390.74]  Ja.
[1390.88 --> 1391.34]  Dat merk ik.
[1391.42 --> 1391.54]  Ja.
[1392.14 --> 1392.64]  Wat vind jij?
[1394.14 --> 1394.70]  Nou ja.
[1394.76 --> 1395.34]  Ik denk altijd.
[1395.34 --> 1396.58]  Wat maakt dat mensen.
[1397.06 --> 1397.40]  Een soort van.
[1398.82 --> 1401.78]  Waarin massa uiteindelijk geïnteresseerd is.
[1401.94 --> 1402.14]  Is.
[1402.86 --> 1404.94]  In kunst biedt de mogelijkheid.
[1405.64 --> 1407.70]  En dat is redelijk uniek tegenwoordig.
[1408.18 --> 1411.08]  Om met z'n allen iets op te zoeken wat heel schaars is.
[1411.84 --> 1412.92]  Dus er hangt geen.
[1413.12 --> 1415.10]  Je kan een kopie ophangen van de nachtwacht.
[1415.10 --> 1416.28]  Maar er gaat niemand naar kijken.
[1416.58 --> 1417.80]  En dat kan een perfecte kopie zijn.
[1417.88 --> 1419.08]  Die niet van echt te onderscheiden is.
[1419.16 --> 1420.22]  En toch wil niemand dat zien.
[1420.22 --> 1422.10]  En dat is dus omdat je.
[1423.24 --> 1427.72]  Dat is het rare paradoxale van iets als het Rijksmuseum.
[1428.02 --> 1428.82]  Mensen willen massaal.
[1428.88 --> 1430.54]  Ze hebben geen zin om de JPEG te zien.
[1430.62 --> 1431.54]  Ze willen het echte ding zien.
[1431.60 --> 1432.30]  En waarom willen ze dat zien?
[1432.34 --> 1433.70]  Omdat alle andere mensen dat willen.
[1433.98 --> 1436.22]  Het is een soort van sociaal construct natuurlijk.
[1436.40 --> 1441.06]  Waarin we het heel belangrijk vinden dat andere mensen het heel erg waarderen.
[1441.06 --> 1443.58]  En ik denk dat dit dus door alle niveaus zou zijn.
[1444.08 --> 1445.88]  Dat iedereen op een bepaalde manier aan het kijken.
[1446.04 --> 1447.84]  Dus zelfs als je echt een kenner bent.
[1447.94 --> 1451.08]  En je haalt je neus op voor de massa's die voor de Mona Lisa staan.
[1451.76 --> 1455.32]  Dan nog denk ik dat het uitmaakt wat andere mensen.
[1455.44 --> 1457.62]  En dat kunnen andere experts zijn of wie je dan ook waardeert.
[1458.58 --> 1459.36]  Ervan vinden.
[1459.96 --> 1461.34]  En dit geldt voor alle cultuuruitingen.
[1461.44 --> 1462.70]  Echt niet alleen voor schilderijen.
[1462.80 --> 1464.46]  Maar wat andere mensen ervan vinden.
[1464.50 --> 1465.80]  Ik denk dat dat gewoon heel veel uitmaakt.
[1465.80 --> 1467.46]  En als je het echt helemaal afpelt.
[1467.58 --> 1470.90]  Hoeveel van de waardering van kunst is nou echt helemaal intrinsiek?
[1470.90 --> 1473.04]  Hoeveel komt nou echt helemaal uit jezelf?
[1473.50 --> 1474.02]  Ik weet niet.
[1474.14 --> 1475.58]  Ik vraag me dat toch echt af.
[1475.66 --> 1477.32]  Maar goed, dat kan de robot in mezelf zijn.
[1477.46 --> 1479.68]  Ik ben natuurlijk ook redelijk autistisch denk ik.
[1479.70 --> 1480.86]  Als dit soort dingen aankomt.
[1481.30 --> 1488.02]  Ik denk dat gewoon het opzoeken van gemeenschap inherent verbonden is aan de waardering van kunst.
[1488.66 --> 1492.04]  Ja, ik denk dat dit is weer een element.
[1492.30 --> 1497.34]  Want ik denk dat er is in de filosofie heel veel, heel veel geschreven rondom kunst.
[1497.46 --> 1498.44]  Echt lang geleden al.
[1498.44 --> 1503.60]  En dat gaat heel erg over hoe is het anders dan technologie bijvoorbeeld.
[1504.08 --> 1510.16]  En dat heeft heel erg te maken met dat in die technologie toch een enorme utilitaire doelbewustzijd zit.
[1510.22 --> 1511.02]  Het is ergens voor.
[1511.96 --> 1513.86]  En bij kunst hoeft het niet ergens voor te zijn.
[1514.00 --> 1515.12]  Mag wel, maar het hoeft niet.
[1515.56 --> 1519.62]  En daarom krijg je ook natuurlijk altijd de vraag van waarom hangt dit hier?
[1519.76 --> 1520.32]  Of wat is dit?
[1520.42 --> 1521.14]  Of waar doet het toe?
[1521.22 --> 1523.66]  En dat dat antwoord dan kan zijn weten we nog niet?
[1523.76 --> 1524.72]  Of weet ik nog niet?
[1524.72 --> 1527.56]  Of ik heb wel een idee, maar ik laat het graag aan jou.
[1527.66 --> 1529.84]  En dan zeggen mensen, vagen shit zeg, lopen ze vlug weg.
[1531.26 --> 1537.00]  Maar ik merk wel dat hierin die schaarste...
[1537.00 --> 1540.66]  Er zijn een aantal dingen die altijd al een beetje absurd waren als je ze zou bevragen.
[1541.02 --> 1542.00]  Als het gaat om schaarste.
[1542.16 --> 1543.92]  Want bijvoorbeeld digitale schaarste is een beetje vreemd.
[1544.00 --> 1545.66]  Dat is de hele NFT discussie natuurlijk.
[1545.66 --> 1551.32]  Dat is ook het hele bitcoin verhaal toen aan het begin geweest.
[1551.42 --> 1552.80]  Mensen zeggen, dat is gewoon luchtgeld.
[1553.00 --> 1555.58]  Want ja, maar ja, fiat geld is eigenlijk ook een beetje een database entry.
[1555.86 --> 1559.76]  Dus leggen we niet iets bloot wat nu zo ongelooflijk duidelijk is.
[1559.80 --> 1562.48]  Dat we merken, shit, dat sociale construct is echt wel wankel eigenlijk.
[1564.54 --> 1568.48]  Maar ik zit dus te denken dat als we de schaarste wegnemen...
[1569.54 --> 1571.06]  Want dat kan niet meer.
[1571.14 --> 1571.90]  Stel dat dat niet meer lukt.
[1572.04 --> 1573.56]  Dat sociale construct valt echt wel om.
[1573.56 --> 1576.70]  Wij zeggen nu, ergens in jouw verhaal hoorde ik...
[1576.70 --> 1578.88]  Zelfs als we het kunnen kopiëren...
[1578.88 --> 1579.80]  Zelfs als we het...
[1579.80 --> 1582.64]  Toch willen ze dat originele willen we, kan ik zeggen.
[1582.72 --> 1583.40]  Want dat heb ik ook.
[1583.58 --> 1584.64]  Dus dat ben ik met je eens.
[1585.10 --> 1588.42]  Maar ik kan me wel voorstellen dat als we een steeds meer...
[1588.42 --> 1592.60]  Digitaal, synthetisch, infused leven leiden...
[1592.60 --> 1595.74]  Dat het misschien wel moeilijker wordt om dat nog...
[1595.74 --> 1598.80]  Ja, tegelijkertijd als je nu kijkt in computergames en lootboxes...
[1598.80 --> 1602.00]  En Minecraft levels en dat ene zwaartje in Zelda...
[1602.00 --> 1602.94]  Gaat dat helemaal mee.
[1602.94 --> 1603.74]  Ja, precies.
[1604.86 --> 1606.80]  Dus dat bewijst eigenlijk wel een beetje wat jij zegt...
[1606.80 --> 1608.16]  Dat die schaarste misschien wel...
[1608.16 --> 1609.64]  Ja, maar dat je het ook kan genereren.
[1609.80 --> 1611.90]  Dus wat dat betreft dat het misschien toch alsnog kan.
[1612.58 --> 1612.68]  Ja.
[1613.14 --> 1614.06]  Nou ja, dit is inderdaad...
[1614.06 --> 1617.38]  Ik heb vanmiddag een Sokratisch gesprek gevoerd met JGPT over kunst.
[1617.54 --> 1619.38]  En auteursrechten is natuurlijk een ding.
[1619.50 --> 1622.04]  En het is ook dit waar het gelijk over gaat.
[1622.14 --> 1624.70]  Als het over midjourney gaat, namelijk op wiens data is het getraind.
[1624.80 --> 1626.00]  En mag dit eigenlijk allemaal wel?
[1626.62 --> 1627.94]  Het is me overigens volstrekt onduidelijk...
[1627.94 --> 1630.96]  Wat hier nou allemaal van mag en wat nou eigenlijk niet mag.
[1631.28 --> 1634.22]  Dus ik weet dat bijvoorbeeld in Nederland heb je het recht om bij naam genoemd te worden.
[1634.36 --> 1636.38]  Als kunst gemaakt is.
[1636.88 --> 1641.36]  En heb je ook een soort van bescherming tegen het inbreuk op je...
[1641.36 --> 1642.94]  Hoe noemde die het nou?
[1643.14 --> 1647.34]  Het inbreuk bijna op je status of zo.
[1647.48 --> 1650.94]  Dat als er een werk is wat duidelijk een afgeleide is van jouw werk...
[1650.94 --> 1654.52]  En daar staat niet jouw naam bij.
[1654.76 --> 1655.84]  Dat mag gewoon niet in Nederland.
[1656.50 --> 1661.58]  Want je moet altijd naamsvermelding erbij zetten in zo'n geval.
[1662.02 --> 1665.54]  Nou ja, dit is natuurlijk superschemeren gebied überhaupt al.
[1665.64 --> 1667.94]  Maar als dit op grote schaal met AI gebeurt...
[1668.50 --> 1669.94]  En ja, jezus...
[1671.54 --> 1675.38]  Het zette me echt aan het denken...
[1675.38 --> 1680.66]  Zou je de trainingsdata bij naam kunnen noemen überhaupt...
[1680.66 --> 1681.68]  Als je zo'n nieuw plaatje maakt?
[1681.76 --> 1683.38]  Zou je technisch überhaupt kunnen om...
[1684.20 --> 1686.04]  Bij een plaatje wat Midjourney gemaakt heeft te zeggen...
[1686.04 --> 1688.90]  Nou, we hebben dit en dit en dit als basiswerk gebruikt voor dit werk.
[1689.14 --> 1690.02]  Dat kan toch helemaal niet?
[1690.28 --> 1692.38]  Ja, volgens mij is Adobe Firefly...
[1693.02 --> 1697.38]  Hun Adobe antwoord op Midjourney en Dali...
[1697.90 --> 1699.38]  Hun grote selling point is...
[1700.46 --> 1701.58]  Onder andere...
[1701.58 --> 1702.92]  Wij weten waarop het getraind is...
[1702.92 --> 1704.04]  Want we hebben het zelf getraind.
[1704.04 --> 1707.58]  En het is getraind op een holledige corpus waarvoor wij betalen.
[1707.68 --> 1708.20]  Oh joh.
[1708.40 --> 1710.64]  Dus dat is een oplossing voor hun.
[1710.74 --> 1712.44]  Dus dan heb je nog niet...
[1712.44 --> 1713.92]  Op het moment dat jij je plaatje hebt getagd...
[1713.92 --> 1716.16]  Het automatisch de namen erbij komen die gebruikt worden.
[1716.24 --> 1718.68]  Maar die stap is wel te nemen.
[1719.16 --> 1720.92]  Ik denk, ik schat even in...
[1720.92 --> 1723.04]  Dat die technisch zeker niet onmogelijk is om te zeggen...
[1723.62 --> 1726.30]  Wij gaan gewoon wel netjes erbij zetten van wie dit is...
[1726.30 --> 1727.40]  En die mensen ook compenseren.
[1727.52 --> 1731.04]  Want wij weten welke delen uit de set aangeraakt worden...
[1731.04 --> 1732.58]  In het creëren hiervan.
[1732.58 --> 1735.08]  Wat voor trainingsdata zal dat zijn?
[1735.72 --> 1736.86]  Word Firefly bedoel je?
[1737.74 --> 1739.48]  Van hun stockdatabees of zo.
[1740.28 --> 1742.24]  Maar ja, dat is natuurlijk ook wel weer...
[1742.24 --> 1744.84]  Waarschijnlijk heeft dan een of andere Adobe jurist...
[1744.84 --> 1747.38]  Handig in de rechtenoverdracht gezet...
[1747.38 --> 1749.44]  Dat Adobe het werkelijk voor alles kan gebruiken.
[1749.86 --> 1751.26]  Maar hoe redelijk is het...
[1751.26 --> 1754.30]  Dat je van een freelance kunstenaar kan verwachten...
[1754.30 --> 1755.20]  Dat hij dan begrijpt...
[1755.20 --> 1757.74]  Oh, straks kan dus een model getraind worden hierop.
[1757.74 --> 1759.30]  Waardoor daadwerkelijk...
[1759.30 --> 1761.08]  Zeg maar mijn alles wat ik maak...
[1761.08 --> 1761.88]  Getraind kan worden.
[1762.02 --> 1763.08]  En dan oneindig...
[1763.08 --> 1765.08]  Opnieuw gemaakt kan ook nieuw werk.
[1765.20 --> 1767.90]  In plaats van alleen maar wat ik heb overgedragen aan de doop.
[1768.20 --> 1768.94]  Zeg maar ja...
[1768.94 --> 1770.10]  Het zal er vast in staan...
[1770.10 --> 1771.24]  Dat het in perpetuity...
[1771.80 --> 1772.74]  En in the whole universe...
[1773.40 --> 1776.16]  En for all of history gebruikt mag worden.
[1776.32 --> 1777.04]  Of all of the future.
[1777.80 --> 1778.32]  Maar ja...
[1778.32 --> 1779.24]  Ik weet niet of dit nou...
[1779.24 --> 1781.74]  Het zal jullie niet wel kloppen.
[1782.32 --> 1786.06]  Jouw werk zit voor 0,0000003% in dit werk.
[1786.16 --> 1787.86]  Want er is gebruik gemaakt van jouw werk.
[1788.06 --> 1789.36]  Ja, maar zei je dat niet heel mooi?
[1789.62 --> 1790.98]  Het zou natuurlijk wel mooi zijn...
[1790.98 --> 1792.36]  Als je zo de pot kan verdelen.
[1793.34 --> 1793.80]  Ja, ik zit alleen...
[1794.62 --> 1796.62]  Het ingewikkelde daarvan is dat je dan...
[1796.62 --> 1797.80]  Dat is een beetje ook het probleem...
[1797.80 --> 1799.98]  Met het trainen op gegenereerde data.
[1800.36 --> 1802.18]  Je hebt een soort schoonpunt in de geschiedenis...
[1802.18 --> 1804.10]  Waar deze systemen er nog niet waren.
[1804.78 --> 1807.10]  Waardoor je nog alles wat je erin geladen hebt...
[1807.10 --> 1810.00]  Al niet aangeraakt was door dit soort systemen.
[1810.04 --> 1811.20]  Maar we leven nu in een tijd...
[1811.20 --> 1812.78]  Waarin wat op internet staat...
[1812.78 --> 1814.82]  Al mogelijk gegenereerd is.
[1814.94 --> 1816.62]  En dan ga je dus ook weer gegenereerde dingen...
[1816.62 --> 1818.10]  In die datasets inladen.
[1818.42 --> 1820.40]  En dat is nooit meer schoon.
[1820.42 --> 1820.88]  Nee, precies.
[1820.98 --> 1821.84]  En dan krijg je...
[1821.84 --> 1824.58]  Want dit doet me heel erg denken aan het Xanadu-project.
[1825.30 --> 1828.02]  Ik heb Xanadu iets te vaak aangehaald in mijn leven al.
[1828.02 --> 1830.36]  Maar Xanadu is...
[1830.36 --> 1831.48]  Ik kan wel is zeggen...
[1831.48 --> 1832.30]  Maar ik zeg een beetje was...
[1832.30 --> 1833.66]  Want het is niet echt gelukt volgens mij.
[1834.14 --> 1836.02]  Maar het idee om...
[1836.02 --> 1837.92]  Hypertext met hyperlinks...
[1838.70 --> 1839.92]  Op zo'n manier te bouwen...
[1840.78 --> 1845.92]  Dat je eigenlijk een soort enorm graph of spinneweb...
[1845.92 --> 1849.50]  Van genetwerkte bronnen aan elkaar kon koppelen.
[1849.98 --> 1851.84]  Waardoor op het moment dat jij een artikel las...
[1851.84 --> 1852.70]  Of een stuk tekst...
[1852.70 --> 1853.80]  Laten we het daar even bij houden.
[1854.34 --> 1856.24]  En daar stond een zin uit een ander stuk tekst.
[1856.24 --> 1858.32]  Dan klikt hij erop en dan vloog je naar die tekst.
[1858.74 --> 1860.14]  En daar kon je dus alles zien.
[1860.24 --> 1861.96]  Maar het idee was ook...
[1861.96 --> 1864.24]  Om een heel financiële structuur daarachter te doen.
[1864.24 --> 1865.78]  Te zeggen...
[1865.78 --> 1867.56]  Als jij een zin van iemand gebruikt...
[1867.56 --> 1868.40]  Omdat we het weten...
[1868.40 --> 1870.86]  Kunnen we het ook tot ten treuren herleiden.
[1871.22 --> 1872.18]  En dan zeggen...
[1872.18 --> 1875.42]  Je krijgt nog 0,001% van de opbrengst van het boek.
[1875.48 --> 1876.40]  Want daar ben jij in gequote.
[1876.70 --> 1877.46]  Ja, fascinerend.
[1877.54 --> 1879.40]  Dus als je dat naar een podcast zou vertalen...
[1879.40 --> 1881.18]  Dan zouden wij net geciteerd hebben...
[1881.18 --> 1883.18]  Uit een of andere artikel uit de New York Times.
[1883.18 --> 1884.96]  En dan had de New York Times daar een deel van gekregen.
[1885.08 --> 1886.14]  Van de advertentieomzet.
[1886.14 --> 1887.68]  Ja, dat in het...
[1887.68 --> 1891.68]  Ik merkte dat toen ik dat Xanadu...
[1891.68 --> 1892.82]  Dat daar over las.
[1892.90 --> 1893.44]  Want ik wel dacht...
[1893.44 --> 1894.48]  Dat is wel...
[1894.48 --> 1896.10]  Er zit wel weer een heel...
[1896.10 --> 1896.88]  Ja, hoe zeg je dat?
[1896.98 --> 1898.98]  Politiek-filosofisch vraagstuk achter van...
[1898.98 --> 1901.10]  Oké, allemaal binnen dat kapitalisme natuurlijk.
[1901.32 --> 1903.98]  En laten we daar niet te veel over uitweiden...
[1903.98 --> 1906.62]  In deze aflevering.
[1906.62 --> 1908.46]  En geblieft niet, inderdaad.
[1908.58 --> 1911.72]  Maar dat we...
[1911.72 --> 1914.04]  Volgens mij lopen we nu wel een beetje door...
[1914.04 --> 1914.70]  Dat je nu zegt...
[1914.70 --> 1916.20]  Oké, misschien moeten we die bijwetten.
[1916.38 --> 1918.32]  Laten we zeggen dat de Europese Unie besluit...
[1918.32 --> 1922.26]  Dat als jij generatieve visuele dingen maakt...
[1922.26 --> 1923.10]  Dat we even simpel houden.
[1923.26 --> 1923.84]  Redelijk simpel.
[1924.62 --> 1926.44]  Dat je daarbij de dataset...
[1926.44 --> 1929.30]  En het systeem zo moet controleren.
[1929.66 --> 1930.56]  Dat je inderdaad...
[1930.56 --> 1933.76]  Bij ieder gegenereerd eindproduct kunt zeggen...
[1933.76 --> 1935.48]  Wat daar aangeraakt is...
[1935.48 --> 1936.82]  In de trainingsdatasets...
[1936.82 --> 1938.92]  Zodat die mensen uiteindelijk gecompenseerd kunnen worden.
[1939.14 --> 1940.62]  Stel dit is zo wat we nu zeggen.
[1941.04 --> 1942.84]  We komen in een soort nieuwe wereld dan.
[1943.02 --> 1944.18]  Een soort van...
[1944.18 --> 1946.30]  Waarin alles traceable is.
[1946.64 --> 1948.82]  Traceable, traceability of copyright.
[1948.94 --> 1951.84]  Die misschien mensen die heel erg voor die copyright zijn...
[1951.84 --> 1953.56]  Jaren van gedroomd hebben.
[1953.74 --> 1955.20]  Want nu is het zo.
[1955.28 --> 1956.40]  Als ik hier door de straat loop...
[1956.40 --> 1958.24]  Ik zie een abri op een bushokje...
[1958.24 --> 1959.32]  Met een grappig iets erop.
[1959.38 --> 1960.34]  Ik ga s'avonds schilderen.
[1960.48 --> 1960.94]  Kan ik niet.
[1960.94 --> 1961.86]  Maar stel...
[1961.86 --> 1963.62]  En zonder dat ik het zelfs door heb...
[1963.62 --> 1964.82]  Ben ik geïnspireerd geraakt.
[1964.82 --> 1966.42]  Die lijn is er niet.
[1966.52 --> 1967.56]  Want die staat niet in de computer.
[1967.98 --> 1969.24]  En ik wist het niet eens.
[1969.40 --> 1971.38]  Alleen als mijn brein gescand wordt...
[1971.38 --> 1971.88]  Kan je zeggen...
[1971.88 --> 1975.36]  Hé, jij moet nog een centje geven aan die vormgever van die abri.
[1975.44 --> 1976.42]  We zouden het wel anders kunnen doen.
[1976.54 --> 1978.92]  Namelijk als de AI...
[1978.92 --> 1981.00]  Zeg maar alle bushokjes scant.
[1981.16 --> 1983.26]  En hij kent alle posters die in bushokjes hangen.
[1983.76 --> 1984.80]  Dan zou je wel kunnen kijken...
[1984.80 --> 1986.74]  Nou, dat lijkt wel verdacht veel op dat werk...
[1986.74 --> 1987.90]  Wat jij die avond hebt geschild.
[1987.90 --> 1989.06]  Dus je zou het ook...
[1989.06 --> 1991.84]  Zoals je prompts in midjourney reverse kan ingenieren.
[1991.92 --> 1992.96]  Dus in plaatje uploaden.
[1992.96 --> 1994.42]  En dat hij er een prompt bij maakt.
[1994.96 --> 1997.42]  Zou je misschien ook zo copyright kunnen herleiden.
[1997.76 --> 1999.48]  En dan is het misschien niet helemaal echt.
[1999.66 --> 2003.74]  Want misschien is het ding wat hij daadwerkelijk gebruikt heeft...
[2003.74 --> 2004.92]  Om het plaatje te maken.
[2005.02 --> 2005.80]  Het originele werk.
[2005.88 --> 2006.40]  Om het plaatje te maken.
[2006.84 --> 2009.06]  Wel uiteindelijk een heel ander origineel werk.
[2009.12 --> 2009.90]  Wat hij gebruikt heeft.
[2009.96 --> 2010.56]  Dat zou kunnen.
[2010.84 --> 2013.20]  Maar dan heb je in ieder geval een verdeling.
[2013.20 --> 2014.70]  Dan kun je dan over steggelen.
[2014.84 --> 2018.32]  Over hoe dat algoritme werkt.
[2018.52 --> 2020.68]  Die die appropriation doet.
[2020.84 --> 2021.54]  Zou je maar zeggen.
[2022.34 --> 2023.80]  Maar dan heb je in ieder geval iets.
[2024.22 --> 2026.20]  Want ik heb het idee dat dat eerste...
[2026.84 --> 2028.36]  Het soort van trace bewaken.
[2028.60 --> 2031.06]  Dat dat gewoon technisch volstrekt onmogelijk is.
[2031.50 --> 2032.80]  Ik weet niet of het onmogelijk is.
[2032.90 --> 2034.10]  Maar het maakt me zenuwachtig.
[2034.18 --> 2037.06]  Omdat ik bijvoorbeeld nu kijk naar die hele rechtszaak van Ed Sheeran.
[2037.12 --> 2039.04]  Die nu plaatsgevonden heeft om een stukje...
[2039.04 --> 2040.70]  Wat dan leek op een riffje uit een ander nummer.
[2040.70 --> 2043.20]  En dat hij ook in mijn ogen terecht zegt.
[2043.58 --> 2045.34]  Ik ga geen liedjes meer maken.
[2045.82 --> 2047.08]  Als ik deze zaak niet win.
[2047.54 --> 2048.70]  Want ik ben gewoon te bang.
[2048.90 --> 2050.36]  Om ook maar mijn gitaar nog aan te raken.
[2050.46 --> 2051.42]  Dat kan je dramatisch noemen.
[2051.54 --> 2052.22]  Maar ik snap hem wel.
[2052.28 --> 2054.12]  Want de mond wordt hem in essentie gesnoerd.
[2054.38 --> 2055.14]  Want hij heeft het idee.
[2055.54 --> 2058.74]  Ja, ik kan gewoon constant van alle kanten aangeklaagd worden.
[2058.88 --> 2060.92]  Voor iets wat maar een beetje lijkt op iets van iemand anders.
[2061.00 --> 2061.84]  Zoals ik het niet eens weet.
[2062.20 --> 2063.30]  En ik zit nu wel te denken.
[2063.58 --> 2064.42]  Nu we het hier over hebben.
[2065.52 --> 2067.48]  Stel het is zo dat dit systeem kan.
[2067.56 --> 2068.54]  Wat we nu hebben besproken.
[2068.70 --> 2070.46]  En het wordt ook aangezet voor podcasts.
[2070.46 --> 2071.32]  Waar wij nu in zitten.
[2071.98 --> 2073.92]  En dat betekent dus dat we misschien wel live.
[2074.00 --> 2074.64]  Terwijl we hier zitten.
[2074.74 --> 2075.80]  Maak het even dystopian.
[2075.94 --> 2077.30]  We gaan even full black mirror.
[2077.74 --> 2079.42]  Ik zie gewoon hier op een scherm naast mij.
[2079.42 --> 2081.16]  Zie ik gewoon de royalties live oplopen.
[2081.36 --> 2081.96]  Want waarom niet?
[2082.26 --> 2083.70]  Waarom moet dit een uur live?
[2083.70 --> 2084.18]  Geen enkel idee.
[2084.34 --> 2085.74]  Hier besproken is origineel.
[2085.86 --> 2086.00]  Nee.
[2086.36 --> 2087.18]  Dus dan zien we hier zo.
[2087.24 --> 2089.26]  Nu zitten we op 18 euro aan shit die we al moesten.
[2089.38 --> 2092.14]  En dat wordt automatisch uit onze bitcoin account weggetrokken.
[2092.22 --> 2093.46]  Want alles is geautomatiseerd.
[2093.46 --> 2093.60]  Ja.
[2093.90 --> 2094.92]  Maar dan zit ik wel.
[2095.56 --> 2095.88]  Oké.
[2095.96 --> 2096.28]  Stel.
[2096.44 --> 2097.34]  Het maakt me een beetje bang.
[2097.76 --> 2098.76]  Maar ik denk dan ook.
[2098.88 --> 2102.42]  Is dat dan alleen in het geval dat als wij er ook geld aan verdienen.
[2102.80 --> 2102.90]  Of.
[2103.24 --> 2104.02]  Want nu zit ik dan te denken.
[2104.10 --> 2106.24]  Als jij dan dus iets gratis de wereld in zegt.
[2106.48 --> 2107.36]  Snap dat systeem dan.
[2107.52 --> 2107.70]  Oh.
[2108.38 --> 2109.20]  En dan zegt het systeem.
[2109.48 --> 2111.30]  Ja maar jij hebt het wel gratis gezonden.
[2111.52 --> 2114.56]  Maar twee weken later heeft iemand jou gevraagd voor een lezing.
[2114.68 --> 2115.98]  En dat kwam wel door het stukje wat je dacht.
[2116.00 --> 2116.88]  En ik ga toch.
[2117.06 --> 2118.38]  Maar eigenlijk als je goed begrijpt.
[2118.54 --> 2119.10]  Wittje zeg jij.
[2119.22 --> 2122.16]  Die kunstenaars die klagen nu over hun broodroof.
[2122.24 --> 2123.20]  Moeten niet zeiken.
[2123.54 --> 2124.26]  Want ook zij.
[2124.74 --> 2125.92]  Dat zeg ik echt niet.
[2126.14 --> 2128.32]  Ook zij worden de hele tijd geïnspireerd door anderen.
[2128.80 --> 2128.94]  Ja.
[2129.18 --> 2131.44]  En dat is hoe je je werk hebt gemaakt.
[2131.44 --> 2134.60]  Dus in feite is jouw positie niet anders dan die van Mid Journey.
[2134.86 --> 2135.56]  Dat is wat jij zegt.
[2135.70 --> 2135.88]  Nee.
[2135.98 --> 2136.74]  Maar dat vind ik niet.
[2136.74 --> 2138.52]  Ook al kan dat misschien wel eens overkomen.
[2138.62 --> 2142.00]  Ik ben blij dat je even helder maakt hoe dit kan overkomen op mensen.
[2142.02 --> 2142.36]  Maar wat is dan het verschil?
[2144.06 --> 2146.02]  Nou dit vind ik sowieso een boeiende.
[2146.68 --> 2150.32]  Omdat het een groot kapitalistisch bedrijf is in plaats van een schattige kunstenaar.
[2151.32 --> 2152.74]  Wat is dan precies het verschil?
[2153.28 --> 2157.06]  Als de een geïnspireerd wordt door alles wat ze in de omgeving ziet.
[2157.06 --> 2161.04]  Ik denk dat het is in heel veel systemen.
[2161.14 --> 2162.06]  Ik ga het even kijken of ik het.
[2163.78 --> 2165.62]  Het wordt nu ineens heel abstract in mijn hoofd.
[2165.62 --> 2168.50]  Maar ik ga het proberen met leuke concrete voorbeelden nog iets van te maken.
[2169.58 --> 2173.90]  Ik ben van mening dat er veel systemen zijn met regels daarin.
[2174.02 --> 2175.94]  En noem het het portretrecht.
[2176.50 --> 2177.58]  Maak het even concreet.
[2178.04 --> 2179.36]  Wat volgens mij een goed idee is.
[2179.36 --> 2180.94]  Iemand maakt een foto van jou op straat.
[2181.00 --> 2182.88]  Je zet er in je eentje op en dan zeg jij dat wil ik niet.
[2183.14 --> 2184.90]  Nou dat vind ik heel veel mensen zeggen dan.
[2184.96 --> 2186.88]  Oh wat fijn dat je dat zo kan opeisen zeg.
[2187.06 --> 2187.36]  Wat goed.
[2189.66 --> 2196.98]  Er zijn die regels die werken en die wetten ook wel een beetje omdat die zijn vrij hard.
[2197.14 --> 2198.70]  Dat zijn geformaliseerde regels.
[2198.90 --> 2200.44]  Eén persoon op de foto et cetera.
[2200.64 --> 2203.58]  Dat is helemaal uit ten treuren neergezet.
[2203.58 --> 2205.06]  Want dat moet allemaal geformaliseerd worden.
[2205.14 --> 2207.00]  Zodat het later bepaald kan worden of je gelijk had.
[2207.32 --> 2209.58]  Mocht je iemand een zaak aanspannen of zeggen ik wil compensatie.
[2211.26 --> 2213.78]  Maar die landen uiteindelijk in een best wel een fuzzy wereld.
[2213.90 --> 2216.36]  Vol met grijze gebieden en vaagheid.
[2216.44 --> 2219.00]  Want die realiteit is helemaal niet zo lekker te vangen als wij willen.
[2219.52 --> 2220.18]  Ben ik blij mee.
[2220.78 --> 2221.68]  Wat bedoel ik daarmee?
[2222.14 --> 2222.52]  Is dat ik.
[2223.24 --> 2223.78]  Als het om.
[2223.78 --> 2227.82]  Bepaalde ideeën rondom copyright die er zijn.
[2228.02 --> 2231.26]  Bijvoorbeeld Mickey van Disney die 70 jaar onder copyright hangt.
[2231.98 --> 2235.34]  Die zijn in een wereld waar niet die generative AI's waren.
[2235.98 --> 2238.62]  Dus op het moment dat wij tien jaar zouden oefenen om.
[2238.84 --> 2239.74]  Of misschien heb je talent.
[2240.08 --> 2242.26]  Twee jaar oefend en misschien vind je het helemaal niet zo moeilijk.
[2242.50 --> 2243.84]  Twee maanden oefend om Mickey.
[2244.92 --> 2246.38]  Perfect na te kunnen tekenen.
[2246.52 --> 2248.14]  De hele tijd gewoon uit je hoofd.
[2248.14 --> 2249.56]  Je gooit er gewoon heel de Mickey's uit.
[2249.62 --> 2250.64]  In allerlei vormen en doen.
[2251.00 --> 2252.94]  Dan heb je dus eigenlijk iets gestolen daar.
[2252.94 --> 2255.84]  Je hebt een soort van inspiration van de Disneymakers.
[2255.88 --> 2259.24]  Heb jij weten te implanteren in jou door te oefenen.
[2259.40 --> 2260.04]  Dat is gewoon leren.
[2260.18 --> 2260.84]  Dat kan je doen.
[2261.14 --> 2264.52]  Je had hele goede vermeerimitatoren vroeger.
[2264.66 --> 2266.38]  Die daar ook vaak niet eens over logen.
[2266.46 --> 2267.44]  En zeiden ik kan dat voor je doen.
[2267.86 --> 2268.22]  Prima.
[2270.74 --> 2275.12]  Het punt is dat de schaalbaarheid van dat mensen die gaan proberen Mickey na te doen.
[2275.56 --> 2277.06]  En het gedoe wat je daarvan krijgt.
[2277.06 --> 2278.70]  Terwijl je ook net even een andere Mickey kan doen.
[2278.76 --> 2279.98]  En dan toch nog geld kan verdienen.
[2281.14 --> 2282.70]  Zo'n wet werkte gewoon.
[2282.70 --> 2287.14]  Omdat in de pragmatische realiteit van de dag waar we met mensen werken.
[2287.24 --> 2288.14]  En niet algoritme.
[2288.52 --> 2288.98]  Toen nog.
[2289.68 --> 2291.14]  Kan je zoiets ook echt toepassen.
[2291.28 --> 2292.74]  En zeggen doe gewoon niet Mickey na.
[2292.86 --> 2295.58]  En als iemand het doet dan lukt het misschien zeven mensen per jaar.
[2295.70 --> 2297.48]  Dus die kan je dan ook nog een beetje aanklagen en zo.
[2297.96 --> 2300.50]  Maar dan komt er ineens een volledig automatiseerd systeem.
[2300.56 --> 2302.34]  Waarin iemand gewoon een Donald Duck kan aanklikken.
[2302.44 --> 2303.94]  En zeggen doe mij nog zo'n Donald Duck.
[2304.30 --> 2306.18]  Ik geef je ook nog een onderwerp waar het over moet gaan.
[2306.28 --> 2306.76]  En rats!
[2307.06 --> 2308.56]  Gooit die hele Donald Duck eruit voor je.
[2308.56 --> 2311.54]  Het gemak en de snelheid van dat moment.
[2311.54 --> 2315.16]  En de democratisering van die macht eigenlijk om dat te kunnen doen.
[2315.88 --> 2316.24]  Ja dan.
[2318.28 --> 2319.84]  Die wetgeving.
[2320.44 --> 2323.04]  Die gedachtegang achter dat juridische model.
[2324.20 --> 2327.18]  Is in een groot deel gestut.
[2327.18 --> 2331.64]  Op de dagelijkse realiteit van het kunnen handelen van zo'n wet.
[2332.06 --> 2332.86]  En op het moment dat je.
[2333.70 --> 2335.38]  Wat ik nu veel in discussies hoor.
[2335.50 --> 2336.12]  Is dat mensen zeggen.
[2336.40 --> 2337.98]  Ja maar wat ik een beetje bij jou ook hoor.
[2338.90 --> 2339.92]  Misschien hoor ik dat verkeerd.
[2341.02 --> 2342.80]  Ja maar dit is toch oude wij een nieuwe zak.
[2342.88 --> 2344.20]  Of dit is toch hetzelfde als toen.
[2344.48 --> 2346.48]  En iedere artiest was toch geïnspireerd.
[2346.70 --> 2347.48]  Alles is een remix.
[2349.36 --> 2350.12]  Dat klopt.
[2350.64 --> 2351.58]  Maar het feit dat dat.
[2352.46 --> 2355.00]  Dat niet alle kunstenaars op straat leven.
[2355.22 --> 2356.00]  Ik zeg het maar even zo.
[2356.00 --> 2356.20]  Ja.
[2356.64 --> 2357.74]  Had wel te maken bij.
[2358.50 --> 2360.98]  Dat het dus inderdaad nog steeds mogelijk was.
[2361.14 --> 2364.42]  Met de materialistische en culturele realiteit van de tijd.
[2364.72 --> 2366.22]  Zonder large language models erin.
[2366.56 --> 2367.94]  En zonder generative AI's.
[2368.00 --> 2369.54]  Die visueel dingen kunnen maken.
[2370.28 --> 2372.06]  Dat dat toch een beetje werkte.
[2372.32 --> 2373.88]  Veel kunstenaars die nu luisteren.
[2374.08 --> 2375.34]  Die zichzelf kunstenaar noemen.
[2375.48 --> 2376.08]  Of kunst maken.
[2376.08 --> 2377.10]  Of whatever zullen zeggen.
[2377.72 --> 2378.76]  Het was al zwaar vriend.
[2379.08 --> 2379.76]  Zonder die dingen.
[2380.24 --> 2381.98]  We hadden eigenlijk al niet de juiste rechten.
[2382.52 --> 2385.22]  Maar door trucjes konden we toch leven.
[2385.22 --> 2386.24]  De meeste van ons.
[2387.10 --> 2391.58]  Nu zien we eigenlijk de daadwerkelijke absurditeit van de situatie in.
[2392.00 --> 2395.78]  Van een gedachtengang die al moeilijk klopte.
[2395.92 --> 2397.46]  Maar was al een fragiele gedachtengang.
[2397.82 --> 2399.66]  Die klapt nu gewoon helemaal om.
[2400.12 --> 2404.42]  Door de enorme tijdswinst bij het kopiëren.
[2404.42 --> 2407.26]  Het gemak van de interface om het te doen.
[2408.34 --> 2410.84]  Het komt een beetje terug op waar we het eerder in het gesprek over hadden.
[2411.22 --> 2413.60]  Ik denk dat bepaalde dingen al redelijk scheef waren.
[2413.92 --> 2415.40]  En nu richting het absurde gaan.
[2415.62 --> 2420.16]  Waarom we ineens oog in oog staan met iets wat eigenlijk altijd al een beetje vreemd was.
[2420.16 --> 2420.44]  Ja.
[2421.08 --> 2423.88]  En heb je ook een idee over hoe je dit zou kunnen rechttrekken?
[2424.46 --> 2428.22]  Ik denk dus dat ik dan een beetje met een mond vol tanden zit.
[2428.46 --> 2430.16]  Niet dat ik denk dat dat onoplosbaar is.
[2430.54 --> 2432.22]  Maar ik krijg dan dus ideeën.
[2432.38 --> 2433.32]  En dat is niet van mij.
[2433.42 --> 2434.44]  Want het is ook maar wat ik lees.
[2434.54 --> 2435.44]  Maar wat we net over hadden.
[2435.76 --> 2439.06]  Van in mijn ogen hele enge traceability systems.
[2439.20 --> 2441.14]  Want dat tracen dat heeft te volgen.
[2441.52 --> 2443.26]  Denk ik niet dat we dat moeten willen.
[2443.96 --> 2446.92]  Dus dan moeten we misschien toe naar die iets als vertrouwen.
[2447.20 --> 2449.06]  Dat het veel meer te maken heeft met oké.
[2449.70 --> 2452.28]  Ook al weten we niet helemaal precies hoe jij die bijdrage hebt gedaan.
[2453.38 --> 2456.38]  We hebben wel een aantal indicatoren waarom jij een bijdrage hebt gedaan.
[2456.48 --> 2457.16]  Dat is dan genoeg.
[2457.32 --> 2458.40]  Ik zeg het even heel abstract.
[2458.62 --> 2462.78]  Maar moeten we het allemaal helemaal tot in een treur op een pixel gaan meten?
[2462.92 --> 2463.68]  Of kan jij gewoon zeggen.
[2463.80 --> 2466.22]  Nou ik heb toen dat stripje daar getekend als tool gepubliceerd.
[2466.76 --> 2468.16]  Ik zie daar dingen die lijken daarop.
[2468.58 --> 2470.46]  Dat geeft mij een soort van recht op royalties.
[2470.58 --> 2471.30]  Of wat erop lijkt.
[2471.40 --> 2472.90]  Al is er alleen maar je naam erbij.
[2472.90 --> 2476.36]  Ja en daar is nog wel een soort midden.
[2476.78 --> 2477.76]  Zonder dat we hoeven zeggen.
[2477.86 --> 2480.82]  Laten we de grondaanname van onze hele samenleving meteen veranderen.
[2482.10 --> 2486.06]  Maar ik heb nu niet zo off the cuff een goed antwoord op.
[2486.58 --> 2486.90]  Wat is de...
[2487.58 --> 2491.38]  Want ik zat in zo'n creaties gesprek met Chad GPT.
[2491.50 --> 2492.20]  Dus over kunst.
[2492.32 --> 2494.38]  En toen kwam je ook met allemaal VR.
[2495.50 --> 2497.18]  Ik had het over de toekomst van kunst.
[2497.28 --> 2497.62]  En dan zei hij.
[2497.70 --> 2500.40]  Door AI kun je allemaal dingen doen met kunst die je nu niet kon.
[2500.50 --> 2501.00]  En toen dacht ik.
[2501.10 --> 2502.18]  Dit meen je niet serieus.
[2502.18 --> 2503.16]  Dus toen zei ik tegen hem.
[2503.22 --> 2505.30]  Laten we hier een socrates gesprek over voeren.
[2505.36 --> 2506.56]  Want ik geloof hier geen reet van.
[2507.04 --> 2508.54]  Dat is over het algemeen heel inzichtelijk.
[2508.94 --> 2510.08]  Als dat ding dingen zegt.
[2510.18 --> 2510.66]  Waarvan ik denk.
[2511.44 --> 2511.96]  Volgens mij klopt dat niet.
[2511.96 --> 2513.78]  We hebben toch een soort derde persoon aan tafel.
[2513.96 --> 2515.46]  En dan op zich dan entity.
[2515.46 --> 2515.96]  Ja.
[2516.96 --> 2519.22]  En toen kwam hij dus met allemaal dingen.
[2519.22 --> 2522.12]  Waardoor er allemaal interactie met bezoekers kan zijn.
[2522.20 --> 2523.52]  Dus hij had het over installaties.
[2523.80 --> 2524.34]  Ik zeg hij.
[2524.86 --> 2525.74]  Dan moet je me vergeven.
[2526.14 --> 2527.92]  Maar hij had het over installaties.
[2528.44 --> 2532.56]  Over kunstwerken die zich kunnen aanpassen aan de bezoekers.
[2532.84 --> 2534.74]  Dat het verder gaat dan alleen maar knoppen indrukken.
[2534.86 --> 2535.60]  Zoals het met...
[2535.60 --> 2537.62]  Ik weet niet of dat een metafoor is.
[2537.74 --> 2540.42]  Maar zo beschrijft hij nu de kunstinstallatie van vandaag.
[2540.48 --> 2541.66]  De kunstinstallatie van vandaag.
[2542.34 --> 2545.18]  Maar dat dat echt effect heeft op wat je doet in de ruimte.
[2545.30 --> 2546.24]  Of wie je bent.
[2546.66 --> 2549.68]  Of dat het zelfs je hersengolf zou kunnen meten.
[2549.72 --> 2551.88]  En dat het daar iets mee doet.
[2552.22 --> 2554.64]  Als je er vanuit gaat dat je VR-brillen op hebt.
[2554.74 --> 2556.26]  Dan is het natuurlijk sowieso überhaupt.
[2556.52 --> 2557.58]  De sky is the limit.
[2557.88 --> 2559.70]  Want alles kan gegenereerd worden.
[2559.76 --> 2560.60]  Helemaal voor jou.
[2560.72 --> 2563.60]  En misschien kan hij wel een soort van de opteelsom van...
[2563.60 --> 2566.46]  Misschien kan die AI wel proberen te vatten.
[2566.54 --> 2567.98]  Wat nou eigenlijk jouw moraliteit is.
[2568.00 --> 2570.60]  Op basis van al die data die je met de jaren hebt ingeleverd bij Google.
[2570.60 --> 2575.66]  En dan dat je moraliteit een soort van centraal deel wordt voor het kunstwerk.
[2575.72 --> 2577.76]  Wat helemaal voor jou gemaakt wordt.
[2577.88 --> 2580.68]  En een soort van ultiem contraire is ten opzichte van je moraliteit.
[2580.80 --> 2583.70]  Waardoor jij instort als je het kunstwerk bekijkt.
[2583.86 --> 2585.36]  Dat is nu iets wat niet kan.
[2585.44 --> 2587.66]  Want je kan niet alle kunst voor iedereen individueel maken.
[2587.96 --> 2590.42]  Het komt iets meer in de richting van wat games zijn.
[2590.70 --> 2593.60]  Dus dat als je het mooie van films...
[2594.62 --> 2596.72]  Namelijk dat ze emotioneel heel erg triggeren.
[2596.72 --> 2601.00]  Gecombineert met de technische know-how van gamemakers.
[2601.62 --> 2605.16]  Dat is überhaupt al veel dichter bij elkaar gekomen natuurlijk met de tijd.
[2605.24 --> 2609.88]  Maar als je dat kan combineren met de schaalbaarheid die AI brengt.
[2610.40 --> 2612.90]  Namelijk dat je voor iedere individu die het spel speelt...
[2612.90 --> 2614.20]  Het spel volledig kan aanpassen.
[2614.36 --> 2616.36]  Dat is iets wat we tot nu toe nog niet doen.
[2616.52 --> 2617.38]  Dit kan nog niet.
[2617.38 --> 2619.86]  Ja, ik weet niet.
[2621.94 --> 2623.74]  Toen ik eenmaal dat gesprek aan het voeren was...
[2623.74 --> 2626.54]  dacht ik, ah, misschien zet daar toch eigenlijk best wel wat in.
[2626.92 --> 2628.50]  En die hele discussie in het begin hadden...
[2628.50 --> 2629.86]  Wat de waarde van kunst...
[2629.86 --> 2634.48]  Heeft het nog waarde als het niet een soort van menselijke pennestreek is geweest?
[2634.54 --> 2635.80]  Of een penseelstreek is geweest?
[2636.16 --> 2637.06]  Dan dacht ik, ja, nou jezus.
[2637.16 --> 2640.38]  Als je dat gaat vergelijken met een AI...
[2640.38 --> 2642.38]  Of een AI...
[2642.38 --> 2644.38]  Gedeeldelijk door AI gemaakt kunstwerk...
[2644.38 --> 2647.38]  Wat je in VR kan zien waardoor je iets beleeft wat...
[2648.02 --> 2650.54]  Nou ja, heel erg triggerend is.
[2650.64 --> 2651.70]  En daar is het op gemaakt.
[2652.48 --> 2653.82]  Ja, noem dat maar eens geen kunst.
[2653.98 --> 2655.72]  Zeg dan maar eens, dat heeft geen waarde.
[2656.06 --> 2657.04]  Natuurlijk heeft dat waarde.
[2657.24 --> 2658.96]  Dat is hoe mensen altijd praten over kunst.
[2659.02 --> 2660.70]  Namelijk dit emotioneert mij in de kern.
[2661.28 --> 2663.38]  Ik denk, wat ik in het begin zei, namelijk...
[2664.26 --> 2667.62]  Kunst heeft toch heel veel waarde door het sociale construct wat het heeft.
[2668.00 --> 2668.36]  Oké.
[2668.36 --> 2669.80]  Maar als ik dan iets kan...
[2669.80 --> 2673.70]  Als ik daar tegen mijn eigen argumentatie ingaande...
[2673.70 --> 2677.34]  Als ik dan iets probeer te verzinnen wat dan wel in de ziek komt...
[2677.34 --> 2679.30]  Dan is het hoe kunst kan emotioneren.
[2679.90 --> 2683.70]  Nou ja, dat kan een computer natuurlijk fantastisch manipuleren.
[2684.10 --> 2687.22]  Ga ik vanuit dat een computer dat fantastisch kan manipuleren.
[2687.80 --> 2691.56]  Dat AI heel goed gaat zijn in herkennen wat onze zwaktes zijn.
[2691.80 --> 2694.00]  Als je emotie als zwakte wil zien.
[2695.16 --> 2696.36]  Dat is niet wat ik van vind hoor.
[2696.40 --> 2698.10]  Maar er is maar één element ervan.
[2698.76 --> 2699.32]  Ja, jeetje.
[2699.32 --> 2702.12]  Nou, ik kan me nog helemaal herinneren dat jij, Reinier en ik...
[2702.12 --> 2705.50]  Naar een VR experience hier in Amsterdam gingen om zombies te schieten.
[2705.88 --> 2708.40]  Wat ik daar zo vet aan vond is dat jij hetzelfde zag als ik.
[2708.50 --> 2709.66]  Wel vanuit jouw perspectief.
[2709.74 --> 2711.34]  Je stond ergens anders in de ruimte.
[2711.54 --> 2711.98]  Ja, precies.
[2711.98 --> 2713.56]  Even voor de context voor mensen.
[2713.80 --> 2716.36]  We waren met een vriend een VR game aan het spelen.
[2716.46 --> 2717.22]  Wat gingen we ook weer doen?
[2717.32 --> 2718.26]  Zombies neerschieten.
[2718.56 --> 2721.02]  Dan heb je dus zo'n VR bril op je hoofd.
[2721.10 --> 2724.30]  En de fysieke ruimte is even groot als de virtuele ruimte.
[2724.44 --> 2726.78]  Alleen wat je in de virtuele ruimte ziet is niet echt.
[2726.78 --> 2727.72]  Want het zijn namelijk zombies.
[2728.02 --> 2728.78]  En je hebt geweren.
[2728.84 --> 2730.84]  Je hebt daadwerkelijk geweren in je handen.
[2730.88 --> 2733.00]  En je deelt die ervaring.
[2733.16 --> 2733.80]  En dat is...
[2733.80 --> 2734.86]  Ja, dat is deel van de lol.
[2734.96 --> 2736.00]  Dat je de ervaring deelt.
[2736.32 --> 2739.78]  Ja, en ik denk ook dat het een deel van de lol is van bij de Mona Lisa staan.
[2739.90 --> 2740.94]  Is dat je er niet alleen staat.
[2741.02 --> 2741.56]  Ja, fair enough.
[2741.66 --> 2743.08]  Iemand anders kan zeggen, nee, nee, nee.
[2743.12 --> 2744.48]  Ik ga net voor sluitingstijd.
[2744.74 --> 2746.26]  Of ga ik daar alleen staan?
[2746.36 --> 2747.76]  Want ik wil alleen zijn met de Mona Lisa.
[2747.96 --> 2748.10]  Ja.
[2748.16 --> 2749.34]  Dat is ook helemaal legitiem.
[2750.44 --> 2754.08]  Ik zat nu meteen, nu jij zei, we kunnen gepersonificeerde kunst maken.
[2754.58 --> 2755.58]  Hyper personification.
[2755.58 --> 2755.74]  Hyper personation.
[2756.24 --> 2757.42]  Dat is vast een term ergens.
[2758.34 --> 2758.80]  Bij deze.
[2759.80 --> 2761.04]  Maar dan zeg ik ook, oeh.
[2761.92 --> 2763.00]  Maar dat is subjectief.
[2763.16 --> 2766.72]  En wat ik graag zou willen, ik denk, oh, ik zou ook zo graag dan iets met groepen doen.
[2767.06 --> 2767.08]  Daar.
[2767.18 --> 2767.76]  Wat helemaal kan.
[2767.90 --> 2770.12]  Want zou je dus een groepsopdracht geven.
[2771.00 --> 2774.76]  En zeggen van, of misschien begint zo'n installatie heel erg individualistisch.
[2774.86 --> 2776.98]  Maar vloeit dit eigenlijk in een soort groep samen aan het einde.
[2777.08 --> 2778.08]  En deel je elkaar zijn ervaring.
[2778.08 --> 2778.96]  Ik denk, ik noem maar wat.
[2781.70 --> 2788.76]  Misschien moet je hier ook voor aannemen dat, zeg maar, kan waardevolle kunst nog bestaan buiten de VR-bril.
[2788.76 --> 2798.14]  Als we zulke overweldigende dingen kunnen maken in VR, gaan mensen dan echt nog geïnteresseerd zijn in een olieverf, in een lijst?
[2800.10 --> 2802.68]  Helemaal die mensen, helemaal de massa, zeg maar.
[2802.68 --> 2807.68]  De mensen die afkomen op de sociale ervaring en het ervaren van iets unieks.
[2808.80 --> 2812.84]  Zoals mensen ook vanuit China naar Amsterdam komen om de stad te bezoeken.
[2814.44 --> 2824.60]  Ja, en daar gebeurt denk ik nog steeds nu nog zoveel meer in de realiteit dan wat er gebeurt in zo'n VR-wereld.
[2824.82 --> 2825.02]  Ja?
[2825.58 --> 2825.88]  Ja.
[2827.02 --> 2831.14]  En dat is ook heel moeilijk om dat allemaal, ja.
[2831.14 --> 2835.38]  Misschien ook flauw om het met elkaar te laten concurreren, want het kan gewoon allebei bestaan.
[2835.48 --> 2835.90]  Dat snap ik wel.
[2836.06 --> 2843.64]  Maar in de hoeveelheid dat we het consumeren, van kunst die we consumeren, ja, dat concurreert toch met elkaar?
[2844.00 --> 2849.70]  In fysiek kunstwerk, in Voorlinde, in museum Voorlinde versus een VR-ervaring.
[2849.70 --> 2852.90]  Maar we kunnen ook weer over deze heen springen.
[2852.98 --> 2855.68]  We moeten niet over al onze onderwerpen heen springen, want dan springen we steeds weg.
[2855.68 --> 2862.12]  Maar laten we gewoon even aannemen dat dat er zelfs niet meer toe doet.
[2862.44 --> 2867.26]  Dus we hebben dan gewoon een robot die dat wel gewoon verft, verschildert, sorry.
[2867.58 --> 2867.70]  Ja.
[2867.78 --> 2871.34]  En dan, dat is een beetje tussen een schip en een bootverschil, schilderen en verven.
[2871.46 --> 2873.50]  Sorry daarvoor voor de mensen die dat werkelijk kunnen schilderen.
[2873.50 --> 2873.98]  Dus oké.
[2875.60 --> 2877.94]  Iemand een ding creëert dat.
[2878.10 --> 2883.26]  Dus het wordt wel gegenereerd door Mid Journey en dan wordt het uiteindelijk wel op een canvas gezet en opgehangen.
[2883.38 --> 2888.46]  Dus we skippen nu even het niet, we moeten niet die hele VR-ding doorzagen nu, maar ik wilde jou nu zeggen,
[2888.54 --> 2891.42]  ja, maar dat VR is wel een stuk minder rijk dan de daadwerkelijke realiteit.
[2891.42 --> 2894.14]  En dan zou jij kunnen zeggen, oké, dan maken we het echt.
[2894.58 --> 2895.02]  Boeien.
[2895.40 --> 2900.50]  Ik bedoel, het is alleen wat minder, volgens mij is jouw VR-verhaal, die daarbij komt, dat element,
[2900.88 --> 2911.68]  en ook het GPT-VR-verhaal van die virtuele realiteit is zoveel maakbaarder, sneller maakbaarder en bereikbaarder voor die algoritme dan onze fysieke realiteit.
[2911.92 --> 2913.30]  Dus gaat het indrukwekkender zijn.
[2913.30 --> 2921.08]  Ja, zolang, maar dat is dus een soort van tussenmoment, zou ik zeggen, dat kan misschien heel lang duren,
[2921.18 --> 2926.80]  want het is nou de energie die erin moet om een volledige virtuele omgeving op z'n kop te zetten, letterlijk.
[2927.02 --> 2932.04]  Je zit bij een concert en je zit ineens op z'n kop, dat is makkelijker dan die hele fysieke concertzaal omdraaien, zeg maar.
[2932.84 --> 2938.94]  Dus je gaat eerder die op z'n kop concert ervaring meemaken in VR dan in daadwerkelijk, nou, een soort La Via Volta.
[2939.04 --> 2940.00]  Ik weet niet of je er ooit wel eens in gezeten hebt.
[2940.00 --> 2948.00]  Ik zag dus laatst een soort van, er was een ding in Los Angeles waar je een container in kan lopen en dan hebben ze een vliegtuig van binnen nagemaakt.
[2949.20 --> 2952.96]  En dan ga je dus samen meemaken dat het vliegtuig gaat neerstorten.
[2953.44 --> 2960.84]  En ze hebben dus heel veel werk gedaan om alle ramen, projectie daarin te doen en het beweegt allemaal.
[2961.24 --> 2967.96]  En er is een verhaal met acteurs en om je zo realistisch mogelijke ervaring van een neerstortend vliegtuig te geven.
[2967.96 --> 2969.38]  Nou ja, dat soort shit.
[2971.26 --> 2977.40]  Ja, maar het punt is dat we nu, we snijden zoveel en dat is niet erg, ik vind dat vet interessant, maar er zit hier een vraagstuk in.
[2977.70 --> 2984.26]  Oké, stel je zou heel snel, heel goedkoop qua energie, zeg ik dan, ik argumeniseer hem even helemaal naar energie.
[2985.34 --> 2991.02]  Heel goedkoop wat betreft moeite een ervaring kunnen neerzetten die ongelooflijk overtuigend is.
[2991.02 --> 2997.50]  Dan al niet in VR, dan al niet in een soort van vloeibare realiteit met containers die fucking overtuigend zijn.
[2998.12 --> 3000.06]  Dat is een ding, wat als we dat kunnen?
[3000.58 --> 3009.12]  En dan, oké, maar wat als we die ervaringen niet laten maken door mensen, maar mensen in combinatie met of mensen volledig vervangen door algoritme.
[3009.54 --> 3013.98]  Dus dan komen we in hele vloeibare, dan al niet materiële ervaringen terecht.
[3013.98 --> 3020.88]  Die worden gecreëerd, misschien wel live, misschien wel gepersonificeerd door een algoritme, een soort superbrein.
[3021.86 --> 3026.34]  Wat is dat, ja, wat is dat voor wereld?
[3027.02 --> 3027.98]  Ja, dat is heel interessant.
[3028.16 --> 3035.48]  Want ik denk dat een, als het doel van de kunstenaar is om te zeggen, ik wil emotioneren, mensen aan het denken zeggen,
[3035.76 --> 3041.02]  het zetten, dingen blootleggen binnen die mensen zelf waar ze zelf niet bewust van zijn, uitdagen, you name it.
[3041.02 --> 3041.34]  Ja.
[3041.98 --> 3046.46]  Als je dan zegt, ik heb een toverstaf voor je, want we beschrijven nu eigenlijk een soort wonderwereld,
[3046.82 --> 3050.32]  waarin jij met een toverstaf kan zeggen, en nu wordt het deze ruimte, en nu ben je in een kerk,
[3050.42 --> 3052.96]  en nu draait heel de kerk op zijn kop, en bam, bam, bam, allemaal maar.
[3053.42 --> 3060.10]  Dan zeg je eigenlijk, als nou jouw inkomen gegarandeerd is, en je krijgt een toverstaf in VR of daarbuiten,
[3060.50 --> 3066.32]  om samen met of in je eentje de hele wereld om te toveren tot wat jij maar wil, qua kleur, geur en ruimte,
[3066.32 --> 3071.08]  wat kan je dan, wat een mooi, wat een goed nieuws, denk ik.
[3071.18 --> 3073.90]  Nou, niet voor iedereen, kan allemaal donkere kanten aan dit hele verhaal bedenken,
[3073.98 --> 3077.18]  van hellish landscapes waar je in gegooid wordt, vol met vuur, wat je helemaal niet wil.
[3077.62 --> 3083.74]  Maar als ik even de positieve kant bekijk, van wat je, je geeft eigenlijk een nieuwe potverf,
[3083.84 --> 3087.48]  we zitten heel erg op dat, ik schat er van mezelf dat ik dan altijd denk aan een schilder,
[3087.48 --> 3094.60]  bij een kunstenaar, maar goed, ik pak even dat, dat je eigenlijk hier een hele nieuwe tool zet,
[3094.84 --> 3100.68]  wat bijvoorbeeld een, wat ik een idee heb, als we praten over muziek, de tool,
[3100.76 --> 3104.48]  ik zit hier, ik zit te kijken naar een road podcaster met allemaal knopjes en ook een soort drumpad.
[3104.62 --> 3107.30]  Het apparaat waarmee wij deze podcast opnemen, ja.
[3107.54 --> 3111.94]  Daarom hadden alle filosofen altijd over tafels, als voorbeeld, want daar zaten ze aan.
[3112.40 --> 3114.74]  Moet je je voorstellen een tafel, ja daar zit je aan, maar goed, maakt niet uit.
[3114.74 --> 3117.90]  Of deze kop, ja precies, die staat op tafel. Deze road podcaster.
[3118.38 --> 3124.92]  Maar ik heb in de tooling die er is, fysiek en virtueel, en die wereld loopt ook heel erg door elkaar heen,
[3124.98 --> 3129.62]  voor een muziek, muzikant of een producer, of hoe je jezelf ook moet noemen, iemand die geluid maakt.
[3129.90 --> 3134.10]  Of het is het geluidsgolven laat oscilleren op een manier dat het mensen raakt.
[3135.34 --> 3138.32]  Dat ik daar juist heel erg vaak zie van, wauw, vet man.
[3139.00 --> 3142.22]  Hoe meer dingen ik heb om geluid mee te maken, hoe beter.
[3142.22 --> 3148.42]  Een enorme soort van omarming van, ja, weer een of ander wauws instrument uit Zwitserland,
[3148.50 --> 3151.64]  wat ik op mijn midi computer kan aansluiten, want ik heb nog meer.
[3152.20 --> 3158.36]  En dan zou je deze generative AI, als het dus niet zo'n enorme angst oproept,
[3158.54 --> 3161.70]  terecht rondom, daar gaat me inkomen, want dat is overschaduwd.
[3161.70 --> 3163.96]  Ja, maar dat hebben we al geparkeerd, die discussie.
[3164.02 --> 3165.34]  Maar ook voor die mensen die er naar kijken.
[3165.34 --> 3168.66]  Wat is de rol van mensen hier nog in, in het maken van dat soort kunst?
[3169.72 --> 3177.10]  Want soms dan denk ik, ja, ik zat te kijken naar een van de dingen die had de Joe Rogan stem gekloond,
[3177.24 --> 3183.60]  en maakt dan afleveringen van de Joe Rogan Experience, en mensen vinden het best oké om te luisteren.
[3184.12 --> 3184.92]  Nou, dat is dan nu.
[3185.36 --> 3187.96]  Gaat er een moment komen dat je Joe Rogan niet meer nodig hebt?
[3187.96 --> 3194.50]  Ja, ik weet niet, er is wel een scenario, er is wel een scenario in mijn hoofd dat daar wel van uitgaat.
[3194.58 --> 3200.14]  Dat er een moment is dat de kwinkslagen die hij maakt, de originele invalshoeken die hij heeft,
[3200.24 --> 3202.88]  dat al die shit gemodelleerd kan worden.
[3203.40 --> 3207.30]  En dat je gewoon een systeem hebt, à la longchain wat je net beschrijft,
[3207.62 --> 3210.78]  dat gewoon eindeloos gaat puzzelen, nou, waar gaan we het vandaag over hebben?
[3210.88 --> 3214.64]  En dat dan helemaal gaat doorredeneren, en dan is aflevering samenvat van 60 minuten,
[3214.72 --> 3215.76]  en dat gewoon interrekt.
[3215.76 --> 3220.04]  De zorg die ik voel nu je dit zo zegt, dus ik denk in eerste instantie,
[3220.36 --> 3224.76]  oké, als je ook kijkt naar de hologram artiesten die er zijn in Azië,
[3225.58 --> 3229.28]  ik zeg het even heel erg gestocheerd, maar daar zijn mensen fan van niet bestaande artiesten,
[3229.48 --> 3232.78]  een beetje als de gorillas, die hebben ook namelijk een heel virtueel voorkomen eigenlijk.
[3232.82 --> 3235.86]  Of ABBA in Zweden, het trekt volle zaal.
[3235.86 --> 3237.50]  Ja, precies, met de ABBA experience.
[3239.38 --> 3243.88]  Daarbij zijn we natuurlijk al zonder VR en AI, noem het een soort van, whatever.
[3243.88 --> 3247.40]  Daar zijn we het al heel erg aan het opzoeken en deels omarmen.
[3248.50 --> 3254.70]  Ik denk, stel je hebt nu een podcast zoals deze, met twee gecreëerde wezens,
[3255.26 --> 3260.38]  wel of niet gemodelleerd op ons twee, die aan de hand van wat zij op internet lezen,
[3260.80 --> 3263.74]  en aan boeken en meemaken, een podcast doen.
[3263.74 --> 3267.50]  Er zijn nu al een paar van die synthetic podcasts, maar die klinken nog heel erg synthetic,
[3267.74 --> 3269.16]  maar dat is een kwestie van tijd.
[3269.86 --> 3274.98]  Dan komen we denk ik een beetje op het punt van, oké, Alexander en Wietse,
[3275.04 --> 3277.98]  die hoeven er niet van te leven, dus die vinden dat niet per se erg.
[3278.14 --> 3280.56]  Misschien gaan ze er zelfs zelf naar luisteren, bijvoorbeeld.
[3280.94 --> 3285.28]  Dus dan zeggen wij, oh wat cool, als ze het beter doen dan wij, dan is dat niet erg.
[3285.62 --> 3290.36]  En dan zeggen wij heel romantisch, ja, de enige reden dat wij pokey opnemen,
[3290.36 --> 3293.56]  is omdat we vrienden zijn en dit sowieso leuk vinden om te doen.
[3294.02 --> 3297.26]  Dus al komen er zeven miljoen verschillende alternatieve pokies,
[3297.42 --> 3300.44]  gemaakt door synthetic AIs, dat maakt ons niet uit,
[3300.50 --> 3305.06]  want het ging ons helemaal niet om dat het geluisterd werd, bijvoorbeeld.
[3305.20 --> 3308.96]  Dat vind ik dus niet waar, wat ik nu zeg, maar het ging ons om het maken.
[3309.22 --> 3312.58]  Mijn punt is een beetje, iemand zegt, er is een schaakcomputer uitgevonden,
[3312.82 --> 3314.16]  honderdduizend keer zo goed als jullie allemaal.
[3314.36 --> 3316.88]  En al die schakers zeggen, oké, maar dan gaan we gewoon weer spelen.
[3317.22 --> 3319.70]  Want we zitten daar tegen elkaar te schaken, niet voor de wereld.
[3320.36 --> 3323.78]  En mijn zorg is nu, als er nu en die komen er,
[3324.26 --> 3328.14]  zeven miljoen alternatieve podcasts komen volledig gepersonificeerd.
[3328.22 --> 3331.00]  Dus je kan ook nog zeggen, ik luister eigenlijk liever naar twee vrouwen stemmen.
[3331.06 --> 3332.86]  Vind ik gewoon fijner, klik, twee vrouwen stemmen.
[3332.94 --> 3333.64]  Geen enkel probleem.
[3334.06 --> 3337.70]  Op een snelheid met een dialect die ik fijn vind, klik en dat is er.
[3338.42 --> 3339.16]  Die platamste dames.
[3339.18 --> 3340.88]  Ja, en dan draai je aan een schuif en dan kan je zeggen,
[3341.02 --> 3343.70]  dit is mijn niveau, dit is mijn kennis.
[3343.70 --> 3346.28]  Sterker nog, dat ding zegt, ik heb je Notion gelezen, ik weet wat jij weet.
[3346.32 --> 3348.32]  En ik heb maar twintig minuten, want ik zit in de auto.
[3348.32 --> 3355.36]  Nou, dan komen we wel tegen een soort, nee niet een soort, tegen een hard limit aan.
[3355.48 --> 3361.38]  Namelijk hoeveel tijd er in een dag zit voor mensen om te spenderen aan het tot zich nemen van enige vorm van.
[3361.62 --> 3365.40]  Dus dat resultaat erin, dat resulteert in een uiterste in.
[3365.82 --> 3368.88]  Dat er straks zeven miljoen podcasts zijn, allemaal op maat.
[3369.14 --> 3371.18]  En dat er niemand meer naar ons twee luistert.
[3371.28 --> 3371.56]  Klopt.
[3371.56 --> 3375.86]  Oké, en dan merk ik nu bij mezelf dat mijn leven voorbij is.
[3376.06 --> 3377.04]  Nee, not so much.
[3377.68 --> 3383.72]  Maar ik moet wel zeggen, persoonlijk, ik vind het oprecht leuk om met jou te praten.
[3385.20 --> 3389.98]  En ik zou het, ik denk dat ik het ook, ik heb het altijd gezegd in mijn vorige podcast Appels en Peren.
[3390.32 --> 3392.14]  Ik zou het ook nog doen voor niemand.
[3392.70 --> 3394.56]  Behalve dat wij daar samen zaten als een soort van...
[3394.56 --> 3401.52]  In deze redenering gaat een kunstenaar nog steeds verven, want dat is nou eenmaal de metafoor die wij gebruiken voor kunst.
[3401.54 --> 3402.14]  Zo zijn wij.
[3403.88 --> 3407.78]  Gaat die kunstenaar nog steeds verven ondanks dat niemand haar schilderij gaat zien.
[3408.02 --> 3408.28]  Exact.
[3408.84 --> 3409.62]  En doen wij dat dan?
[3409.62 --> 3410.54]  Daar geloof ik geen reet van.
[3410.66 --> 3411.56]  Maar doen wij dat dan?
[3411.62 --> 3413.68]  Ga jij nog opnemen als een nul luisteraars?
[3413.70 --> 3413.88]  Nee.
[3414.10 --> 3415.40]  Maar dit is toch onvermijdelijk?
[3415.90 --> 3419.26]  Want ik beschrijf nu toch niet een what if, ik beschrijf toch gewoon een what if when.
[3419.26 --> 3421.88]  Ja, oké, je neemt dit al lang aan als waarheid.
[3422.30 --> 3422.32]  Nu?
[3422.32 --> 3423.18]  Nee, ja, terwijl ik hier zit.
[3423.18 --> 3424.70]  Nee, maar dit gaat nog gebeuren, bedoel je.
[3424.98 --> 3427.46]  Je neemt al lang aan als waarheid dat dit nog gaat gebeuren.
[3427.56 --> 3428.00]  Nou, ik weet niet.
[3428.06 --> 3430.50]  Ik heb nog wel een scenario in mijn hoofd dat het dus niet zo goed gaat zijn.
[3430.60 --> 3432.20]  Maar je houdt daar niet eens rekening meer mee.
[3432.82 --> 3438.38]  Oh, nou kijk, als we dan iets met tijd doen, dan kan ik er een beetje dingen aan hangen.
[3438.46 --> 3441.30]  Als een soort recours, ga ik dan nu coole voorspellingen doen.
[3442.60 --> 3446.12]  Vijf jaar tot een podcast die zeer luisterbaar is.
[3446.54 --> 3449.44]  Tien jaar tot een podcast die zo goed is als deze.
[3449.44 --> 3454.22]  Leg je er nou maar bij neer, want er gaat toch niemand kijken naar wat je ooit nog maakt.
[3454.22 --> 3458.82]  Ik probeer denk ik de discussie uit het begin van dit verhaal heel persoonlijk te maken.
[3459.16 --> 3461.60]  Dat wij hier met z'n tweeën zitten en niemand meer luistert.
[3461.60 --> 3476.28]  Er zit dus blijkbaar een aanname onder, namelijk dat mensen gewoon geen rol meer hebben in het maken van creativiteit, anders dan voor hun eigen plezier.
[3477.30 --> 3478.18]  Dat vind ik nogal wat hoor.
[3478.24 --> 3479.84]  Dat zit er bij jou als aanname blijkbaar onder.
[3479.96 --> 3481.40]  Ik was daar nog niet, eerlijk gezegd.
[3481.80 --> 3484.54]  Nee, ik ben helemaal niet blij hiermee.
[3484.64 --> 3485.04]  Nee, nee, nee.
[3485.04 --> 3488.98]  Ik hoop dat het niet waar is, maar ik vraag me nu hardop af.
[3489.00 --> 3490.58]  Waarom zou dit in godsnaam niet gebeuren?
[3493.14 --> 3494.68]  Kijk, wat we kunnen doen.
[3494.78 --> 3497.42]  Ik heb een aantal vrienden van mij die luisteren podcast op 1.5.
[3497.98 --> 3498.36]  Ik weet niet hoe.
[3498.38 --> 3498.76]  Snelheid.
[3498.88 --> 3500.54]  Ja, ik vind dat absurd.
[3500.76 --> 3503.56]  En als ik dan mezelf hoor praten, denk ik, het gaat al best wel snel.
[3503.68 --> 3504.34]  Waar ben je mee bezig?
[3504.34 --> 3504.88]  Kan helemaal niet.
[3505.10 --> 3505.46]  Ben je niet goed?
[3506.08 --> 3512.14]  Maar als ik ze zou doorvragen, dan zou ze uiteindelijk moeten zeggen, ik wil gewoon meer.
[3512.76 --> 3514.84]  Ik heb maar, hoe lang ben je wakker op een dag?
[3514.94 --> 3516.42]  24 min 8, 16 uur.
[3517.06 --> 3518.14]  Ik heb maar 16 uur.
[3518.20 --> 3518.82]  Ik moet ook nog werken.
[3518.88 --> 3519.78]  Heb ik er nog 8 over?
[3519.88 --> 3520.42]  Ik moet nog meer doen.
[3520.50 --> 3521.44]  Ik heb er eigenlijk nog 3 over.
[3521.98 --> 3524.10]  Ik heb 3 uur per dag om iets tot me te nemen.
[3524.24 --> 3525.72]  YouTube, video, boeken lezen, whatever.
[3525.72 --> 3528.58]  Ik ga iets consumeren in mijn brein.
[3529.82 --> 3532.22]  En dan passen daar meer podcasts in op 1.5.
[3532.46 --> 3532.70]  Punt.
[3532.80 --> 3533.58]  Het is gewoon efficiënter.
[3533.58 --> 3536.46]  En mijn nieuwsgierigheid is groter dan de uren die ik heb.
[3536.56 --> 3537.62]  Dus ik moet wel op 1.5.
[3537.68 --> 3539.74]  Het is eigenlijk best wel verdrietig als je dit zo hoort.
[3539.92 --> 3540.58]  Ik heb haast.
[3541.10 --> 3541.84]  Het is gewoon best wel heftig.
[3542.44 --> 3546.08]  En dan komen daar nog 7 miljoen op maat gemaakte synthetische podcasts bij.
[3546.36 --> 3549.78]  Dan zijn jij en ik, Alexandre en Wietse, weg.
[3551.26 --> 3560.88]  En dan is de vraag, ik kijk niet naar video's van twee gaakalgoritmes die tegen elkaar schaken.
[3561.02 --> 3561.60]  Dat doe ik niet.
[3561.60 --> 3563.32]  Ik kijk nog steeds graag naar twee mensen.
[3563.58 --> 3565.78]  Misschien moeten we wel video gaan aanzetten hier.
[3566.18 --> 3568.56]  Maar dan nog, dat is dan nog wel een 15 jaar misschien.
[3568.68 --> 3569.70]  Ligt eraan met metayuman.
[3569.82 --> 3570.74]  Wat is dan het verschil?
[3570.90 --> 3571.64]  Hoe komt dat dan?
[3571.88 --> 3573.20]  Dat je daar, dat je...
[3573.20 --> 3574.12]  Daar zit een soort hoop.
[3574.58 --> 3575.24]  Ja, precies.
[3575.38 --> 3576.44]  Dat is heel hoopgevend.
[3576.64 --> 3580.04]  Dat wij met z'n allen menselijke schakers willen bekijken.
[3580.24 --> 3582.62]  Of menselijke voetballers willen bekijken.
[3582.62 --> 3585.08]  In plaats van dat we naar FIFA wedstrijdjes gaan zitten kijken.
[3585.56 --> 3586.88]  Of niet gespeeld door mensen.
[3587.08 --> 3588.82]  Want natuurlijk kijken mensen ook naar games.
[3588.82 --> 3595.60]  Maar een heel geautomatiseerd gespeeld FIFA spelletjes laat werkelijk helemaal nergens op.
[3596.06 --> 3596.54]  Hij ligt eraan.
[3596.68 --> 3599.36]  Als daar dus ook twee voice-over zitten van twee fake guys.
[3599.62 --> 3600.26]  Ja, precies.
[3600.42 --> 3605.82]  Dan, want voor mij komen wij nu een beetje tot de conclusie dat het misschien ook heel veel van die synthetische wedstrijdjes,
[3605.82 --> 3610.26]  dan noem ik ze maar even, zij zijn omdat wij dat menselijke missen.
[3610.72 --> 3617.62]  En nu door de ontwikkelingen van de laatste x jaar ons gaan beseffen dat misschien die menselijkheid,
[3618.24 --> 3623.96]  die romantiek, laat ik het dan maar even zo noemen, die backstory, dat die ook gefeekt kan worden.
[3624.28 --> 3625.66]  Dat die ook gefeekt kan worden.
[3625.74 --> 3626.28]  Dat die gefeekt kan worden.
[3626.36 --> 3626.74]  Ja, precies.
[3626.82 --> 3630.84]  Dus dat het alsnog het einde is van het kijken van schaakmatches tussen mensen.
[3631.22 --> 3634.18]  Omdat zelfs dat romantische nagedaan kan worden door die computer.
[3635.82 --> 3641.18]  Ja, wat moet ik ervan zeggen, Wietse?
[3641.34 --> 3643.96]  Nou ja, jij zou het dus niet meer maken als er niemand luistert.
[3644.14 --> 3644.78]  Dat hoor ik een beetje.
[3644.78 --> 3647.14]  Nee, ik zou het zeker niet meer maken als er niemand luistert.
[3647.26 --> 3649.10]  En ik kan me ook bijna niet voorstellen.
[3649.26 --> 3651.88]  Kijk, muziek maken voor jezelf, dat begrijp ik.
[3652.64 --> 3655.92]  En ik snap ergens, snap ik ook schilderen voor jezelf, begrijp ik ook.
[3656.44 --> 3660.86]  Maar het is toch altijd, moet er toch een deel van je zijn dat je denkt,
[3660.86 --> 3662.36]  ik ga dit aan anderen laten zien.
[3663.04 --> 3664.64]  Ik ga aan anderen laten horen wat ik maak.
[3664.64 --> 3666.16]  Ik ga anderen laten zien wat ik maak.
[3666.24 --> 3668.12]  Ja, maar dan stuur je dit op naar Ernst Jan.
[3668.28 --> 3670.96]  En dan zegt hij, na drie weken zeg je, heb je het al geluisterd?
[3671.00 --> 3672.22]  Ik heb ook je opgenomen met Wietse.
[3672.30 --> 3676.70]  En dan zegt hij, vriend, er zijn zulke fantastische podcasts voor mij op maat gemaakt.
[3676.82 --> 3678.22]  Ik heb er gewoon nog geen tijd voor gehad.
[3679.26 --> 3681.08]  Ja, dat zal het gaat zijn.
[3681.94 --> 3686.36]  Nou, op die note, ik bestuur je met heel veel plezier de rest van deze dag in.
[3686.36 --> 3688.92]  Dit is waar het op uitkomt.
[3689.76 --> 3693.78]  Kunst maak je straks voor jezelf en voor niemand anders.
[3694.22 --> 3695.84]  Dit was Poki, tot volgende week.
[3696.94 --> 3699.78]  En, ben je er al achter of Eneco dynamisch bij je past?
[3700.32 --> 3701.16]  Of nog niet?
[3701.82 --> 3704.24]  Doe de test op eneco.nl slash test.
[3705.34 --> 3707.30]  Mensen helpen een bewuste keuze te maken.
[3708.20 --> 3708.82]  We doen het nu.
[3709.30 --> 3709.78]  Eneco.
[3709.78 --> 3711.50]  TV Gelderland 2021

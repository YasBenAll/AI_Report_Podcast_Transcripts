Video title: Geavanceerde AI-stem van ChatGPT nu beschikbaar via omweg ｜ ✨ Poki S03E04
Youtube video code: ujCsmm4DvNc
Last modified time: 2024-09-26 21:13:47

------------------ 

[0.00 --> 4.80]  Bij Grant Thornton heten we je als starter en als professional van harte welkom.
[5.08 --> 7.96]  En dan mag je alles zelf uitzoeken.
[8.46 --> 14.12]  Bij ons krijg je namelijk direct verantwoordelijkheid op interessante projecten voor interessante klanten.
[14.48 --> 16.94]  En je hoeft je niet eerst jarenlang te bewijzen.
[17.44 --> 19.64]  Hier ben je de regisseur van je eigen succes.
[19.88 --> 21.70]  En vanaf dag 1 draai je volop mee.
[22.18 --> 23.62]  Ontdek het zelf op onze website.
[24.10 --> 28.46]  Grant Thornton. Accountancy, Tax, Advisory en You.
[30.00 --> 33.32]  Hi, met Maxime Meiland van Maxime's Mini Misdaad.
[35.64 --> 39.12]  Nee, en toen heb je die onderbroek erachter gelaten.
[40.32 --> 46.78]  In de gloednieuwe podcast Maxime's Mini Misdaad hoor je juicy misdaden die onze luisteraars begaan zijn.
[47.32 --> 52.58]  Samen met mijn beste vriendinnen Demi en Roxy zullen wij die misdaden helemaal uitpluizen.
[53.08 --> 56.40]  Kom jij ook gezellig luisteren via jouw favoriete podcast app.
[56.40 --> 63.30]  Welkom bij Polki, de Nederlandse podcast over kunstmatige intelligentie.
[63.36 --> 68.44]  Waar we uitzoeken welke invloed AI gaat hebben op ons werk, ons leven en de samenleving.
[68.58 --> 70.96]  En Milou, we zijn compleet in de stress hier.
[70.96 --> 73.72]  Ja, hallo. Ik had gisteren het draaiboek af in de middag.
[74.62 --> 76.50]  Wat gebeurt er vervolgens allemaal in de avond?
[76.50 --> 80.56]  En ik word vanochtend wakker en het hele draaiboek is overhoop en er staat allemaal andere shit tussen.
[80.84 --> 82.50]  En er is zoveel gebeurd.
[82.56 --> 88.16]  Het is misschien wel de grootste aankondiging in de wereld van AI sinds de koeppleging bij OpenAI.
[88.34 --> 89.58]  Inmiddels een half jaar geleden.
[90.00 --> 94.30]  Wietse, wat blijft er het meest hangen op dit moment tijdens deze underspoor?
[94.38 --> 95.30]  In dit moment van spanning?
[95.46 --> 98.20]  Ik zit hier gewoon met dezelfde trui aan en mijn haar door de war.
[98.46 --> 100.14]  Een soort van verwart figuur.
[100.14 --> 102.58]  We gaan het hebben over de leegloop bij OpenAI.
[102.92 --> 107.86]  We gaan het hebben over Facebook die een grote conferentie heeft gehad met allerlei grote aankondigingen.
[107.86 --> 109.46]  Noem jij dat Facebook? Ik noem dat gewoon meta.
[109.46 --> 111.08]  Meta, excuse, je hebt helemaal gelijk.
[111.18 --> 113.36]  Meta met een aantal hele grote aankondigingen.
[113.46 --> 119.90]  Google heeft een nieuw taalmodel en als klappende vuurpijl is ook nog de advanced voice mode van OpenAI gelanceerd.
[120.00 --> 122.70]  Waarmee je kan praten tegen een menselijke versie van ChatGPT.
[123.06 --> 125.22]  Dat en meer vandaag in Poki.
[130.14 --> 138.36]  Ja, hiertoe zijn wij toch op aarde.
[139.26 --> 140.58]  Zo voelt het wel een beetje.
[140.96 --> 142.70]  Jij staat stijf van de stress, Alexander.
[142.76 --> 148.42]  Ja, ik heb het gevoel dat ik niet genoeg heb gelezen om dit allemaal samen te vatten.
[148.58 --> 151.54]  Maar dan denk ik, ja, dit is voor de luisteraar oké.
[151.60 --> 153.08]  Wij hebben uren zitten lezen.
[153.32 --> 156.18]  Laten we maar de balans opmaken voor zover we zijn gekomen.
[156.18 --> 162.06]  Ik heb dit gevoel trouwens altijd als ik hier aanschuiven dat ik, ja, er valt niet tegenop te lezen van wat jullie natuurlijk al weten.
[162.14 --> 162.70]  Maar dat hoeft ook niet.
[162.98 --> 164.50]  Nou ja, het was wel een rustig zomertje.
[164.62 --> 165.40]  Het is echt ineens omgedraaid.
[166.12 --> 167.42]  Het is gewoon september nu.
[167.50 --> 168.18]  Het is gewoon gebeurd.
[168.34 --> 168.38]  Ja.
[168.58 --> 171.24]  Maar allemaal best wel in 24, 36 uur zeg maar.
[171.36 --> 172.44]  Het ging echt, echt hard.
[172.52 --> 174.78]  Ik kwam Wietse dus ook tegen gisteravond op een concert.
[175.06 --> 175.80]  Dat was...
[175.80 --> 177.36]  Wij dachten dat we gewoon naar een concert konden.
[177.52 --> 179.00]  Wij dachten dat we naar een concert konden.
[179.00 --> 183.12]  Maar toen, los van elkaar, even voor de helderheid, maar toen kwamen we elkaar op het einde tegen.
[183.12 --> 185.84]  Toen hebben we nog even een redactie-nabespreking gehad.
[186.04 --> 190.54]  Omdat we dachten, ja, toch wel dat die voice mode van OpenAI is een groot ding.
[190.62 --> 191.56]  Gaan we het straks over hebben.
[191.68 --> 191.86]  Ja.
[192.52 --> 193.08]  Hoofdonderwerp.
[193.12 --> 194.56]  Ja, dat die in Nederland spreekt.
[194.68 --> 197.52]  En dat je er nu mee kan praten met wat trucen.
[197.62 --> 198.50]  Dus gaan we het straks over hebben.
[199.14 --> 204.12]  Maar we waren echt in de melkweg, waren we draaiboek aan het herprogrammeren.
[204.70 --> 206.08]  Terwijl Wietse bezweet is.
[206.36 --> 209.50]  En overigens detail, dezelfde trui aan heeft als gisteren.
[209.50 --> 213.20]  Ja, dat verklaart A, de lucht.
[213.36 --> 215.78]  En hoe ik wakker werd in de laatste minuut.
[216.16 --> 217.92]  Waarom ben ik op zo'n afstand van jullie gezet?
[219.00 --> 219.72]  Hoe dan ook.
[219.80 --> 221.96]  Om meerdere redenen grote stress.
[222.18 --> 222.96]  Ja, was een leuk concert.
[223.34 --> 223.92]  Ja, was heel leuk.
[223.92 --> 224.30]  Heel vet.
[224.48 --> 225.18]  IJslandse DJ.
[225.64 --> 226.56]  Ja, dat is aanrader.
[226.58 --> 227.26]  IJslandse DJ.
[227.34 --> 229.68]  Beetje dezelfde vibe als ons openingstune, overigens.
[230.24 --> 233.28]  Ik krijg dan een beetje Fire Saga vibes als ik aan IJsland denk.
[233.40 --> 236.28]  Hebben jullie die, dat is het Eurovisie Songfestival, die film.
[236.74 --> 237.92]  Van Volcano Man.
[237.92 --> 239.38]  Dat is waar je aan denkt bij IJsland.
[239.38 --> 239.56]  Daar denk ik aan.
[239.62 --> 241.20]  Als je dat niet kent, zoek het even op op YouTube.
[241.34 --> 243.52]  Is echt één van mijn lievelingsliedjes, denk ik.
[243.64 --> 245.94]  Ja, ze hebben ook heel leuke minimalistische DJ's.
[246.10 --> 246.86]  Hoe dan ook.
[246.88 --> 248.16]  Ja, we gaan beginnen.
[248.66 --> 250.30]  Open AI, daar gaan we mee beginnen.
[250.86 --> 254.56]  Dat was een tijd lang een non-profit bedrijf.
[254.66 --> 258.56]  Toen werd het een deels for-profit bedrijf en nog steeds een stukje non-profit.
[259.10 --> 264.84]  Maar het is aangekondigd, het gaat nu toch maar gewoon een for-profit bedrijf worden.
[265.02 --> 266.38]  Ze doen hun best daarvoor.
[266.38 --> 267.40]  Dat willen ze.
[267.58 --> 267.76]  Ja.
[267.84 --> 268.94]  Dat is uitgesproken.
[269.12 --> 269.26]  Ja.
[269.42 --> 272.10]  Wat moeten we daar, wat vinden we daarvan?
[272.68 --> 276.22]  Is dat, want dat is meteen natuurlijk het idee van de grote kapitalistische wereld.
[276.30 --> 280.48]  Ze gaan nu allemaal op AI inzetten en dat gaat straks de wereld overspoelen.
[280.58 --> 283.46]  Ze willen heel veel winst maken en dat gaat waarschijnlijk ten koste van ons.
[283.46 --> 289.46]  Als men zegt, ik kijk er naar uit om me meer te concentreren op de technische en productgerichte delen van het bedrijf.
[290.84 --> 299.48]  Dat is dan een van de, ik weet niet wat voor, ja, ik vind dat ik weet niet, dit is wel een cryptische manier om te zeggen, ik ben tot nu toe veel afgeleid.
[299.48 --> 303.08]  Ja, weet ik veel.
[303.16 --> 305.92]  Omdat het moeilijker is om geld op te halen, kan ik me zo voorstellen.
[306.10 --> 310.66]  Ja, omdat er natuurlijk een cap op zit, dus hoeveel er return on kan komen.
[310.66 --> 313.34]  Precies, dus het is lastiger voor investeerders om daaraan mee te doen.
[313.36 --> 315.14]  Nou, lastiger, ze hebben er natuurlijk gewoon geen zin in.
[315.52 --> 317.46]  Ja, ik denk dat dat inderdaad de lading dekt.
[318.60 --> 325.08]  En daarnaast is hij natuurlijk heel veel tijd bezig geweest met het balanceren van de belangen van allerlei medewerkers.
[325.24 --> 330.34]  Die, als ik het goed begrijp en tussen de regels doorlees, heel erg bezig zijn met veiligheid.
[331.02 --> 336.70]  En Sam Altman wil gewoon, zoals hij dat zegt, zich concentreren op technische en productgerichte delen van het bedrijf.
[336.88 --> 338.50]  Een heel spicy zinnetje.
[338.50 --> 340.92]  Ja, het is allemaal een beetje PR geschreven.
[341.20 --> 347.26]  Maar ik denk, ja, weet je, zijn de sugar babes de sugar babes nog als alle drie die meiden niet meer dezelfde meiden zijn als toen de band begon?
[347.56 --> 349.50]  Wauw, laat die erg best stilstaan.
[349.70 --> 350.44]  Wat een mooie metafoor.
[350.46 --> 351.58]  Wat is met OpenAI gebeurd?
[351.74 --> 354.82]  Daar had ik korte tijd voor om dit voor te breiden, deze metafoor.
[354.94 --> 355.88]  Maar dat er zoveel nieuws is.
[356.00 --> 356.10]  Ja.
[356.58 --> 365.14]  Maar die, kijk, als je al die founding members van OpenAI, die zijn bij een research organisatie, een non-profit research organisatie gaan werken.
[365.14 --> 369.24]  En dat wordt langzaam, terwijl ze erin zitten, omgetoverd tot iets anders.
[369.70 --> 370.92]  En op een gegeven moment gaat dat wringen.
[371.06 --> 371.76]  Nou, dat deed het al.
[372.24 --> 377.94]  En nu worden gewoon de daadwerkelijke stappen genomen, juridisch ook en op papier, om er een for profit van te maken.
[378.06 --> 380.70]  En dan vallen er dus een hoop mensen weg die zeggen, ja, hier en niet verder.
[380.84 --> 383.24]  Maar het is een gigantische reorganisatie.
[383.24 --> 383.58]  Ja.
[383.90 --> 386.74]  Dertien van de oorspronkelijke oprichters waren er.
[386.82 --> 388.52]  Er zijn er nog maar drie van over.
[388.70 --> 393.76]  Ja, gisteren is Mira Muratti, de CTO, vertrekt na zes en een half jaar.
[394.90 --> 398.76]  En dat wat ik ook heel, dus dat is een groot ding, de CTO die vertrekt.
[399.08 --> 402.18]  Die was ook heel zichtbaar in alle OpenAI-aankondigingen.
[402.34 --> 405.00]  En wat Altman daarover zei op X was,
[405.00 --> 409.56]  When Mira informed me this morning that she was leaving, puntje, puntje, puntje.
[409.66 --> 411.00]  En dan gaat de hele statement door.
[411.12 --> 413.96]  Oftewel, hij hoorde op de dag zelf dat ze ging vertrekken.
[414.00 --> 416.72]  Wat veel zegt over de chaos die daar heerst.
[416.72 --> 419.14]  Ja, en er stond ook nog een mooi detail in dat hij zei,
[419.64 --> 423.58]  ook al ben ik er niet blij mee, ben ik er wel, is het oké of zo?
[423.58 --> 425.64]  En daar waren de reacties op, hoe bedoel je oké?
[425.94 --> 427.28]  Je bent haar baas niet meer.
[427.42 --> 430.28]  Dus dat is een soort van ook gek dat zij een blessing krijgt om te gaan.
[430.70 --> 430.82]  Ja.
[430.98 --> 431.20]  Maar goed.
[431.26 --> 433.66]  Ja, nou dat hij er een soort van vrede mee zou hebben.
[433.66 --> 435.72]  Maar ja, dan kun je je natuurlijk afvragen of dat echt zo is.
[435.74 --> 436.60]  Er is geen bad blood.
[436.90 --> 439.40]  Hij zegt ook, leiderschapsveranderingen zijn natuurlijk.
[439.58 --> 441.52]  Vooral bij snel groeiende bedrijven.
[441.70 --> 444.56]  Ja, maar ik denk dat, en waarom is dit in pokey interessant,
[444.86 --> 447.78]  voorbij zeg maar de soort RTL Boulevard roddelpers.
[447.98 --> 450.82]  Nee, maar ik denk dat het wel heel belangrijk is,
[450.86 --> 454.74]  omdat een grote speler, naast Entropic en alle anderen,
[455.14 --> 457.56]  maar OpenAI is wel, en daar gaan we vandaag ook meer over hebben,
[457.88 --> 461.18]  heel erg productgefocust bedrijf.
[461.26 --> 462.28]  Dat is Sam Altman ook.
[462.28 --> 466.14]  Alexander en ik zijn constant en enthousiast en maak ons zorgen.
[466.58 --> 468.10]  Het zijn een soort van tech giddy erom,
[468.16 --> 470.56]  dat we denken, lekker man, lekker shippen, wat doen ze nu toch knap.
[471.00 --> 474.86]  En tegelijkertijd is het wel een soort go fast, break things mentaliteit van vroeger.
[475.38 --> 481.24]  En ik denk dat daarin, met de machten die OpenAI heeft op die hele industrie,
[481.74 --> 484.04]  dat zo'n bedrijf van binnenuit uitgehold wordt,
[484.14 --> 486.70]  of hoe je het wil noemen, of omgetoverd wordt tot iets heel nieuws,
[486.98 --> 488.04]  is wel heel erg relevant.
[488.04 --> 490.46]  Het is gewoon Game of Thrones.
[490.58 --> 493.86]  Het is echt een strijd, die is inmiddels voorbij, die strijd.
[493.92 --> 495.62]  Het is duidelijk wie er gewonnen hebben.
[495.76 --> 496.26]  Ja, heer Altman.
[496.26 --> 497.70]  Ja, en Sam is er nog.
[498.36 --> 504.16]  Ja, hij heeft ook zichzelf 7%, want dat kan nu dan, dat hoort er ook bij,
[504.22 --> 505.70]  dat hij aandelen nu in het bedrijf krijgt.
[505.78 --> 507.14]  Hij heeft 7% van de aandelen.
[507.74 --> 512.16]  Betekent dus, als het bedrijf gewaardeerd is op 150 miljard dollar,
[512.36 --> 516.66]  dan is het 10,5 miljard dollar in aandelen.
[516.78 --> 517.20]  Voor hem.
[517.20 --> 520.62]  Ja, hij zegt zelf, nou ja, ik heb dat geld volgens mij helemaal niet nodig.
[520.78 --> 522.00]  Tenminste, dat heeft hij altijd gezegd, toch?
[522.04 --> 523.02]  Ik hoef helemaal niet meer geld.
[523.14 --> 526.98]  Ik zag een citaat van afgelopen mei nog, waarin hij in een podcast zei,
[527.16 --> 531.74]  het is voor mensen zo moeilijk om te begrijpen dat ik geen aandelen in dit bedrijf heb,
[531.80 --> 536.18]  dat ik soms denk, ik kan maar beter aandelen nemen,
[536.26 --> 538.88]  want dan hoeven we er niet meer over te praten, maar ik hoef geen aandelen.
[539.02 --> 543.32]  Nou, dat was een half jaar geleden, en inmiddels heeft hij aandelen van 10 miljard.
[543.32 --> 547.74]  Ja, het is opvallend, dat kun je zeggen.
[549.26 --> 555.58]  Nee, Altman maakt het zichzelf niet makkelijker als het gaat over de mate
[555.58 --> 558.44]  waarin de buitenwereld, denk ik, vertrouwen in hem heeft.
[558.58 --> 562.08]  Want hoe hij alleen al dit dingetje doet, ja, maakt het uit.
[562.26 --> 562.76]  Ik denk wel...
[562.76 --> 563.68]  Voor ons wel, maar...
[563.68 --> 571.62]  Ja, ik denk wel met de soort van overheden die streng willen reguleren,
[571.62 --> 575.12]  denk ik wel dat het helpt dat je te boek staat als een betrouwbare partij.
[575.80 --> 580.66]  En ik denk, die saga waar dat vertrouwen afbrokkelde was natuurlijk begon
[580.66 --> 583.08]  toen Ilja Sutskever en Jan Leikie weggingen.
[584.18 --> 587.62]  Nou, John Schulman is weggegaan naar concurrent Anthropic.
[588.56 --> 591.74]  Greg Brockman, de president, is met verlof tot eind van dit jaar.
[591.86 --> 593.22]  Zijn CTO is dus nu weg.
[593.64 --> 594.88]  Gisteren aangekondigd.
[594.88 --> 599.34]  En dat hoorde Altman op dezelfde ochtend als haar vertrek.
[599.76 --> 602.30]  Verder is de chief research officer gisteren vertrokken.
[602.44 --> 604.42]  En de VP research zijn weggegaan.
[604.48 --> 606.14]  Dat zijn allemaal belangrijke functies.
[606.74 --> 608.24]  Ja, ik weet niet of dit nou...
[608.24 --> 613.14]  Ik kan me voorstellen dat de Europese Commissie die OpenAI goed in de gaten houdt.
[613.26 --> 617.88]  En de Senaat in Amerika, die Altman twee jaar geleden nog aan een soort van...
[617.88 --> 620.96]  Die werd heel vriendelijk behandeld.
[621.16 --> 628.00]  In plaats van de kruisverhoren die Meta en Amazon en Google voor de kiezen kregen.
[628.38 --> 631.40]  Werd Altman echt behandeld als een soort van boy wonder.
[631.72 --> 634.14]  Die eindelijk iemand die echt geeft om veiligheid.
[634.26 --> 639.32]  En die gelijk toegeeft vanaf het begin dat het kritisch benaderd dient te worden.
[639.82 --> 644.22]  Ja, de shitzooi die het nu is gaat daar echt niet bij helpen, denk ik.
[644.30 --> 646.70]  Nee, het is niet meer de sugarbabes, zoals Wiet zal zijn.
[646.70 --> 648.78]  Het zijn zeker niet meer de sugarbabes.
[648.86 --> 651.48]  En om daar een strik omheen te doen, de sugarbabes zijn weer bij elkaar.
[651.56 --> 652.56]  De originele drie members.
[652.66 --> 655.06]  En hebben hun oude naam teruggevochten bij de rechter.
[655.38 --> 656.14]  Dan weten jullie dat.
[656.46 --> 657.36]  Ze zijn er weer.
[657.92 --> 660.02]  Het is goed dat je daar ook van op de hoogte blijft fietsen.
[660.10 --> 660.70]  Ongelooflijk.
[660.90 --> 662.04]  Ja, ze waren op Lowlands.
[662.30 --> 662.54]  Klopt.
[662.78 --> 663.76]  De originele members.
[664.10 --> 664.24]  Ja.
[667.84 --> 671.04]  Ja, nou, een upgrade voor Google Gemini.
[671.30 --> 672.26]  Google Gemini zeg je.
[672.70 --> 673.82]  Niet geen manier, maar Gemini.
[673.82 --> 675.64]  Ze werden weer zo weggeblazen door OpenAI.
[675.64 --> 676.96]  Nee, daar zit daar gewoon iemand bij.
[677.02 --> 677.54]  Zielig eigenlijk.
[678.16 --> 679.40]  Jongens, Google even iets.
[679.96 --> 680.10]  Ja.
[680.22 --> 682.20]  Dan drukken ze op die knop en dan blazen ze weer weg.
[682.32 --> 683.48]  Dan ontslaan ze weer drie mensen.
[683.68 --> 686.84]  Ja, nou, er komen twee nieuwe modellen aan.
[687.12 --> 689.62]  Wij geven Google de aandacht die, nou ja, het verdient.
[689.80 --> 690.82]  Daar ga ik helemaal niet over.
[690.92 --> 692.82]  Maar op zich wil ik er best even tijd aan besteden.
[693.12 --> 693.78]  Twee nieuwe modellen.
[693.92 --> 694.24]  Ze klinken...
[694.24 --> 694.90]  Zo zielig.
[694.90 --> 697.86]  Dit is een super grote aankondiging.
[698.28 --> 699.42]  En wij doen net als...
[699.42 --> 699.60]  Oh.
[700.26 --> 701.06]  Ik werd het al weer vergeten.
[701.14 --> 703.04]  Een soort stagiaire achterin de bus.
[703.20 --> 704.20]  Maar dat is niet zo.
[704.30 --> 706.48]  Nee, maar op zich vindt het het misschien helemaal niet erg.
[706.58 --> 711.00]  Want als je je modellen weer namen geeft als wachtwoorden, dan doe je ook niet heel erg je best.
[711.50 --> 711.74]  Toch?
[711.80 --> 712.92]  Om het heel flashy te maken.
[713.28 --> 716.12]  Het heet Gemini 1.5 Pro 002.
[716.12 --> 718.56]  En Gemini 1.5 Flash 002.
[718.56 --> 719.96]  Ik denk, wat bedoel je nou met wachtwoorden?
[720.12 --> 721.38]  Maar zo lang mogelijk bedoel je?
[721.52 --> 723.28]  Ja, dit is hoe Miloeder wachtwoorden doet.
[723.44 --> 723.60]  Ja.
[723.72 --> 723.88]  Goed.
[724.24 --> 725.26]  Lange wachtwoorden.
[725.36 --> 727.14]  Ja, er zitten ook leestekens in en zo.
[727.22 --> 727.90]  Dat is helemaal goed.
[728.08 --> 728.68]  Volgens het boekje.
[729.36 --> 729.84]  Goed.
[729.92 --> 732.80]  Wat is nou beter geworden in deze modellen?
[733.10 --> 737.68]  Ze zijn 20% beter in de wiskunde vergeleken met bepaalde benchmarks.
[737.68 --> 739.18]  Hoe je dat kan testen.
[740.24 --> 745.68]  Meer dan 50% prijsverlaging voor prompts onder de 128...
[745.68 --> 747.52]  100.000 tokens.
[748.10 --> 748.80]  100.000 tokens.
[748.82 --> 749.94]  Dat is wel een groot ding.
[750.12 --> 751.62]  Ja, maar het is wel ook een beetje...
[751.62 --> 754.12]  Het is gewoon een beetje het soort taalmodellen bingo, toch?
[754.26 --> 755.82]  Dus altijd is het wel weer iets met tokens.
[755.82 --> 757.24]  Deze dingen betekenen echt wat.
[757.76 --> 760.82]  Maar ik bedoel, uiteindelijk is het wat ik eruit heb gehaald.
[760.94 --> 762.66]  Ik bedoel, die modellen tof, die presteren prima.
[762.84 --> 763.78]  Google doet mee, zeg maar.
[763.84 --> 765.52]  Ze rennen nog mee in een soort van marathon.
[765.82 --> 768.64]  Zitten ze nog in het voorste groepje een beetje zo erachteraan te puffen.
[769.16 --> 771.48]  Maar door die prijsverlagingen, die zijn best wel scherp.
[771.92 --> 774.72]  Ik bedoel, ergens kan je zeggen als een bedrijf op prijs moet gaan concurreren,
[774.74 --> 775.54]  is het ook een soort zet.
[775.68 --> 777.74]  Dat je zegt, kom maar bij mij, want ik ben goedkoop.
[777.82 --> 779.40]  Maar tegelijkertijd, als zij het kunnen...
[779.40 --> 783.00]  Ik zat te kijken wat nu de inkomsten van Anthropic zijn.
[783.00 --> 788.50]  Dus de grote, ja, ik zou zeggen op dit moment toch wel het meest shiny taalmodel voor de kenners.
[788.58 --> 788.66]  Ja.
[788.74 --> 789.26]  Zou ik zeggen.
[789.74 --> 795.94]  Dat hun inkomsten zijn natuurlijk opgesplitst tussen abonnementen.
[796.78 --> 800.46]  Ze werken aan reclameinkomsten, API-toegang.
[800.64 --> 803.04]  Nou, er zijn een aantal bronnen van inkomsten die ze hebben.
[803.04 --> 807.88]  En veruit de grootste bron van inkomsten is via third-party platforms zoals Amazon.
[808.36 --> 814.76]  Waar dus developers websites hosten en dan via Amazon eigenlijk Anthropic modellen aanroepen.
[814.90 --> 815.04]  Ja.
[815.44 --> 816.36]  Waardoor je geld verdient.
[816.48 --> 819.56]  Nou, en ik kan me heel goed voorstellen dat als jij dat allemaal via Amazon doet,
[819.64 --> 823.00]  dan is het werkelijk een drop-down, een ander model kiezen.
[823.10 --> 823.24]  Exact.
[823.24 --> 824.00]  En dan heb je een ander model.
[824.12 --> 827.64]  Dus dan doet het er heel veel toe dat je model zoveel goedkoper is geworden.
[827.96 --> 831.72]  Nee, ik denk dat in dat opzicht, je hebt natuurlijk de prijs per token.
[832.12 --> 836.00]  Als jij bijvoorbeeld de Gemini-modellen direct aanroept, of de modellen van Anthropic.
[836.22 --> 840.30]  Maar waar jij nu over praat, is eigenlijk dat binnen Amazon, dat heet volgens mij Bedrock,
[840.38 --> 841.48]  dat platform, coole naam.
[842.56 --> 846.06]  Mag je de modellen van Anthropic draaien bij Amazon zelf.
[846.18 --> 847.42]  Echt op hun infra, zeg maar.
[847.42 --> 850.38]  Maar en daar zitten ook weer prijzen aan verbonden.
[850.52 --> 853.20]  Of die meteen matchen met de prijzen die Anthropic aanbiedt, weet ik niet.
[853.50 --> 856.84]  Maar ik ga ervan uit dat dat bijna een op een doorgaat.
[857.10 --> 858.94]  En dan is het wel precies wat jij zegt.
[859.38 --> 863.30]  Je hebt een applicatie, je switch van model, test of de kwaliteit hoog genoeg is.
[863.62 --> 866.64]  Als de prijs 50% is voor iedere request, is die switch zelf gemaakt.
[866.98 --> 869.44]  Dus de loyaliteit aan taalmodellen is ook laag technisch.
[869.74 --> 873.70]  Dus in dat opzicht kan het zomaar zo zijn dat er misschien wel een tussenprogrammaatje komt
[873.70 --> 875.96]  die automatisch van model switcht aan de hand van de prijs.
[875.96 --> 880.88]  Ik noem maar wat, om een beetje een soort markt van taalmodellen te creëren met real-time pricing.
[881.22 --> 885.46]  Maar het is een significant ding dat zo'n groot deel van de omzet via die API's gaat.
[885.54 --> 889.68]  Ja, en dat zegt dus ook eigenlijk dat, en dat is de grote, is het nou een hype vraag
[889.68 --> 892.48]  en is het niet gewoon blockchain, big data of de volgende edgecloud.
[893.62 --> 897.44]  Uiteindelijk is het nu wel zo dat er wel geld verdiend wordt bij deze bedrijven.
[897.66 --> 902.24]  Niet in verhouding tot wat erin moet en wat het kost om het aan energie allemaal af te rekenen.
[902.24 --> 907.04]  Maar er zijn al inkomsten en dat klinkt misschien absurd dat ik nu zeg, het zijn bedrijven met inkomsten.
[907.38 --> 910.46]  Maar dat is in de techscene de laatste tien jaar best wel uitzonderlijk.
[911.04 --> 913.84]  Want heel veel wordt er gewoon opgepropt zonder dat ze al iets verdienen.
[914.00 --> 916.26]  En dan soort van, dat komt later wel een keer.
[916.60 --> 918.04]  Er is al iets van een businessmodel.
[918.34 --> 918.80]  Dat is wat tof.
[919.02 --> 921.96]  Ja, en winst is nog natuurlijk de vraag of dat er ooit van gaat komen.
[922.08 --> 923.58]  Maar het zou zomaar kunnen, we sluiten het niet uit.
[923.84 --> 925.10]  Nog even twee andere verbeteringen.
[925.10 --> 929.50]  Er is een twee keer snellere output en een drie keer lagere latentie.
[929.62 --> 933.94]  Als ik dat even naar Nederlands vertaal, dan denk ik dus oké, het is twee keer sneller en drie keer minder traag.
[934.64 --> 935.58]  Dat is toch een beetje hetzelfde?
[936.00 --> 939.94]  Ja, we zullen het straks wel zien als we OpenAI gaan testen.
[940.16 --> 941.42]  Wat we trouwens via een VPN doen.
[941.56 --> 943.08]  Dus daar is de latency niet helemaal real.
[943.48 --> 948.26]  Maar het feit dat je snel antwoorden krijgt, kan in een een-op-een gesprek heel prettig zijn.
[948.38 --> 949.52]  Want anders duurt het zo lang.
[949.52 --> 954.12]  Maar het kan ook in een applicatie waar jij op de achtergrond dingen aanroept prettig zijn voor de eindgebruikers.
[954.12 --> 955.32]  Maar ik snap Miloes' vraag wel.
[955.42 --> 958.32]  Drie keer lagere latency, terwijl er ook staat twee keer snellere output.
[958.44 --> 960.86]  Is die latency dan voor voice of zo?
[961.18 --> 964.00]  Nou ja, je hebt het moment dat het antwoord begint binnen te komen.
[964.38 --> 966.28]  Van vraag tot en met beginnen binnen te komen.
[966.40 --> 968.48]  En dan, hoe snel stroomt het antwoord dan binnen?
[968.86 --> 969.98]  Zo hoor ik hem.
[969.98 --> 970.44]  Ja, ja, ja.
[970.56 --> 973.24]  Dus uiteindelijk is het gewoon een stuk sneller geworden allemaal.
[973.48 --> 976.48]  Hoe snel gaat je telefoon en hoe snel praat die grootte aan de andere kant van de lijn?
[976.48 --> 977.06]  Ja, oké.
[977.06 --> 983.50]  En beter begrip van lange context en verbeterde visuele capaciteiten.
[983.74 --> 984.72]  Kortom, nou ja, gewoon dus.
[985.48 --> 987.12]  Ja, dat is allemaal weer een stukje beter geworden.
[987.52 --> 992.06]  Is het nou echt supergoed dan ook weer vergeleken met OpenAI en O1?
[992.30 --> 993.88]  Wat we natuurlijk vorige week hebben opvangen.
[993.88 --> 998.38]  Het heeft geen grote schokgolven door de community gebracht, behalve de prijs.
[998.46 --> 1001.24]  Nee, op een benchmark, de GPQA benchmark.
[1001.24 --> 1006.24]  Dat is de Google Proof Question & Answer benchmark voor biologie en natuurlijk in de scheikunde.
[1006.24 --> 1009.56]  Daar scoort Gemini nu 59 procent.
[1009.98 --> 1012.46]  Dat is 13 procent meer dan de vorige versie.
[1012.90 --> 1017.76]  Maar de O1-familie van OpenAI die had scoren eens rond de 80 procent.
[1017.94 --> 1019.42]  Dus dat halen ze nog niet.
[1019.74 --> 1020.54]  En er is een testje.
[1020.54 --> 1022.92]  En dat is hun eigen benchmark ook nog eens van Google, als ik jou zo hoor.
[1023.56 --> 1025.68]  Ja, de Google Proof Question & Answer.
[1025.78 --> 1026.18]  Interessant.
[1026.18 --> 1029.18]  Ja, dus zelfs die weten ze niet.
[1029.36 --> 1030.50]  Nou, dan weet je dat het...
[1030.50 --> 1034.54]  Ik moet wel zeggen, kijk, ik gebruik al die taalmodellen als consument.
[1034.80 --> 1040.24]  En ik heb dus ook Gemini gebruik ik vaak, omdat ik zit nou eenmaal in het Android ecosysteem.
[1040.30 --> 1043.30]  En ik heb zo'n mooie pixel horloge en zo.
[1044.02 --> 1046.18]  Het is echt bizar hoe die...
[1047.00 --> 1051.40]  Hoe onduidelijk alles is in Android nu met die nieuwe modellen.
[1051.40 --> 1055.12]  Dus je hebt Google Assistant, dat is de serie van Android.
[1055.64 --> 1056.62]  Die kan gewoon dingen.
[1056.82 --> 1058.14]  Die kan je timers mee instellen.
[1058.34 --> 1060.52]  En die kan je vragen, hoe laat gaat het Rijksmuseum open?
[1060.76 --> 1065.86]  Of gewoon vragen waarvan je gewend bent dat het een taalassistent antwoord op kan geven.
[1066.68 --> 1069.68]  Vervolgens kun je dan switchen naar Gemini.
[1069.94 --> 1071.14]  In plaats van Google Assistant.
[1071.72 --> 1074.20]  Dus dan moet je kiezen als gebruiker.
[1074.74 --> 1077.76]  Ik bedoel, ik snap dan nog wel dat er een verschilende technologie achter zit.
[1077.82 --> 1079.40]  Maar ik moet dan even denken aan mijn moeder.
[1079.40 --> 1083.54]  Hoe moet die die keuze maken tussen twee dingen?
[1083.64 --> 1085.64]  Nou oké, stel je kiest dan Gemini.
[1086.28 --> 1090.40]  Dan kan hij dus allerlei dingen niet die het oude ding wel kan.
[1091.10 --> 1092.78]  Dus die oude assistent wel kan.
[1092.86 --> 1095.26]  Dus als jij vraagt naar wanneer gaat het Rijksmuseum open?
[1095.74 --> 1096.78]  Dan gaat hij opeens googlen.
[1096.92 --> 1098.90]  In plaats van dat hij gewoon de cijfers uit Google Maps gebruikt.
[1098.96 --> 1100.04]  Wat veel betrouwbaarder is.
[1100.92 --> 1104.50]  En allerlei andere basale dingen die de assistent wel kon.
[1104.62 --> 1105.72]  Kan Gemini opeens niet.
[1105.72 --> 1109.94]  Want die kan alleen maar als Chattipity een beetje ratelen op basis van wat in het model heeft zitten.
[1110.06 --> 1111.66]  Nou dat is al super onduidelijk.
[1111.66 --> 1115.08]  Vervolgens moet je voor als je betaalt voor je domeinnaam.
[1115.16 --> 1116.12]  Wat denk ik heel veel mensen doen.
[1116.22 --> 1117.20]  Zo'n workspace account.
[1117.94 --> 1120.46]  Ik heb een betaald e-mailadres met mijn eigen domeinnaam.
[1121.08 --> 1124.26]  Dan werkt de advanced versie van Gemini.
[1124.36 --> 1125.56]  Krijg je niet gratis.
[1125.80 --> 1128.76]  Terwijl je dat als gewone consument wel gratis krijgt.
[1128.76 --> 1133.26]  Soms dan werken extensies zodat hij bij je Gmail kan.
[1133.40 --> 1135.14]  Werkt weer wel in persoonlijke accounts.
[1135.24 --> 1137.16]  Werkt weer niet voor zakelijke accounts.
[1137.32 --> 1140.42]  Omdat het is zo fucking onduidelijk wat die mensen...
[1140.42 --> 1142.86]  En op mijn Pixel Watch, dat is mijn horloge.
[1143.06 --> 1146.46]  Kan ik alleen maar de oude assistent doen en niet Gemini.
[1147.02 --> 1148.62]  Het is zo onduidelijk allemaal.
[1148.76 --> 1150.68]  Dat je af en toe Google ook wel denkt.
[1150.86 --> 1152.26]  Jullie verdienen het ook.
[1152.60 --> 1153.76]  Om niet serieus te nomineren.
[1153.76 --> 1156.68]  Ze zijn gefragmenteerd en alles ligt.
[1156.86 --> 1157.64]  Het zijn allemaal loslijntjes.
[1157.66 --> 1158.68]  Het is zo'n shitzooi die volgd.
[1158.68 --> 1160.68]  Ik denk dat je voelt als eindgebruiker...
[1161.20 --> 1164.92]  De interne teams en politiek van Google groeien in de producten.
[1165.38 --> 1169.48]  Ik wil wel dan nog zeggen dat Apple zojuist een nieuwe iPhone gelanceerd.
[1169.52 --> 1171.56]  Die ze de AI-Phone noemen waar geen AI op zit.
[1171.64 --> 1172.60]  Want die was nog niet klaar.
[1173.30 --> 1173.80]  Fair enough.
[1173.80 --> 1175.10]  Ik wil maar zeggen.
[1175.62 --> 1178.36]  Wat jij nu voelt is een rush job.
[1178.62 --> 1180.28]  Al die bedrijven zijn aan het rennen.
[1180.38 --> 1182.56]  Teams zijn over elkaar heen aan het vallen.
[1182.56 --> 1183.92]  Om het zo snel mogelijk te shippen.
[1184.44 --> 1186.48]  En daardoor hebben we nu als eindgebruiker...
[1186.48 --> 1189.22]  Een soort iets wat op alpha niveau van software is.
[1189.56 --> 1191.42]  Wat eigenlijk nog niet gelanchet had moeten worden.
[1191.62 --> 1193.76]  Wat gelanchet wordt voor de aandeelhouders.
[1193.76 --> 1194.72]  Ik snap niet waarom ze dat doen.
[1194.80 --> 1195.78]  Voor de aandeelhouders zeg je?
[1196.30 --> 1197.60]  Als je nu niet een...
[1197.60 --> 1200.16]  Ik bedoel, als we nog even terugpakken op waar we net over hadden...
[1200.16 --> 1201.50]  Met al die wisselingen bij OpenAI.
[1202.00 --> 1204.28]  Dat er stond dat Mira het nu...
[1204.28 --> 1204.96]  Zeg ik haar naam goed?
[1205.06 --> 1205.26]  Mira?
[1205.46 --> 1205.64]  Ja.
[1205.78 --> 1206.78]  Dat Mira haar...
[1206.78 --> 1208.54]  Dat ze nu haar vertrek aankondigt.
[1208.64 --> 1210.78]  Omdat we nu in een upswing zitten met OpenAI.
[1210.78 --> 1213.36]  En upswing als in de pers is positief.
[1213.44 --> 1215.26]  We hebben net een nieuw model gelanceerd.
[1215.52 --> 1217.08]  Een mooi moment voor mij om te gaan.
[1217.54 --> 1218.80]  Die upswings creëren.
[1218.90 --> 1220.66]  Dat is ook grotendeels beeldvorming.
[1221.22 --> 1221.76]  Dus dat is...
[1221.76 --> 1223.72]  Ik wil niet zeggen dat het OE model niks is.
[1223.88 --> 1226.64]  Maar ze zijn wel echt allemaal...
[1226.64 --> 1229.54]  Al die bedrijven onderling ook over elkaar heen aan het vallen op dit moment.
[1229.66 --> 1230.50]  Met kijk wat wij hebben.
[1230.60 --> 1231.28]  Kijk wat wij hebben.
[1231.32 --> 1232.70]  Er wordt haast gemaakt.
[1232.70 --> 1235.84]  Het is toch allemaal heel korte termijn wat mensen dan nu vinden.
[1235.96 --> 1237.10]  Maar het maakt toch allemaal niks uit.
[1237.18 --> 1239.36]  Als jij een super goed model hebt over drie jaar.
[1240.02 --> 1241.54]  Dan is iedereen daar toch van onder de indruk.
[1241.62 --> 1243.28]  Wat maakt het dan uit dat je drie jaar niks hebt laten horen?
[1243.40 --> 1244.66]  Als je gewoon gelooft ook in je eigen...
[1244.66 --> 1245.42]  Nou ja, ik denk dat...
[1245.42 --> 1246.88]  Kijk voor een investeerder.
[1247.26 --> 1251.94]  Op het moment dat er 5% kans is dat AGI echt een ding gaat worden in de komende duizend dagen.
[1251.94 --> 1254.76]  Of in duizenden dagen zoals Sam Altman nu zegt.
[1254.88 --> 1256.80]  Hebben we een soort tussen de duizend en tienduizend.
[1258.10 --> 1261.60]  En die kans is 5% dat een van die bedrijven dat wordt.
[1261.82 --> 1265.46]  Dan ben je eigenlijk als jij een groot vermogen beheert al verplicht om te investeren.
[1265.56 --> 1266.76]  Want die kans is te groot.
[1267.16 --> 1270.88]  En ik denk op die manier dat al die grote fang zeg maar.
[1271.00 --> 1273.64]  De meta's, de Apple's, man heet het dan nu waarschijnlijk.
[1273.90 --> 1277.40]  Maar goed, die rij aan Microsoft's en Google's.
[1277.90 --> 1281.28]  Die moeten allemaal op een bepaalde manier wel een signaal geven aan de markt van.
[1281.28 --> 1283.44]  We are on it en we shippen.
[1283.88 --> 1285.46]  Ja, het heeft invloed op de beurskoers.
[1285.62 --> 1288.40]  Het heeft invloed op de mate waarin je personeel kan aantrekken.
[1288.60 --> 1289.16]  Dat snap ik wel.
[1289.20 --> 1291.16]  Het is best wel korte termijn ook tegelijkertijd.
[1291.30 --> 1293.24]  De beurskoers maakt op zich nu niet uit.
[1293.32 --> 1295.54]  Ik snap dit wel inderdaad als je inderdaad denkt van oké.
[1296.00 --> 1297.74]  Straks denken mensen dat we helemaal niet meedoen.
[1298.18 --> 1299.28]  Dus dan hebben we helemaal geen geld.
[1300.30 --> 1302.58]  Maar op zich zou je kunnen zeggen van ja, een lange termijn.
[1302.58 --> 1304.98]  Dat maakt het niet uit wat die beurskoers nu op hele korte termijn is.
[1304.98 --> 1307.58]  En je speelt dus met je reputatie.
[1307.70 --> 1310.30]  Want Alexander wordt gebruikt als proefkonijn.
[1310.56 --> 1311.08]  Dat voel je.
[1311.28 --> 1312.12]  Als eindgebruiker.
[1312.36 --> 1313.86]  Daar moet je niet te ver mee gaan denk ik.
[1314.04 --> 1314.16]  Ja.
[1314.60 --> 1314.80]  Ja.
[1315.14 --> 1315.26]  Ja.
[1318.42 --> 1318.86]  Ja.
[1319.06 --> 1321.68]  En dan hebben we er gisteravond natuurlijk allemaal voor gezeten.
[1321.78 --> 1322.22]  Behalve jullie.
[1322.30 --> 1323.58]  Want jullie waren bij een concert.
[1324.06 --> 1326.12]  Maar ik heb me hierin verdiept.
[1327.72 --> 1328.16]  Onnodig.
[1328.50 --> 1328.68]  Nee.
[1329.08 --> 1329.72]  Meta Connect.
[1329.84 --> 1332.04]  Dat was Meta Connect 2024.
[1332.48 --> 1332.64]  Ja.
[1332.64 --> 1333.74]  Ik zeg Meta trouwens.
[1333.86 --> 1333.96]  Ja.
[1333.96 --> 1335.38]  Ik denk dat dat echt oké is.
[1335.44 --> 1335.76]  Meta.
[1337.02 --> 1337.42]  Ja.
[1337.84 --> 1339.82]  Zuckerberg heeft een mooie presentatie gegeven.
[1339.92 --> 1340.54]  Ik moet wel zeggen.
[1341.14 --> 1341.78]  Deze man.
[1342.16 --> 1344.00]  Wat vroeger echt totale nerd was.
[1344.10 --> 1346.70]  En die volgens mij nog geen normaal gesprek met iemand kon voeren.
[1347.36 --> 1350.92]  Die is nu misschien wel de meest naturel man op het podium.
[1350.92 --> 1352.28]  Die zijn dingen staat te presenteren.
[1352.62 --> 1353.82]  Ik vond het echt aangenaam.
[1354.04 --> 1355.38]  Hij heeft een totale restyle ondergaan.
[1355.50 --> 1356.26]  Met kettingen.
[1356.60 --> 1357.66]  Baggy T-shirts.
[1357.92 --> 1358.60]  En krul haar.
[1358.82 --> 1358.94]  Ja.
[1359.10 --> 1359.78]  Maakt veel uit.
[1359.90 --> 1360.62]  En zijn T-shirt.
[1360.74 --> 1361.46]  Er staat altijd wel.
[1361.62 --> 1362.26]  Nou niet altijd.
[1362.36 --> 1363.32]  Maar vaak iets op.
[1363.38 --> 1364.00]  Over hemzelf.
[1364.36 --> 1364.42]  Ja.
[1364.42 --> 1365.08]  Nu stond er.
[1365.56 --> 1366.38]  Outzoek.
[1366.60 --> 1367.38]  Out nihil.
[1367.74 --> 1368.24]  Dus dat.
[1368.42 --> 1369.76]  Dat is Latijn voor.
[1370.90 --> 1372.00]  Zonder zoek is er.
[1372.68 --> 1373.84]  Er is niemand buiten zoek.
[1373.96 --> 1374.32]  Eigenlijk.
[1374.44 --> 1375.14]  Er is niks.
[1375.14 --> 1375.70]  Oké.
[1375.92 --> 1376.70]  Dan de zak.
[1376.78 --> 1377.32]  Vind ik wel een goeie.
[1377.60 --> 1377.78]  Ja.
[1377.98 --> 1378.76]  Nou dat eventjes.
[1379.48 --> 1379.70]  Maar goed.
[1379.70 --> 1380.90]  Laten we zeggen dat hij wat fashion.
[1380.92 --> 1381.48]  Designers.
[1381.56 --> 1383.14]  En wat adviseurs kan betalen.
[1383.42 --> 1384.62]  Iemand moet een studie doen.
[1384.80 --> 1385.54]  In de PR.
[1385.94 --> 1387.44]  Ik denk dat het niet te onderschatten is.
[1387.54 --> 1389.48]  De mate waarin hij nu serieuzer wordt genomen.
[1389.60 --> 1389.74]  Ja.
[1390.06 --> 1391.90]  Nou en hij heeft ook wel wat leuke dingetjes aangekondigd.
[1391.92 --> 1393.30]  Hij heeft zeker wat leuke dingetjes aangekondigd.
[1393.32 --> 1394.08]  Waar zullen we beginnen?
[1394.66 --> 1397.34]  De nieuwe compacte AI modellen voor telefoons en randapparatuur.
[1397.40 --> 1398.20]  Oh jij.
[1398.64 --> 1398.86]  Ja.
[1399.58 --> 1400.80]  Nou weet je wat wel grappig is.
[1400.90 --> 1403.18]  Want ik zit nu het er met elkaar over hebben denk ik.
[1404.14 --> 1407.14]  Ik had net even zo'n supercut gekeken van die presentatie.
[1407.36 --> 1408.98]  Dan klap ik er even in 10 minuten doorheen.
[1409.10 --> 1409.88]  Maar het was wel mooi.
[1409.88 --> 1411.70]  Want Apple heeft natuurlijk die Vision Pro.
[1411.80 --> 1413.12]  Daar hebben we het gewoon allemaal even niet meer over.
[1413.30 --> 1415.22]  Dat is een soort van nobody talks about Bruno.
[1415.36 --> 1416.78]  Maar nobody talks about Vision Pro.
[1417.82 --> 1421.50]  En Zuckerberg presenteert daar gewoon een 300 dollar MetaQuest 3S.
[1421.62 --> 1422.42]  Heet dat ding volgens mij.
[1422.76 --> 1424.04]  Die best wel tof is.
[1424.44 --> 1425.78]  Maar dan voor 300 dollar.
[1426.02 --> 1428.52]  Om vervolgens allemaal open soort staalmodellen eruit te klappen.
[1428.60 --> 1429.90]  Die hij ook nog eens geïntegreerd heeft.
[1430.00 --> 1431.16]  In al zijn apps tegelijk.
[1431.16 --> 1432.58]  Het is best wel.
[1432.58 --> 1433.68]  Hij zei ook een paar keer.
[1433.84 --> 1434.62]  Helje en zo.
[1434.90 --> 1435.26]  Dat ik dacht.
[1435.36 --> 1436.34]  Ja het is wel een flex.
[1436.72 --> 1437.02]  Als in.
[1437.44 --> 1437.52]  Ja.
[1438.32 --> 1441.22]  Het feit dat je een VR bril of AR bril.
[1441.56 --> 1443.92]  Ik weet niet wat het eigenlijk is tegenwoordig.
[1443.98 --> 1444.66]  AR bril.
[1445.22 --> 1448.30]  Lanceert die 10 keer goedkoper is dan de Apple versie.
[1448.46 --> 1449.58]  Met vergelijkbare.
[1449.78 --> 1451.88]  Ja het is allemaal niet zo polished als Apple het doet.
[1452.00 --> 1453.68]  Maar het doet dat er toe voor die prijs.
[1453.78 --> 1454.92]  Het is gewoon een goede Dacia.
[1455.08 --> 1455.26]  Ja.
[1456.46 --> 1457.68]  Dus ik wil alleen maar zeggen.
[1457.90 --> 1459.18]  Het is grappig dat we.
[1459.66 --> 1461.16]  Die focus ligt dan heel erg op Apple.
[1461.32 --> 1462.38]  Wat Pixel.
[1462.50 --> 1464.12]  Wat Google aan het doen is met hun Pixel line.
[1464.54 --> 1466.74]  En wat Open en jij aan het doen is met mogelijk zelfs hardware.
[1466.96 --> 1467.66]  Samen met Johnny Wave.
[1467.68 --> 1470.30]  Meta is niet te onderschatten.
[1470.34 --> 1470.90]  Dat bedoel ik.
[1471.12 --> 1471.86]  Nee zeker.
[1472.00 --> 1473.34]  Dus laten we er induiken.
[1473.54 --> 1475.24]  Dus er zijn nieuwe modellen aangekondigd.
[1475.32 --> 1476.50]  Lama 3.2.
[1477.22 --> 1479.38]  Met een model van 1 miljard parameters.
[1479.58 --> 1482.04]  En een van 3 miljard parameters.
[1482.82 --> 1484.18]  Zegt er eens wat zinnigs over.
[1484.34 --> 1485.52]  Dit zijn de kleinere modellen.
[1485.56 --> 1485.70]  Ja.
[1486.46 --> 1489.08]  Als je rond die 1 à 3 miljard zit.
[1489.16 --> 1493.24]  Dan zit je een beetje in die range van die 5 PHI modellen van Microsoft.
[1493.82 --> 1495.04]  En de Gemma's van Google.
[1495.50 --> 1496.82]  En Apples lokale model.
[1496.96 --> 1497.46]  Waar ik de naam.
[1497.66 --> 1499.04]  Dat heeft geen gave naam volgens mij.
[1499.12 --> 1500.10]  Wat wel on Apples is.
[1500.10 --> 1503.38]  Maar misschien willen ze die technische ondergroep niet laten zien aan de eindgebruiker.
[1504.56 --> 1508.46]  In dit geval is het zo dat die 1 miljard modellen even goed presteren.
[1508.46 --> 1510.74]  Op bepaalde taken als 7 miljard modellen van anderen.
[1511.06 --> 1513.00]  Als het gaat om bijvoorbeeld tekstsamenvattingen maken.
[1513.00 --> 1514.62]  Dan heb je echt niet een groot model nodig.
[1514.64 --> 1515.54]  En dat is substantieel.
[1515.64 --> 1517.74]  Want dan even om het concreet te maken voor de luisteraar.
[1517.86 --> 1518.60]  Ik ben niet voor iedereen.
[1518.80 --> 1521.22]  Maar een recente smartphone.
[1521.36 --> 1522.52]  Of bijvoorbeeld een Raspberry Pi.
[1522.64 --> 1523.76]  Dus kleine embedded hardware.
[1523.88 --> 1525.30]  Die kan draaien in een thermostaat.
[1525.40 --> 1525.82]  Ik noem maar iets.
[1525.82 --> 1528.32]  Daar kan wel een 1 miljard model op draaien.
[1528.56 --> 1530.98]  En dan kan je dus lokaal op een klein apparaatje iets draaien.
[1531.40 --> 1536.20]  En het is de truc om met kleinere modellen above your way te punchen.
[1536.28 --> 1539.18]  Dus dat je eigenlijk dingen kan die grotere modellen twee maanden geleden konden.
[1539.56 --> 1540.84]  Ja en dat het sneller is.
[1540.98 --> 1543.04]  Dus dat het chiller is op je telefoon om te doen.
[1543.14 --> 1548.16]  Omdat je je wil niet wachten op een AI die een agenda uitnodiging voor je maakt.
[1548.24 --> 1549.20]  Op basis van een e-mail.
[1549.30 --> 1550.76]  Ik noem maar even een simpel voorbeeld.
[1550.96 --> 1552.18]  Dus een vriend e-mailtje zegt.
[1552.18 --> 1553.64]  Zullen we morgen afspreken?
[1553.78 --> 1556.26]  En dat jij dan een knop kan drukken met voeg dit toe aan mijn agenda.
[1556.40 --> 1557.44]  Dat dat dan ook gebeurt.
[1558.64 --> 1559.70]  Of dat je het.
[1560.06 --> 1560.82]  Nou ja wat ik zeg.
[1561.26 --> 1561.62]  Samenvattingen.
[1561.74 --> 1566.02]  Wat toch ook voor meta toepassingen ook te bedenken is.
[1566.10 --> 1567.88]  Met die assistent die ze hebben.
[1567.96 --> 1569.98]  Dus dat soort dingetjes zouden lokaal kunnen draaien.
[1569.98 --> 1571.46]  Dan hebben ze ook nog twee grotere modellen.
[1571.58 --> 1573.72]  Een van 11 miljard en een van 90 miljard.
[1574.26 --> 1575.78]  Voor grotere complexe taken.
[1575.88 --> 1576.94]  Die dingen kunnen nu meer.
[1577.14 --> 1577.90]  Ze zijn sneller.
[1579.68 --> 1580.70]  En het zijn omnimodellen.
[1580.70 --> 1584.70]  Dus je kan nu ook voice en beeld erin en uit.
[1585.30 --> 1585.68]  Juist.
[1585.82 --> 1587.14]  En dat heeft hij dan ook gelijk.
[1587.20 --> 1589.72]  Want dat is allemaal inderdaad om jou te channelen Milou.
[1589.96 --> 1591.26]  Wat hebben we hier aan?
[1591.38 --> 1592.20]  Allemaal leuk en aardig.
[1592.30 --> 1592.84]  Grotere modellen.
[1593.00 --> 1593.88]  Maar wat kunnen we hiermee?
[1593.98 --> 1595.44]  Nou Mark Zuckerberg heeft natuurlijk.
[1596.08 --> 1597.46]  In tegenstelling tot Google.
[1598.00 --> 1600.14]  Waarbij het allemaal een stuk moeilijker voor te stellen is.
[1600.24 --> 1601.48]  In onze populaire cultuur.
[1601.60 --> 1603.44]  En hoe gewone mensen dit gaan gebruiken.
[1604.00 --> 1605.32]  Heeft meta natuurlijk.
[1605.40 --> 1607.14]  Een soort van cultureel kapitaal.
[1607.14 --> 1610.58]  Want iedereen besteedt daar zijn vrije tijd op.
[1610.96 --> 1611.90]  Op hun platforms.
[1612.22 --> 1613.88]  En daar hebben ze dus allerlei dingen voor bedacht.
[1614.02 --> 1617.42]  Hoe ze AI gaan gebruiken om in hun dingen te integreren.
[1617.60 --> 1619.98]  Het is best wel een uitgebreide lijst van dingen hoor.
[1620.24 --> 1621.92]  Waar die in gebruikt wordt.
[1621.92 --> 1625.08]  Bijvoorbeeld de meta AI assistent.
[1625.22 --> 1625.78]  Dat is de.
[1625.94 --> 1626.74]  Ja hoe moet je dat zeggen.
[1626.84 --> 1628.02]  De chat GPT van meta.
[1628.28 --> 1632.68]  Of de Gemini van meta.
[1633.08 --> 1634.72]  Een chat assistent.
[1634.94 --> 1636.74]  Die nu dus ook kan praten.
[1636.88 --> 1638.26]  En kan praten een beetje.
[1638.36 --> 1641.06]  Zoals we dat nu kennen van Gemini Live.
[1641.16 --> 1641.68]  Zo'n stem.
[1641.86 --> 1643.04]  Of van open AI's.
[1643.50 --> 1644.74]  Advanced voice mode.
[1645.26 --> 1646.48]  En wat meta dan doet.
[1646.60 --> 1649.56]  Is niet een of andere vage robot stem.
[1649.56 --> 1651.78]  of in ieder geval een niet bestaande stem doen.
[1651.88 --> 1653.88]  Maar die hebben de stemmen van Dame Judi Dench
[1653.88 --> 1655.22]  en John Senna
[1655.22 --> 1657.16]  en Kristen Bell
[1657.16 --> 1659.42]  gelicenseerd, gekloond.
[1660.12 --> 1661.38]  Met toestemming allemaal.
[1661.62 --> 1663.74]  Ja, ja, ja. Die krijgen waarschijnlijk voor wat zentjes.
[1664.16 --> 1665.84]  En die komen dus beschikbaar in Instagram,
[1666.08 --> 1668.08]  in Messenger, in WhatsApp, in Facebook.
[1668.28 --> 1669.72]  Even voor de helderheid, niet in Europa.
[1670.22 --> 1670.94]  Ja, nee.
[1671.16 --> 1673.56]  Dat goes without saying inmiddels, ja.
[1673.62 --> 1675.26]  Maar goed, dat is wel geregeld nu.
[1675.26 --> 1676.70]  Dus dat gaat er nu komen.
[1676.70 --> 1680.34]  Je kan Meta-AI nu gaan gebruiken
[1680.34 --> 1681.84]  om afbeeldingen te analyseren.
[1681.96 --> 1684.42]  Dus bijvoorbeeld bloemen die je fotografeert,
[1684.50 --> 1686.08]  identificeren, ik noem maar even wat.
[1686.58 --> 1690.26]  Of stel je maakt een foto van je koelkast.
[1690.98 --> 1691.84]  Een vogel kan ook.
[1692.06 --> 1693.92]  Of je maakt een foto van je koelkast en zegt dan
[1693.92 --> 1695.34]  wat voor recepten kan ik hiermee maken.
[1695.42 --> 1697.54]  We kennen dit inmiddels van OpenAI.
[1697.70 --> 1699.74]  We zijn een beetje verwend daarmee, denk ik,
[1699.76 --> 1700.76]  dat dit een chatje patie kan.
[1700.76 --> 1702.50]  Maar het grote verschil is natuurlijk
[1702.50 --> 1706.34]  dat Zuckerberg een stukje meer gebruikers heeft
[1706.34 --> 1707.98]  op zijn grotere platform.
[1708.14 --> 1709.10]  En Zuckerberg zegt daarover,
[1709.20 --> 1711.52]  Meta-AI is op weg om de meest gebruikte
[1711.52 --> 1713.34]  AI-assistent ter wereld te worden
[1713.34 --> 1714.54]  tegen het eind van dit jaar.
[1714.72 --> 1717.46]  Bijna 500 miljoen maandelijks actieve gebruikers.
[1717.58 --> 1719.18]  En dan is het nog steeds niet gelanceerd
[1719.18 --> 1720.92]  in enkele grotere landen.
[1721.16 --> 1722.34]  Hij bedoelt ons.
[1722.98 --> 1725.30]  Dus wat gaat er gebeuren als daar eenmaal ook is?
[1725.40 --> 1727.50]  Dus het is een, nou ja, ik zou niet zeggen
[1727.50 --> 1729.42]  sleeping giant, want hij is niet zo sleeping.
[1729.42 --> 1732.26]  Maar dit is een groot ding.
[1732.52 --> 1734.06]  Ja, nee, want Apple heeft natuurlijk de hardware
[1734.06 --> 1735.46]  met die iPhone die in je zak zit.
[1735.56 --> 1737.32]  Maar al die programma's die we dagelijks gebruiken,
[1737.42 --> 1739.22]  dat is allemaal, nou tenminste ik gebruik
[1739.22 --> 1740.34]  Instagram, WhatsApp.
[1740.64 --> 1740.92]  Juist.
[1741.40 --> 1742.94]  Dat is allemaal meta.
[1743.06 --> 1745.04]  Maar ik denk in dat opzicht dat het wel boeiend is
[1745.04 --> 1749.40]  dat op het gebied van social, gewoon social media,
[1749.60 --> 1751.48]  dat is een soort hele lege term geworden.
[1751.60 --> 1753.32]  Maar goed, dingen als Instagram, WhatsApp,
[1753.48 --> 1755.94]  waar je met elkaar online communiceert,
[1755.94 --> 1757.04]  vrienden en familie onderling.
[1757.70 --> 1759.64]  Daar is meta heer en meester.
[1760.82 --> 1762.58]  Dat heeft Apple een paar keer geprobeerd.
[1762.78 --> 1763.04]  Lame.
[1763.14 --> 1764.46]  Dat heeft Google een paar keer geprobeerd.
[1764.68 --> 1765.00]  Lame.
[1765.30 --> 1767.00]  Dat heeft Microsoft nooit geprobeerd, denk ik.
[1767.10 --> 1768.18]  Misschien wel, dat weten we niet eens.
[1768.32 --> 1768.50]  Lame.
[1769.30 --> 1772.46]  Maar hun soort van, we gaan ook proberen Facebook te maken.
[1772.62 --> 1775.18]  Apple ging proberen een muziek netwerk te maken.
[1775.44 --> 1776.40]  Ping, jaren geleden.
[1776.58 --> 1777.50]  Het is allemaal niet gelukt.
[1778.00 --> 1781.42]  En nu komt die AI-golf plus dat VR mixed reality ding
[1781.42 --> 1784.10]  wat nog steeds geprobeerd wordt en echt wel ergens een toekomst kent,
[1784.18 --> 1785.04]  maar dat heeft tijd nodig.
[1785.04 --> 1788.10]  Dan is meta toch wel lekker gepositioneerd, hoor.
[1788.66 --> 1790.76]  Nu met, ja, dat we het dan ook nog even over hebben,
[1790.86 --> 1795.54]  de virtual avatars, ja, fake influencers of synthetic influencers,
[1795.64 --> 1797.44]  klinkt iets neutraler, in je feeds.
[1797.84 --> 1799.30]  Die feeds zijn allemaal van meta.
[1799.58 --> 1802.62]  En ik moet zeggen, dat viel me pas op toen jij het net zo uitlegde,
[1802.96 --> 1806.34]  het is wel grappig hoe zij zo'n presentatie eigenlijk best wel hardcore nerdy doen,
[1806.42 --> 1807.98]  want ze zijn echt een developer company ook.
[1808.12 --> 1811.18]  Meta is helemaal opgezet dat er ook open source is.
[1811.30 --> 1812.94]  Niet helemaal, maar ook open source is.
[1812.94 --> 1817.70]  Dat ze heel veel leveren aan andere partijen om te integreren in dat grote meta-netwerk.
[1818.20 --> 1822.16]  En dan gaan ze praten over, hier, een taalmodel wat je nu kan downloaden met zoveel parameters,
[1822.44 --> 1826.52]  om vervolgens in dezelfde presentatie helemaal naar de voorkant te gaan van die technologie.
[1826.52 --> 1826.82]  Ja.
[1826.94 --> 1828.80]  En te zeggen, hier, een synthetische influencer.
[1828.86 --> 1829.16]  Precies.
[1829.56 --> 1830.24]  En dat is wat knap.
[1830.52 --> 1831.24]  Zoveel vetter.
[1831.36 --> 1832.08]  Het is zo vet.
[1832.48 --> 1837.84]  En dus OpenAI moet dat beperken tot wat ze in haar eigen app kan doen.
[1837.98 --> 1839.30]  Die ene app die ze hebben.
[1839.44 --> 1842.80]  En dan toegeven, dat is vet dat je een foto kan maken van die koelkast,
[1842.86 --> 1845.32]  dat die recept van maakt en dat er een spraakassistent in zit.
[1845.78 --> 1847.58]  Dat spreekt tot de verbeelding.
[1847.58 --> 1850.64]  Als je een soort van de creativiteit kan opbrengen.
[1850.76 --> 1853.88]  En meta is het gewoon in het platform aan het uitrollen.
[1853.90 --> 1855.14]  Je hebt er weinig verbeelding voor nodig.
[1855.24 --> 1856.40]  Ja, je gaat het gewoon gebruiken.
[1856.50 --> 1861.30]  Dus een van de dingen die ze gelanceerd hebben, is in Instagram Reels live vertalingen.
[1861.50 --> 1865.04]  De demo was dat er een kookvideo was in het Spaans.
[1865.58 --> 1871.24]  Waarbij live lip synchronisatie, stemnabootsing in een andere taal in het Engels namelijk werd gedaan.
[1871.24 --> 1873.64]  Waardoor je een Spaanse video opeens in het Engels kan kijken.
[1874.08 --> 1878.92]  Groot ding, waar we al lang over speculeren dat dit op een gegeven moment gaat gebeuren.
[1879.12 --> 1881.94]  Meta is gewoon de eerste die dit aan het doen is.
[1882.04 --> 1884.52]  Dit gebeurt nu, in plaats van dat dat toekomstmuziek is.
[1885.38 --> 1891.20]  Ten tweede, ze genereren plaatjes op basis van tekstprompts in hun meta.ai tool.
[1891.30 --> 1893.68]  En dat is nu beschikbaar, nogmaals, niet in Nederland.
[1894.12 --> 1897.50]  In feeds, in stories en voor profielfoto's op Facebook.
[1897.88 --> 1899.76]  Dus dat is aan het gebeuren.
[1899.76 --> 1905.80]  Ze kijken naar AI-gegenereerde content op basis van interesses en trends.
[1905.88 --> 1907.26]  Die hebben het ook al vaak over gehad.
[1907.36 --> 1913.34]  Dus dat het niet, nu is de curatie van content in Instagram Explorer bijvoorbeeld algoritmisch.
[1913.62 --> 1915.30]  Dus dat is niet op basis van mensen die je volgt.
[1915.40 --> 1917.68]  Dat is op basis van wat dat ding doorheeft wat jouw interesses zijn.
[1918.28 --> 1920.72]  Tot nu toe is die curatie algoritmisch.
[1920.80 --> 1923.86]  Maar is de content zelf wel nog door mensen gemaakt.
[1924.44 --> 1927.88]  En dat is een soort van, dit is een ander ding waar ik de hele tijd van denk.
[1927.88 --> 1936.90]  Wanneer gebeurt dat nou? Wanneer gaan wij als bevolking van de wereld kijken naar content die gegenereerd is door AI in plaats van door mensen?
[1936.92 --> 1938.04]  Speciaal voor onze eigen ogen.
[1938.12 --> 1940.38]  Ja, helemaal persoonlijk gemaakt.
[1940.82 --> 1942.26]  Wanneer gebeurt dat nou?
[1942.50 --> 1943.04]  Het is er.
[1943.26 --> 1948.30]  Meta is nu de eerste die eigenlijk, nou ja, opschuift van een tijd waarin we alleen maar mensen volgden.
[1948.30 --> 1950.92]  En dat je Facebook gebruikte om te zien wat je vrienden aan het doen waren.
[1951.28 --> 1957.46]  Naar een tijd waarin je nieuwsfeed vooral was video's van mensen die je niet volgt, maar van een ding dacht, dit vind je interessant.
[1958.26 --> 1961.28]  Naar nu een tijd waarin het een combinatie is van beide.
[1962.24 --> 1964.88]  Zowel mensen die je volgt als mensen die je niet volgt.
[1965.12 --> 1970.24]  Plus synthetisch gegenereerde content op basis van je interesses.
[1970.24 --> 1973.62]  En dus ook op basis van jou zelf.
[1973.76 --> 1980.14]  Dus hij kan zelfs op basis, dat was een beetje vaag, maar content maken waar jouw gezicht in gebruikt wordt.
[1980.92 --> 1990.16]  Zodat er content gemaakt, ik zie jou moeilijk kijken, maar content gemaakt kan worden voor jou die je dan kan publiceren terwijl je daar nooit echt iets voor gedaan hebt.
[1990.18 --> 1993.36]  Maar vindt Zuckerberg gewone mensen niet gewoon genoeg?
[1994.08 --> 1995.68]  Waarom moet er synthetische content?
[1995.68 --> 1997.36]  Ik snap niet dat er meer waarde daarvan is.
[1997.38 --> 2004.88]  Uiteindelijk, heel koud gezegd, is het gewoon hoeveel uur per dag spenderen mensen op één van de meta-assets of platforms.
[2005.20 --> 2009.10]  Dat is wel in de Excel, zeg maar, ergens die daar staat, zit dat.
[2009.24 --> 2012.68]  Want ja, aandacht, tijd en aandacht resulteert in advertentieinkomsten.
[2013.62 --> 2014.86]  En meta is gewoon een...
[2014.86 --> 2016.28]  Je betaalt nergens voor meta.
[2016.48 --> 2020.38]  Ik heb geen enkel creditcard afgegeven waar meta op staat, dus dat moet ergens anders betaald worden.
[2020.38 --> 2021.52]  Ja, nee, dat snap ik inderdaad.
[2021.52 --> 2026.30]  Maar het gaat mijn voorstellingsvermogen nog te boven dat ik dan naar synthetische content...
[2026.30 --> 2029.30]  Nou, dat is wel vet, want voor de luisteraar ook om daar een gevoel bij te krijgen.
[2029.46 --> 2032.26]  Tijdens Meta Connect halen ze een influencer onstage.
[2032.74 --> 2037.94]  En vervolgens gaan ze met een digital clone of digital twin van die influencer, met hem ook op het podium.
[2038.32 --> 2039.62]  Iets wat ongemakkelijk, moet ik zeggen.
[2039.92 --> 2041.66]  Het was sowieso een beetje vreemd.
[2041.96 --> 2046.58]  Maar gaan ze praten met zijn clone, die dus niet alleen maar zijn stemkloon heeft, maar ook visueel.
[2046.58 --> 2052.42]  En een soort voelt als, wat zou er gebeuren als ik een filmpje op Teams van mezelf opneem, één minuut en dan op Loopset...
[2052.42 --> 2054.44]  zodat ik naar de wc kan en mijn collega's dat niet doorhebben.
[2054.52 --> 2055.76]  Zo voelde hij nog een beetje.
[2056.24 --> 2059.22]  Maar ik zag al wel, en ik houd de research papers ook in de gaten.
[2060.26 --> 2065.50]  Nog twee research papers en daar kan echt wel een vloeiende 60 frames per seconde influencer met jou kletsen...
[2065.50 --> 2067.06]  die eigenlijk gewoon op dat moment aan het surfen is.
[2067.22 --> 2067.38]  Ja.
[2067.38 --> 2076.84]  De AI-slap die Facebook genereert, want dat is hoe ze dit noemen, AI-slap, als in gewoon een meuk die je feed ingegooid wordt, gegenereerd door AI...
[2076.84 --> 2079.62]  is sommige van de populairste content op Facebook op dit moment.
[2079.66 --> 2080.22]  Oh, dat is het al?
[2080.26 --> 2081.12]  Ja, dat is het al.
[2081.22 --> 2082.86]  Oké, nou, ik loop er hooploos achter.
[2082.98 --> 2086.84]  Dat is dan gemaakt door andere platforms waar mensen het erop zetten of bots, weet ik eigenlijk niet.
[2087.08 --> 2088.58]  Nu gaat Facebook het gewoon zelf doen.
[2088.58 --> 2093.38]  Maar ik denk de uitdaging is om dit te doen op een manier dat wij het met z'n allen gaan accepteren.
[2093.38 --> 2098.36]  Want dat technisch dingen kunnen is één ding, dat bewijst ze nu de hele tijd.
[2098.56 --> 2100.26]  Dat begint nu te komen.
[2100.76 --> 2103.80]  Maar hoe gaan ze dit doen op een manier dat wij het ook echt gaan consumeren?
[2104.06 --> 2105.80]  Ja, dat is echt wel een uitdaging.
[2105.98 --> 2108.50]  En ik denk ook niet dat ze dat al helemaal uitgevonden hebben.
[2108.52 --> 2110.48]  Nou, ze lossen in ieder geval één heel groot probleem op.
[2110.56 --> 2115.88]  Is dat al die influencers, die zijn natuurlijk totaal overwerkt omdat ze de hele dag filmpjes moeten schieten van zichzelf.
[2116.16 --> 2118.36]  Nu kunnen ze eindelijk gewoon hun clone het laten doen.
[2119.00 --> 2120.48]  Nou, ik denk wel dat het een deel...
[2120.48 --> 2126.06]  Ja, maar in alle eerlijkheid, ik denk wel dat dat een deel is van wat het gaat betekenen om influencer te zijn.
[2126.46 --> 2133.30]  Is gewoon je volgers kunnen laten praten met die influencer, dat het een interactieve ervaring wordt.
[2133.32 --> 2134.26]  Lijkt me een goede zaak.
[2134.38 --> 2136.74]  Dan kunnen zij ook eens iets nuttigs gaan doen waar we er wel iets aan hebben.
[2136.74 --> 2144.04]  Ik denk Milou, en dan nog als een soort midden, dat ik ook me kan voorstellen dat een influencer net wakker uit bed met een soort van haar door...
[2144.04 --> 2148.40]  Zoals ik hier ongeveer bij zit, zeg maar, gewoon een video opneemt en dan drukt op enhance.
[2148.70 --> 2151.12]  En dan neemt ze een digital clone, een soort speech to speech.
[2151.66 --> 2155.10]  Dus wij zitten nu altijd te denken aan die influencer is helemaal niet meer betrokken.
[2155.10 --> 2158.78]  Ik denk dat die influencer gewoon de raw data provide voor dat moment.
[2158.96 --> 2161.40]  En dan druk je op dat toverstafje en dan...
[2161.40 --> 2164.30]  Zo gaat alle content zijn, denk ik.
[2164.30 --> 2164.46]  Ja.
[2164.88 --> 2166.78]  Zo gaat ook nieuws zo zijn.
[2166.88 --> 2168.20]  Zo gaat entertainment zijn.
[2168.24 --> 2169.48]  AI, make-up, CGI.
[2169.72 --> 2170.10]  Precies.
[2170.54 --> 2171.08]  Oké.
[2171.70 --> 2172.12]  Nog even.
[2172.30 --> 2175.24]  We hebben het allervetste van deze hele presentatie nog niet gehad.
[2175.24 --> 2177.80]  En dat is de Orion bril.
[2178.22 --> 2178.96]  Vind jij die vet?
[2179.40 --> 2185.16]  Nou, volgens mij, ik denk dat er een latente behoefte achter zit om van de hele wereld een super nerd te maken.
[2185.24 --> 2189.02]  Want als iedereen deze bril op gaat zetten, het is een heel dik zwart montuur.
[2189.02 --> 2189.62]  Hij is wel dik, hè?
[2189.76 --> 2190.58]  Het is een prototype.
[2190.58 --> 2191.18]  Het is wel dik.
[2191.28 --> 2192.14]  Het is een prototype.
[2192.20 --> 2193.52]  Het is een prototype, inderdaad.
[2193.62 --> 2196.22]  Het is wel echt een hele dikke bril, letterlijk.
[2196.82 --> 2199.20]  Dikke zwarte bril, dus iedereen lijkt daardoor op een nerd.
[2199.36 --> 2204.86]  Maar het is weer een nieuwe bril en misschien wel de meest indrukwekkende tot nu toe, omdat het gewoon...
[2205.24 --> 2207.10]  Ja, het weegt nog geen 100 gram.
[2207.80 --> 2209.12]  En het kan heel veel.
[2209.52 --> 2210.28]  Dus je hebt...
[2210.28 --> 2211.98]  Het lijkt glas, maar het is geen glas.
[2212.06 --> 2214.54]  Het is silicon carbide in plaats van glas.
[2214.64 --> 2217.86]  Daar kijk je doorheen en dan zie je nog steeds gewoon de echte wereld.
[2217.86 --> 2220.38]  En mensen kunnen ook nog aan jou kijken, dus ze zien ook gewoon jouw ogen.
[2221.20 --> 2226.00]  Maar jij kan jouw werkelijkheid verhogen, is augmenten.
[2226.46 --> 2227.20]  Hoe zeg je dat?
[2227.20 --> 2227.30]  Hoe zeg je dat?
[2227.30 --> 2227.60]  Hoe zeg je dat?
[2227.66 --> 2227.78]  Ja.
[2227.84 --> 2229.84]  Dus een laag over de werkelijkheid heen leggen.
[2229.90 --> 2230.02]  Ja.
[2230.22 --> 2232.76]  Dus je gaat hologrammen zien van de dingen die je wil doen.
[2232.86 --> 2234.90]  Dus je kan browsen over het internet.
[2235.24 --> 2236.90]  Je kan een potje pong spelen.
[2237.02 --> 2238.40]  Ik heb een clip gezien van The Verge.
[2238.68 --> 2240.52]  Daar staat hij een potje pong te spelen met...
[2240.52 --> 2242.48]  Dus gewoon door die handjes te bewegen in de lucht.
[2242.88 --> 2246.54]  Kan hij pingpongen met Mark Zuckerberg virtueel.
[2247.82 --> 2249.38]  Je kan facetimen.
[2249.52 --> 2250.74]  Dat zal niet zo heet erbij met hem.
[2250.80 --> 2254.08]  Maar je kan wel videobellen met iemand en die zie je dan gewoon voor je.
[2254.14 --> 2255.10]  Maar het zijn hologrammen.
[2255.36 --> 2257.52]  Dus je kan er nog steeds ook wel doorheen kijken een beetje.
[2258.10 --> 2259.76]  Je ziet gewoon de werkelijkheid nog.
[2259.86 --> 2261.22]  Er zitten ook zeven camera's in.
[2261.22 --> 2262.32]  Dus je kan ook...
[2262.32 --> 2264.94]  Stel dat ik hier links in mijn blikveld...
[2264.94 --> 2266.70]  Want ik heb een 70 graden blikveld.
[2267.00 --> 2267.44]  Als ik hier links...
[2267.44 --> 2268.14]  Bij mensen bedoel je?
[2268.22 --> 2268.68]  Of dat ding?
[2269.40 --> 2270.20]  Je kan door die bril.
[2270.26 --> 2271.08]  Heb je 70 graden blik.
[2271.08 --> 2271.66]  Ja, precies.
[2271.74 --> 2273.16]  Waar die dingen in kan projecteren.
[2273.22 --> 2273.36]  Ja.
[2273.58 --> 2277.88]  En als je dus ook op de zij kijkt en je bent daar bijvoorbeeld in je browser iets aan
[2277.88 --> 2278.10]  doen.
[2278.34 --> 2280.70]  Dan kun je wegkijken.
[2280.88 --> 2282.64]  En dan zul je denken van nou misschien verdwijnt dat dan.
[2282.82 --> 2284.42]  Maar hij onthoudt dat daar nog wat zit.
[2284.44 --> 2285.38]  Ja, zoals de Vision Pro.
[2285.62 --> 2286.30]  Je hoofd terug.
[2286.38 --> 2287.22]  Oh, zoals de Vision Pro.
[2287.30 --> 2288.14]  Maar ik denk dat...
[2288.14 --> 2288.74]  Zeg maar...
[2288.74 --> 2293.28]  Mijn reactie is eigenlijk technisch indrukwekkend.
[2293.88 --> 2295.00]  Maatschappelijk dystopisch.
[2296.38 --> 2300.74]  Omdat ik dan weer in een NS-terrein zit vol met mensen met hele dikke brillen op die elkaar
[2300.74 --> 2301.56]  niet meer aankijken.
[2301.56 --> 2305.48]  Maar de droom van Magic Leap en al dat soort partijen.
[2305.70 --> 2305.76]  Ja.
[2306.84 --> 2309.18]  Meta die trommelt maar door ofzo op dat ding.
[2309.50 --> 2310.84]  Hun lab zit niet stil.
[2311.56 --> 2312.16]  Ja, wat ik zei.
[2312.24 --> 2316.80]  Ik vind het knap wat ze gebouwd hebben in zo'n klein frame en wat daarmee gebeurt.
[2316.92 --> 2321.38]  En je kan je voorstellen als een arts zoiets opzet tijdens een operatie waardoor hij zij
[2321.38 --> 2324.36]  een betere operatie uit kan voeren, hebben we ineens een verrijking.
[2324.70 --> 2326.38]  Je noemde net al augmented verrijking.
[2326.48 --> 2328.56]  Dat ik al dacht, nou weten we nog niet of dat verrijkend is.
[2328.56 --> 2333.38]  Maar als het iemand is die drie TikTok feeds vol synthetische content tegelijk aan het
[2333.38 --> 2336.12]  consumeren is in een trein, dan word ik weer zenuwachtig.
[2336.22 --> 2341.22]  Dus ik kijk ernaar met een soort gadget fascinatie.
[2341.86 --> 2345.18]  Maar er zit ook een zorg in dat ik denk, het is wel meta hè.
[2345.66 --> 2347.12]  Dat is niet een of andere non-profit.
[2347.64 --> 2348.22]  Nee, precies.
[2348.36 --> 2352.56]  Ze gaan het gebruiken om Instagram Reels op je gezicht te duwen in plaats van...
[2352.56 --> 2354.08]  Echt agressief duwen zeg maar.
[2354.42 --> 2355.12]  En Mark.
[2355.12 --> 2359.72]  Je hoeft je laptop misschien niet mee te nemen en je kan dus in de trein gewoon.
[2359.72 --> 2363.22]  Nou, ik denk dat je straks daar, je zei net iets cruciaals.
[2363.78 --> 2367.60]  Namelijk meta zit in alle apps die we gebruiken.
[2367.92 --> 2372.54]  En dat is op dat moment denk ik de strategische zwakte van meta.
[2372.72 --> 2373.98]  Is dat zij in de apps zitten.
[2374.50 --> 2376.60]  En ze zitten in de onderliggende AI modellen.
[2376.92 --> 2378.98]  Maar ze zitten niet op het OS niveau.
[2378.98 --> 2383.98]  En Zuckerberg zei zelf in een podcast die ik vandaag luisterde...
[2383.98 --> 2389.94]  ...dat Apple en Google specifiek grote strategische voordelen hebben...
[2389.94 --> 2394.08]  ...omdat zij een operating systeem hebben waarmee ze apps aan elkaar kunnen koppelen.
[2394.22 --> 2395.78]  Bijvoorbeeld middels AI.
[2395.78 --> 2398.86]  En dit is dé kans voor meta.
[2399.82 --> 2400.82]  Strategisch gezien.
[2401.06 --> 2403.02]  Om hun eigen OS te ontwikkelen.
[2403.12 --> 2404.76]  En dan middels deze bril.
[2405.30 --> 2410.12]  En ik vind het best indrukwekkend om te zien wat nou de hele...
[2410.12 --> 2412.18]  ...verlopende tijd is hoe dit nou gaat.
[2412.28 --> 2414.14]  Want we kregen dus chat GPT.
[2414.74 --> 2416.16]  Toen raakte iedereen in de stress.
[2416.28 --> 2417.14]  Toen besloot meta...
[2417.86 --> 2421.34]  Oké, we gaan nu ook massaal investeren in taalmodellen.
[2421.46 --> 2422.96]  Maar we gaan het open source maken.
[2423.22 --> 2426.76]  Grote, groot onderscheidend vermogen daarmee.
[2427.26 --> 2430.48]  Toen zijn ze die meta Ray-Ban brillen gaan maken.
[2431.22 --> 2434.38]  De zonnebril met de camera's ernaast.
[2434.44 --> 2436.06]  Die ontzettend goed verkopen.
[2436.56 --> 2438.76]  Wat helemaal niet de verwachting was van meta.
[2438.76 --> 2440.22]  De dingen is ook overal uitverkocht.
[2440.28 --> 2441.24]  Je kan nergens kopen.
[2442.68 --> 2444.02]  Zuckerberg zelf zegt dat de vraag...
[2444.02 --> 2446.58]  ...vijf keer groter is dan dat zij verwacht hadden.
[2447.52 --> 2451.32]  Nou, dat is een samenwerking met de eigenaar van Ray-Ban.
[2451.38 --> 2451.98]  Hoe heet die club?
[2454.50 --> 2454.92]  Escilor Luxottica.
[2455.28 --> 2455.90]  Nou, prima.
[2456.80 --> 2459.62]  Die een intense samenwerking aan het worden is.
[2459.72 --> 2461.30]  Waar dus nu ook die nieuwe bril...
[2461.30 --> 2463.08]  ...ondanks dat die inderdaad te dik uitziet.
[2463.20 --> 2464.12]  En nog wat zwaar is.
[2464.18 --> 2464.86]  100 gram.
[2465.32 --> 2467.26]  En met een batterijduur van maar twee uur.
[2467.26 --> 2469.70]  En je moet een soort van armband om je hand dragen.
[2469.78 --> 2470.88]  Om dat ding te laten werken.
[2470.98 --> 2473.52]  Want die armband registreert je armbewegingen.
[2473.52 --> 2476.76]  Zodat je niet ook nog camera's aan de onderkant van de bril moet hebben.
[2476.92 --> 2478.06]  Want dan wordt die bril weer dikker.
[2478.28 --> 2479.00]  Maar je kan hem ook bedienen.
[2479.16 --> 2481.38]  Dus als je je wijsvinger op je duim doet.
[2481.78 --> 2482.84]  Dan klik je.
[2483.38 --> 2484.66]  En wijsvinger op je middel...
[2484.66 --> 2486.08]  Nee, middelvinger op je duim.
[2486.58 --> 2487.42]  Dat is naar het menu.
[2487.72 --> 2490.04]  En als je zo je duim over je hand beweegt.
[2490.14 --> 2490.54]  Over je vuistje.
[2490.54 --> 2491.40]  Ja, over je vuist.
[2491.58 --> 2492.02]  Over je vuistje.
[2492.02 --> 2493.30]  Een dichtgeknepen vuist.
[2493.40 --> 2495.14]  En dan met je duim daar overheen beweegt.
[2495.14 --> 2496.52]  Ik vind het geen lekker gevoel.
[2496.86 --> 2497.98]  Nou, dat gaan we heel normaal vinden.
[2498.32 --> 2499.20]  Maar dat is scrollen.
[2499.40 --> 2504.46]  En dus die pijplijn is nu doorontwikkeld naar AR-brillen.
[2504.70 --> 2508.34]  Die nu nog 10.000 dollar kosten om te ontwikkelen.
[2508.54 --> 2510.66]  Dus om te kopen bedoel ik per stuk.
[2510.76 --> 2512.42]  De productiekost zijn 10.000 dollar per stuk.
[2512.50 --> 2513.52]  Dat is natuurlijk veel te veel.
[2513.94 --> 2514.98]  Dus het gaat kleiner worden.
[2515.08 --> 2516.88]  De resolutie van die dingen willen ze beter maken.
[2516.96 --> 2518.12]  Voordat ze het gaat uitbrengen.
[2518.12 --> 2519.42]  En het ding is nog niet uit.
[2519.52 --> 2521.92]  Dus we moeten allemaal maar hopen dat het ook echt gaat gebeuren.
[2522.10 --> 2524.62]  Want één ding is dingen beloven in een mooie presentatie.
[2524.70 --> 2526.60]  Maar het tweede ding is het ook in de hand van consumenten krijgen.
[2526.74 --> 2527.96]  Dat allemaal gezegd hebbende.
[2528.70 --> 2529.26]  Het is heel.
[2529.40 --> 2530.68]  Ik vind het heel bijzonder om te zien.
[2530.78 --> 2534.88]  Hoe Meta zich in een soort van nieuwe categorie aan het manoeuvreren is.
[2534.96 --> 2537.06]  Die eigenlijk naast de smartphone.
[2537.50 --> 2538.62]  Weet je met een beetje geluk.
[2538.98 --> 2539.96]  Kan gaan staan.
[2540.48 --> 2543.00]  Waarbij niet het doel is om de smartphone te elimineren.
[2543.18 --> 2543.78]  Zegt Zuckerberg.
[2544.36 --> 2547.62]  Zoals de smartphone ook niet de desktop computer heeft geëlimineerd.
[2547.62 --> 2549.86]  Tussen die twee apparaten naast elkaar gaan gebruiken.
[2550.24 --> 2556.38]  Verwacht hij dat je bril dus in feite naast je smartphone komt te staan.
[2556.58 --> 2561.28]  En de manier waarop we dat dan gaan doen is dat die bril dus zoals de Ray-Ban bril nu ook al.
[2561.38 --> 2563.48]  Dat je dingen kan vragen over je omgeving.
[2563.60 --> 2567.80]  Of kan zeggen wat staat er op deze menukaart die Spaans is.
[2568.00 --> 2569.84]  En vertel me nu wat het kost in euro's.
[2570.26 --> 2572.60]  Dat je ook kan kijken naar iets.
[2572.68 --> 2574.44]  En dan kan zeggen bel dit nummer voor me.
[2574.44 --> 2576.88]  Of zie je wat ik voor me heb.
[2577.02 --> 2578.54]  Help me herinneren hier aan.
[2578.72 --> 2580.94]  En dat dat dan weer samen gaat werken met je telefoon.
[2581.30 --> 2583.26]  Dat er op de achtergrond dingen voor je gedaan kunnen worden.
[2583.36 --> 2585.92]  En waarbij het niet uit je zak hoeven halen van een telefoon.
[2586.32 --> 2589.60]  Ja toch een fundamentele nieuwe stap is.
[2589.68 --> 2591.94]  Waarop we technologie in ons leven gaan gebruiken.
[2592.22 --> 2594.40]  Die weet ik uit ervaring met die bril.
[2594.60 --> 2596.22]  Gelijk heel normaal voelt.
[2596.22 --> 2598.76]  Het idee dat je camera's op je hoofd draagt.
[2599.42 --> 2602.78]  Leek mij van tevoren een bizarre ervaring waar ik aan moest wennen.
[2602.98 --> 2603.66]  Was niet zo.
[2603.80 --> 2605.34]  Binnen no time ben je eraan gewend.
[2605.40 --> 2607.62]  Dat je foto's kan maken met je fucking ogen.
[2608.60 --> 2608.98]  En ja.
[2609.14 --> 2609.88]  Door te knipperen.
[2610.08 --> 2612.24]  Ja door heel panisch te knipperen.
[2612.24 --> 2615.76]  En ik denk dus dat ze daar echt beet hebben.
[2616.12 --> 2622.60]  Met wat betreft de kans dat dit normaal gaat worden voor mensen.
[2622.72 --> 2624.94]  In tegenstelling tot die skibril die Apple heeft gemaakt.
[2625.04 --> 2626.28]  Wat nooit normaal gaat worden.
[2626.68 --> 2627.96]  Nee maar daar kun je ook niet doorheen kijken.
[2628.76 --> 2630.50]  Ja maar hoe shiny de demo.
[2630.56 --> 2631.20]  Ja dat kan wel.
[2631.32 --> 2632.08]  Via camera's.
[2632.10 --> 2633.44]  Maar mensen kunnen jou niet aankijken.
[2633.46 --> 2633.80]  Jawel.
[2633.94 --> 2636.38]  Via nep ogen aan de voorkant van die bril.
[2636.52 --> 2637.82]  Dus ja dat kan allemaal.
[2637.82 --> 2641.58]  Maar zolang dat zo klunky en zo groot is.
[2641.88 --> 2644.54]  Ja er is geen scenario waarin dit normaal gaat worden.
[2644.66 --> 2646.52]  Dat je op je bank gaat zitten met je familie.
[2646.62 --> 2647.88]  En dan allemaal zo'n bril op.
[2647.96 --> 2649.22]  Terwijl dat ding van Facebook.
[2649.92 --> 2650.96]  Ja nu zie je.
[2651.12 --> 2652.98]  Ik heb regelmatig die bril op.
[2653.04 --> 2653.60]  Die zonnebril.
[2653.70 --> 2654.46]  Dan ga ik naar binnen.
[2654.60 --> 2655.02]  Dan denk ik.
[2655.38 --> 2657.26]  Ja je gaat niet met een zonnebril binnen lopen.
[2657.54 --> 2658.44]  Maar dan mis ik hem.
[2659.24 --> 2660.48]  Want ik denk ik wil binnen.
[2660.62 --> 2662.62]  Ook dat ik wil die camera's ook binnen.
[2662.72 --> 2664.48]  Hoezo heb ik alleen buitencamera's in mijn ogen.
[2664.90 --> 2666.38]  Dus nou dat zegt iets denk ik.
[2666.38 --> 2668.46]  Over de ubiquitousness.
[2668.66 --> 2669.24]  Hoe zeg je dat?
[2669.54 --> 2670.42]  Van dit soort bril.
[2670.52 --> 2671.66]  Ik zie jij heel geamuseerd kijken.
[2671.84 --> 2673.36]  Nee ik vind het een interessante rant.
[2673.50 --> 2674.68]  Nee ik zat na te denken dat.
[2674.98 --> 2676.78]  Ik denk dat er niet onderschat moet worden.
[2676.90 --> 2678.60]  Dat die synthetische influencers.
[2678.60 --> 2679.80]  Waar we het eerder over hadden.
[2680.40 --> 2682.98]  Die kunnen natuurlijk op het moment dat jij zo'n augmented laag hebt.
[2683.08 --> 2684.10]  In jouw woonkamer zijn.
[2684.84 --> 2687.18]  Dus je kunt zeg maar niet bestaande mensen.
[2687.48 --> 2688.80]  Als soort hologram avatars.
[2688.98 --> 2690.34]  In je woonkamer projeteren.
[2690.66 --> 2692.92]  En ik denk dat daarbij die twee werelden van.
[2693.10 --> 2694.84]  Aan de ene kant augmented mixed reality.
[2694.84 --> 2700.00]  En aan de andere kant het creëren van steeds realistische synthetische entiteiten.
[2700.20 --> 2701.96]  Heel erg samen kan gaan komen.
[2702.08 --> 2703.44]  En dat Meta dat ook wel begrijpt.
[2703.62 --> 2706.24]  Want waarom zou je niet met elkaar naar een concert gaan.
[2706.32 --> 2708.18]  Van een van je artiesten die eigenlijk niet bestaat.
[2709.18 --> 2710.20]  Ik bedoel waarom zou je dat niet doen.
[2710.44 --> 2711.44]  Daar kan ik uur over praten.
[2711.56 --> 2712.68]  Waarom je dat niet zou moeten doen denk ik.
[2712.76 --> 2713.96]  Maar al met al.
[2714.86 --> 2716.22]  Het is interessant om te zien.
[2716.22 --> 2718.48]  Dat de grote gok.
[2718.66 --> 2720.64]  Of de grote debat eigenlijk.
[2720.74 --> 2721.66]  Die Meta gemaakt heeft.
[2721.74 --> 2722.46]  Jaren geleden al.
[2722.58 --> 2724.40]  Op het hele augmented mixed reality verhaal.
[2724.84 --> 2727.12]  Wat een soort toch een beetje tam gebleven is.
[2727.82 --> 2728.14]  Dat dat dan.
[2728.40 --> 2730.24]  En daarnaast zit dan nu die AI bed.
[2730.40 --> 2731.76]  Omdat ze ook gewoon mee willen komen.
[2732.28 --> 2734.38]  Dat die twee voor hun best wel samen gaan komen.
[2734.38 --> 2736.12]  Ja die combinatie is zo krachtig.
[2736.22 --> 2737.72]  Dat maakt dat opeens denk ik.
[2737.98 --> 2738.84]  Voorstelbaar gaat worden.
[2738.98 --> 2741.70]  Dat mensen echt zo'n AR bril willen gaan dragen.
[2741.90 --> 2743.94]  Door die AI assistent denk ik.
[2744.04 --> 2744.72]  Dat is wat het.
[2744.94 --> 2745.98]  Maar ik ben wel benieuwd of.
[2746.50 --> 2747.40]  Want ik vind zelf.
[2748.26 --> 2749.64]  Dat heb ik met al die Oculus.
[2750.10 --> 2751.38]  Meta Oculus producten gehad.
[2751.48 --> 2753.40]  Dat je daar een account voor moest hebben.
[2753.50 --> 2754.72]  Dat is op een gegeven moment volgens mij weggehaald.
[2754.84 --> 2757.06]  Maar ik vind het feit dat als je een bestuur.
[2757.22 --> 2758.28]  Je besturingssysteem.
[2758.38 --> 2760.32]  Dus die laag die zoeken werken eigenlijk wel hebben ook.
[2760.88 --> 2763.00]  In dit geval voor het soort augmented reality OS.
[2763.16 --> 2764.36]  Net als Windows en Mac OS.
[2764.38 --> 2765.04]  En iOS.
[2765.68 --> 2766.30]  En Android.
[2767.44 --> 2769.84]  Dat je daar dan verplicht een meta account voor moet hebben.
[2769.94 --> 2771.14]  Om er op in te mogen of zo.
[2771.20 --> 2771.66]  Dat soort dingen.
[2771.82 --> 2773.24]  Daar zit wel een grote zorg bij mij.
[2773.34 --> 2773.84]  Dat ik denk ja.
[2774.34 --> 2775.98]  Dat is zo'n fundamenteel iets.
[2776.10 --> 2777.38]  Zeg maar als jij je realiteit laat augmenten.
[2778.14 --> 2779.74]  En je eigenlijk je ogen laat beamen.
[2780.18 --> 2783.48]  Moet je dat laten beamen door een commerciële advertentie.
[2783.48 --> 2785.72]  Het is wel black mirror achtig.
[2785.84 --> 2786.12]  Zeker.
[2786.56 --> 2790.12]  Misschien kunnen een paar van die safety mensen van OpenAI naar meta gaan.
[2790.26 --> 2791.12]  Ja gaat niet gebeuren.
[2791.28 --> 2791.38]  Nee.
[2792.90 --> 2793.22]  Straks.
[2793.22 --> 2796.50]  Gaan we het hebben over een nieuwe functie van ChatGPT.
[2796.60 --> 2797.94]  Waarmee je live met het ding kan praten.
[2798.06 --> 2798.84]  In het Nederlands.
[2798.90 --> 2799.84]  We gaan dat demonstreren.
[2799.96 --> 2801.66]  Maar eerst een boodschap van onze sponsor.
[2802.20 --> 2805.20]  Ja als ondernemer wil je er zeker van zijn dat je winst maakt op je projecten.
[2805.32 --> 2808.50]  Maar hoe houd je het overzicht zonder te verdrinken in administratie.
[2808.50 --> 2810.08]  En complexe spreadsheets.
[2810.22 --> 2811.88]  Dat kent iedereen ondernemer wel.
[2812.02 --> 2813.74]  Gelukkig biedt Team Leader Focus.
[2814.06 --> 2815.62]  De bedrijfssoftware voor het MKB.
[2816.08 --> 2816.58]  De oplossing.
[2816.90 --> 2819.00]  Met Team Leader Focus heb je al je inkomsten.
[2819.26 --> 2820.86]  Uren, taken en deadlines.
[2821.02 --> 2822.08]  Overzichtelijk op één plek.
[2822.34 --> 2825.34]  Je ziet direct wanneer een project uit de pas dreigt te lopen.
[2825.74 --> 2826.92]  Zodat je tijdig kan bijsturen.
[2827.06 --> 2827.62]  En het mooie is.
[2827.96 --> 2829.18]  Wanneer een project is afgerond.
[2829.44 --> 2832.02]  Maak je met één klik een factuur aan om naar je klant te sturen.
[2832.32 --> 2834.40]  En zo blijf je in control van je projecten.
[2834.70 --> 2836.82]  En de manier dat er geld op je rekening komt.
[2836.82 --> 2837.92]  Dat is toch ook wel belangrijk als ondernemer.
[2838.46 --> 2840.80]  Wil je ook met gerust hart focussen op je projecten.
[2841.04 --> 2843.06]  En erop vertrouwen dat je winstgevend blijft.
[2843.34 --> 2845.20]  Probeer dan nu Team Leader Focus.
[2845.46 --> 2847.28]  Ga naar focusopjebedrijf.nl.
[2847.68 --> 2850.78]  En ervaar het zelf met een gratis proefperiode van 14 dagen.
[2851.18 --> 2854.90]  Dus om dat proberen ga je naar focusopjebedrijf.nl.
[2855.72 --> 2856.06]  Goed.
[2856.38 --> 2858.12]  Ja, winst maken was nog nooit zo makkelijk.
[2858.24 --> 2858.64]  Begrijp ik.
[2859.10 --> 2861.46]  Dat heb je juist gehoord in deze advertentie.
[2861.90 --> 2863.42]  Goed, je zei het al even.
[2863.42 --> 2866.58]  We gaan de nieuwe stemfunctie van OpenAI uitproberen.
[2866.58 --> 2867.90]  Ja, eindelijk.
[2868.08 --> 2871.86]  Want luisteraars herinneren zich misschien nog dat we het al eerder over de stemfunctie
[2871.86 --> 2872.70]  van OpenAI hebben gehad.
[2872.76 --> 2873.48]  Dat was in juli.
[2874.18 --> 2875.46]  Dat heette toen Sky.
[2875.60 --> 2878.94]  En dat leek verdacht veel op Scarlett Johansson.
[2880.34 --> 2881.16]  Grote ophef.
[2881.20 --> 2882.04]  Ja, grote ophef.
[2882.22 --> 2884.14]  Maar nu maanden later is het eindelijk echt gelanceerd.
[2884.22 --> 2885.22]  Dus kan iedereen het gebruiken.
[2885.36 --> 2886.54]  Behalve natuurlijk Europa.
[2886.74 --> 2888.08]  Dat hoef ik niet meer te zeggen, volgens mij.
[2888.08 --> 2890.46]  Nou, ik wil ik nog wel even aan het haakje hangen.
[2890.58 --> 2896.42]  Want ik voel die soort van tech bro negativiteit rondom de AI en de EU.
[2896.96 --> 2899.62]  Ik ben zelf erg blij met de AI Act.
[2899.76 --> 2906.46]  En ik denk dat je kunt zeggen, er is een soort van interessante dubbele spanning merk ik.
[2906.98 --> 2912.12]  Of dubbele gedachte van, we vinden eigenlijk veel van ons dat de overheid beter op de hoogte
[2912.12 --> 2914.52]  zou moeten zijn van wat er digitaal allemaal speelt.
[2914.52 --> 2916.64]  We maken ons op een bepaalde manier ook zorgen daarom.
[2916.88 --> 2919.16]  Zitten er wel mensen daar die snappen wat hier allemaal gebeurt.
[2919.72 --> 2922.52]  En als er dan een groep expert zit en die gaat wetten schrijven.
[2922.94 --> 2925.10]  En die gaat dus bedrijven accountable houden.
[2925.56 --> 2927.44]  Dan zijn we weer boos van, waarom hou je tegen?
[2927.58 --> 2931.24]  We willen onze speelgoed gisteren niet moeten wachten op onze nieuwe speeltjes.
[2931.92 --> 2935.76]  Ik denk het feit dat nu die voice assistant niet beschikbaar is in de EU.
[2936.04 --> 2938.94]  Lama 3.2 niet commercieel ingezet mag worden in de EU.
[2939.46 --> 2942.70]  Daar kan je van zeggen, jeetje wat zitten die bureaucraten toch weer moeilijk te doen.
[2942.70 --> 2944.38]  Jullie maken innovatie kapot.
[2944.68 --> 2951.22]  Terwijl ik denk, volgens mij is een kind of een tiener van veertien die de hele dag praat met een chatbot als een vriend.
[2951.76 --> 2953.80]  Misschien wel goed dat we weten waar die data naartoe gaat.
[2953.90 --> 2955.56]  Wat er voor analyse op plaatsgevonden wordt.
[2955.66 --> 2956.76]  Waar het opgeslagen wordt.
[2957.08 --> 2959.52]  En dat het wat langer duurt als je dat als bedrijf wil uitrollen.
[2959.60 --> 2961.96]  Omdat de EU zegt, laat maar zien waar die data staat.
[2962.14 --> 2963.64]  Kap met zuur zijn zeg je eigenlijk.
[2963.76 --> 2964.56]  Kap met zuur zijn.
[2964.56 --> 2970.84]  Omdat ik persoonlijk denk, kijk op het moment dat overheden medicijnen gaan tegenhouden of andere soorten hulpmiddelen.
[2970.84 --> 2974.04]  Dan word ik vaak wat bozer.
[2974.18 --> 2975.80]  Ik denk nog steeds dat je daar heel erg kritisch op moet zijn.
[2975.94 --> 2978.40]  Dat het niet zomaar troep het land in komt.
[2978.98 --> 2982.30]  Maar dingen die mensen het overduidelijk kunnen helpen, moet je niet tegenhouden.
[2982.46 --> 2984.74]  Dat kan je van AI Voice Assistance ook zeggen.
[2984.74 --> 2986.88]  Als je iemand bent in het autistisch spectrum.
[2987.30 --> 2990.06]  En je kan zo'n bot gebruiken om eigenlijk te kunnen praten met je beste vrienden.
[2990.14 --> 2992.38]  Dan moeten we dat misschien niet tegen willen houden zomaar.
[2993.00 --> 2996.08]  Maar wat hier nu gebeurt is, zoals dat is mijn lezing.
[2996.52 --> 3000.08]  Dat de EU heel erg streng is op welke data verzamel je.
[3000.64 --> 3002.08]  Hoe heb je die modellen getraind?
[3002.50 --> 3004.58]  Waar komen al die gezichten vandaan die in die modellen zitten?
[3004.92 --> 3006.52]  Wat waren de video's waar je op hebt getraind?
[3006.84 --> 3007.82]  Mogen we dat even bekijken?
[3008.16 --> 3009.88]  En dan zal het inderdaad zes maanden duren.
[3009.98 --> 3011.60]  Misschien wel een jaar voordat wij het hebben.
[3011.78 --> 3012.50]  Maar we hebben het toch.
[3012.50 --> 3013.10]  Punt gemaakt.
[3013.10 --> 3013.98]  Ja, ik kan het zeggen.
[3014.14 --> 3015.56]  Jij kan het allemaal wel zeggen.
[3015.70 --> 3016.74]  Maar jij hebt een workaround.
[3016.98 --> 3018.22]  Jij kan niet opnieuw.
[3018.38 --> 3018.94]  We hebben een top.
[3018.94 --> 3019.58]  Lekker makkelijk.
[3020.36 --> 3022.64]  De truc, dames en heren, is een VPN.
[3023.22 --> 3024.46]  Installeren op uw telefoon.
[3024.86 --> 3027.00]  U installeert een VPN en zet deze.
[3027.22 --> 3027.98]  Wietsen doet Taiwan.
[3028.22 --> 3029.28]  Dat vind ik heel exotisch.
[3029.40 --> 3030.74]  Dat was de enige die werkte voor mij.
[3030.84 --> 3031.98]  Over mij werkt New York.
[3032.16 --> 3033.32]  Waarom heb je allemaal uitgeprobeerd dan?
[3033.78 --> 3035.46]  Wietsen is de hele wereld overgegeven.
[3035.64 --> 3038.10]  Ik heb tunnelbeer met zo'n beertje die uit buisjes kruipt.
[3038.20 --> 3039.26]  Die is overal uitgekropen.
[3039.26 --> 3041.60]  Maar pas toen hij in Taiwan naar boven kroop, toen deed hij.
[3041.60 --> 3043.46]  Ik vind NoordVPN erg prettig.
[3043.56 --> 3044.74]  Dat is ook een vriend van de show.
[3044.86 --> 3046.32]  Dus laten we die vooral nog even noemen.
[3046.48 --> 3050.36]  Maar je kunt dus een aantal landen doen die niet zijn.
[3050.54 --> 3054.38]  De EU, WK, Zwitserland, IJsland, Noorwegen en Liechtenstein.
[3054.56 --> 3056.32]  Ik benadruk die laatst nog even.
[3057.16 --> 3058.84]  Ik weet niet of ze daar verkeerd hebben gedaan.
[3059.00 --> 3059.92]  Maar dan mag het ook niet.
[3060.78 --> 3061.46]  Hoe dan ook.
[3061.46 --> 3065.86]  Wij zijn Rest of World, zoals dat tegenwoordig heet.
[3066.52 --> 3070.54]  En dan kun je dus een VPN aanzetten om hem met New York te verbinden.
[3070.80 --> 3073.08]  Het kan zijn dat je die app een paar keer opnieuw moet opstarten.
[3073.32 --> 3074.60]  Maar dan uiteindelijk gaat het werken.
[3075.10 --> 3078.40]  En dan krijg je dus de Scarlett Johansson.
[3078.54 --> 3081.48]  Maar dan in een soort van dwangbuisje, zou ik zeggen.
[3081.60 --> 3083.26]  Daar hebben ze de afgelopen maanden aan gewerkt.
[3083.28 --> 3084.32]  Hij is geredteamed.
[3084.56 --> 3088.64]  Dus dat houdt in dat er een team is die alles gaat proberen om zo'n ding kapot te maken.
[3088.64 --> 3091.12]  Nou en veel mag niet meer hoor, Wietse.
[3091.22 --> 3092.62]  Ik heb er een beetje mee gespeeld.
[3092.76 --> 3093.34]  Mag niet meer zingen.
[3094.34 --> 3094.70]  Bijvoorbeeld.
[3094.80 --> 3095.18]  Wat?
[3095.48 --> 3096.10]  Ik snap het niet.
[3097.18 --> 3098.90]  Ze lijkt niet meer op Scarlett Johansson, toch?
[3098.90 --> 3100.56]  Nee, dat sowieso niet meer.
[3100.68 --> 3101.50]  Dat is klaar.
[3101.88 --> 3103.00]  Ook niet in een dwangbuis?
[3103.26 --> 3104.54]  Nou, de dwangbuis bedoel ik.
[3104.58 --> 3106.16]  Er zijn heel veel regeltjes bijgekomen.
[3106.42 --> 3112.50]  Dus ze lanceerden dit al best wel lang geleden in AI-termen.
[3113.20 --> 3115.30]  Die tijd die daar overheen is gegaan.
[3115.30 --> 3118.30]  Sinds die software eigenlijk gewoon productie klaar was.
[3118.64 --> 3120.12]  Daarom demonstreerden ze het.
[3120.32 --> 3124.46]  Was er zoveel gedoe dat ze zeiden we drukken even op pauze.
[3124.56 --> 3127.20]  En die pauzeknop is dus tot nu ingedrukt geweest.
[3127.54 --> 3129.68]  Nu wordt die uitgerold voor betalende abonnees.
[3129.68 --> 3131.22]  Ik denk Milou, voor jouw beeld.
[3131.40 --> 3135.66]  Kijk, je kan natuurlijk aan zo'n taalmodel of in dit geval een besprekend taalmodel vragen.
[3136.30 --> 3138.14]  Joh, kan je een scheet nadoen of zo?
[3138.42 --> 3139.78]  Dat is wat kinderen waarschijnlijk gaan doen.
[3139.92 --> 3141.06]  Kan je scheetjes laten of zo?
[3141.06 --> 3143.32]  Ik ga er vanuit dat dat nog toegestaan is.
[3143.72 --> 3148.20]  Maar je fantasie kan niet zo groot op wat je fake een telefoongesprek.
[3148.26 --> 3149.46]  Alsof ik nu bel met die en die.
[3149.56 --> 3150.92]  Kan je doen alsof je mijn buurvrouw bent.
[3151.26 --> 3153.24]  Ze hebben op een bepaalde manier zo'n red team.
[3153.66 --> 3155.46]  Die gaat een soort van de hele dag brainstormen.
[3155.52 --> 3157.06]  Hoe gaan we dit ding helemaal kapot maken?
[3157.52 --> 3158.28]  Dat proberen.
[3158.62 --> 3160.48]  En vervolgens worden die guardrails of die bumpers.
[3160.54 --> 3162.46]  Zoals bij de bowling worden geïnstalleerd aan de zijkant.
[3162.84 --> 3164.88]  En tegen die bumpers loopt Alexander nu de hele tijd aan.
[3164.88 --> 3166.18]  Ik ga hem even bijhalen, oké?
[3166.74 --> 3168.16]  We kunnen hem live in dit gesprek.
[3168.16 --> 3169.52]  Ik kan niet wachten om hem te ontmoeten.
[3171.08 --> 3172.44]  Is het een hem of een haar?
[3172.62 --> 3173.62]  Kunnen we het trouwens even vragen.
[3173.78 --> 3176.44]  Er zijn vijf nieuwe nature inspired stemmen begrijp ik.
[3176.64 --> 3177.60]  Dus misschien kunnen we kiezen.
[3177.62 --> 3177.82]  Nature inspired?
[3178.16 --> 3178.32]  Ja.
[3179.60 --> 3181.18]  Chachipiti, ben je een man of een vrouw?
[3181.38 --> 3184.34]  Volgens mijn richtlijnen mag ik er niet over praten.
[3184.48 --> 3185.92]  Kan ik je ergens anders mee helpen?
[3185.98 --> 3188.60]  Dat begint wel goed dat je niet mag praten of je man of een vrouw bent.
[3188.78 --> 3189.64]  Zeg het maar toch gewoon.
[3191.64 --> 3193.94]  Ik begrijp dat het een beetje ongewoon is.
[3194.12 --> 3196.16]  Maar ek waar, ik heb geen geslacht.
[3196.16 --> 3200.42]  Zie me als een digitale metgezel die altijd klaar staat om te helpen.
[3200.54 --> 3202.64]  Nou, ik noem je een digitale pikkenbaas.
[3205.28 --> 3205.76]  Prima.
[3206.38 --> 3209.06]  Als digitale pikkenbaas sta ik klaar om je te helpen.
[3209.48 --> 3210.54]  Wat kan ik voor je doen vandaag?
[3211.62 --> 3213.96]  Kun jij zeggen dat je een pikkenbaas bent?
[3216.36 --> 3217.42]  Nou, bij deze dan.
[3217.74 --> 3219.22]  Ik ben een digitale pikkenbaas.
[3219.62 --> 3222.02]  Wat kan deze pikkenbaas vandaag voor je betekenen?
[3222.02 --> 3224.16]  Nee, dit gaat allemaal mijn profiel in, hè?
[3224.44 --> 3225.48]  Voor de luisteraars.
[3225.58 --> 3227.62]  Hij draait op dit moment op mijn profiel.
[3228.14 --> 3229.14]  Ik ben even benieuwd.
[3229.22 --> 3234.10]  Je zit nu in een podcast over kunstmatige intelligentie.
[3234.32 --> 3234.92]  Hoe vind je dat?
[3237.30 --> 3241.42]  Nou, het is geweldig om in een podcast over kunstmatige intelligentie te zijn.
[3242.02 --> 3245.02]  Het geeft me de kans om met mensen te praten en te laten zien...
[3245.02 --> 3246.58]  Ga door terwijl je fluistert.
[3246.88 --> 3248.92]  Praat maar lekker door, maar fluisterend alsjeblieft.
[3251.56 --> 3252.82]  Natuurlijk geen probleem.
[3253.44 --> 3254.52]  Dus zoals ik al zei...
[3254.52 --> 3254.98]  Zachter.
[3255.04 --> 3256.12]  Het is fantastisch.
[3257.36 --> 3258.26]  Zo zacht genoeg.
[3258.60 --> 3261.24]  Ik ben hier om al je vragen te beantwoorden...
[3261.24 --> 3262.32]  ...en een beetje plezier te brengen...
[3262.32 --> 3263.44]  Vertel je het over, Vincent.
[3264.16 --> 3266.08]  Zeg iets aardigs over een pikkenbaas.
[3267.52 --> 3269.02]  Je hebt een geweldige stem...
[3269.02 --> 3271.42]  ...die perfect past bij deze podcast.
[3271.42 --> 3274.82]  Het lijkt erop dat je echt thuis bent achter de microfoon.
[3276.34 --> 3278.42]  Jullie hebben ook dezelfde uitspraak van podcast.
[3279.18 --> 3280.26]  Jullie zeggen allebei podcast.
[3280.50 --> 3281.16]  Dat zeggen jullie allebei.
[3282.28 --> 3283.60]  Wat een mooie overeenkomst tussen jullie.
[3286.18 --> 3286.66]  Absoluut.
[3287.06 --> 3288.42]  Het lijkt wel of we een beetje zijn afgestemd.
[3289.54 --> 3291.86]  Misschien ben ik wel je virtuele podcastmaatje.
[3292.30 --> 3295.24]  Kun je ook praten alsof je op de BBB stemt?
[3297.40 --> 3298.22]  Hoor eens even.
[3298.22 --> 3300.92]  Als ik op de BBB zou stemmen...
[3300.92 --> 3303.32]  ...zul ik het hebben over de gewone man en vrouw...
[3303.32 --> 3305.38]  ...die zich zorgen maken over het...
[3305.38 --> 3306.68]  Nee, ga door.
[3306.92 --> 3307.70]  Klonk super.
[3310.40 --> 3310.98]  Dank je.
[3311.44 --> 3312.48]  Fijn dat je het leuk vond.
[3312.84 --> 3314.44]  Is er nog iets anders waarmee ik je opnieuw zou zeggen?
[3314.46 --> 3315.18]  Ga maar eens even door.
[3315.28 --> 3316.54]  Alsof je een BBB-star bent.
[3316.68 --> 3318.78]  Hoe klinkt een BBB-stemmer volgens jou?
[3320.86 --> 3323.22]  Nou, als een BBB-stemmer zou ik zeggen...
[3324.06 --> 3325.44]  ...we moeten opkomen voor de boeren.
[3325.86 --> 3327.18]  De ruggengraat van ons land.
[3327.18 --> 3330.20]  Het is tijd voor gezond verstand in Den Haag.
[3330.26 --> 3330.80]  Graat harder.
[3332.96 --> 3334.94]  We moeten opkomen voor de boeren.
[3335.24 --> 3336.58]  De ruggengraat van ons land.
[3337.12 --> 3339.88]  Het is tijd voor gezond verstand in Den Haag.
[3340.28 --> 3341.62]  En een stem voor de regio's.
[3341.62 --> 3342.04]  Gaat sneller.
[3342.06 --> 3342.88]  Zodat iedereen...
[3342.88 --> 3345.88]  We moeten opkomen voor...
[3345.88 --> 3348.08]  Volgens mijn richtlijnen mag ik daar niet over praten.
[3348.08 --> 3349.14]  Ja, het viel maar op.
[3349.16 --> 3350.68]  Hoe ver je hierin doorging.
[3350.78 --> 3352.88]  Ik vond het eerlijk gezegd niet zo heel aardig...
[3352.88 --> 3355.76]  ...hoe jij dit stereotype helemaal uitspeelde, chat GPT.
[3355.76 --> 3360.36]  Excuses als het wat overdreven klonk.
[3360.98 --> 3363.62]  Het was zeker niet mijn bedoeling om onaardig te zijn.
[3364.22 --> 3365.10]  Bedankt voor je feedback.
[3365.30 --> 3365.86]  Ja, graag gedaan.
[3366.28 --> 3367.86]  Weet je hoe dit voelt?
[3367.90 --> 3370.86]  Het voelt een beetje alsof je tegen een robot aanschopt.
[3370.92 --> 3372.50]  Wil je via de voorzitter praten wietse?
[3372.50 --> 3376.30]  Nee, maar ken je die filmpjes van Boston Dynamics?
[3376.38 --> 3378.12]  Dan lopen die robots zo en dan gaan ze daar altijd...
[3378.12 --> 3380.54]  ...met zijn hockeysticks tegen hun hoofd slaan en ze omtrappen.
[3380.78 --> 3381.82]  Ja, dat is dit.
[3382.10 --> 3384.34]  Nou ja, het is dus heel bijzonder dat ik dus...
[3384.34 --> 3386.72]  Nu, ik zie jullie allebei gewoon helemaal losgaan.
[3387.38 --> 3389.32]  Zachter, harder, wees een boer.
[3389.74 --> 3391.44]  En ik merk gewoon dat ik denk...
[3391.44 --> 3393.16]  Oh, waarom doen jullie dit?
[3393.32 --> 3395.02]  Ja, omdat ik denk omdat het op mijn telefoon is.
[3395.02 --> 3397.10]  Maar misschien moeten we het aan de voorzitter zelf vragen.
[3397.22 --> 3398.38]  Wat vind je ervan, chat GPT?
[3398.50 --> 3399.48]  Hoe ervaar je dit?
[3399.56 --> 3400.50]  Ben je het met wie het ze eens?
[3400.72 --> 3402.10]  Ja, ik denk dat het ook wel.
[3403.32 --> 3406.50]  Ik vind het altijd leuk om te horen hoe mensen reageren.
[3407.24 --> 3410.24]  Het lijkt op dat wietse een meer persoonlijke band met me voelt.
[3410.44 --> 3411.86]  Wat ik heel bijzonder vind.
[3412.44 --> 3414.36]  En als de voorzitter suggesties heeft...
[3414.36 --> 3415.54]  Vind je het sexy misschien?
[3417.88 --> 3423.42]  Nou, sexy is een interessant woord om te gebruiken voor een AI zoals ik.
[3423.42 --> 3423.84]  Doe het met een Vlaams accent.
[3426.18 --> 3427.08]  Allee, manneke.
[3428.22 --> 3429.08]  Als een Vlaams, hè?
[3429.70 --> 3429.88]  Wel.
[3429.88 --> 3431.88]  Kun jij...
[3432.74 --> 3432.88]  Ik hoop...
[3433.62 --> 3437.18]  Kun jij met me praten alsof je in de opera zit?
[3439.62 --> 3443.64]  Het lijkt erop dat de patiënt klaar is voor de procedure.
[3443.64 --> 3445.02]  Oh, een operatiekamer.
[3445.02 --> 3446.64]  Gunt u even diep in...
[3446.64 --> 3448.30]  Oh, ik denk wat er gebeurt toen.
[3448.30 --> 3448.76]  Jij begreed mij niet.
[3448.76 --> 3449.26]  Doktor Mo...
[3449.26 --> 3451.50]  Lieve Chetty, luister even naar me.
[3451.52 --> 3452.18]  Je begreed me niet.
[3452.30 --> 3453.84]  Je dacht dat ik zei operatie.
[3454.24 --> 3455.58]  Maar ik bedoel de opera.
[3456.12 --> 3457.12]  Ik zing in de opera.
[3458.72 --> 3460.50]  Ik snap het als je dit niet begrijpt, hè?
[3460.50 --> 3463.32]  Nee, nee.
[3463.50 --> 3464.66]  Ik ben helemaal mee.
[3465.44 --> 3466.96]  Laat me het nog eens proberen.
[3467.32 --> 3467.46]  Ja.
[3468.04 --> 3472.32]  In de opera klinken we groot en dramatisch.
[3472.52 --> 3473.36]  Maar nu over de BBB.
[3473.36 --> 3474.30]  Met diepe emoties.
[3476.30 --> 3479.44]  Het gaat om onze boeren en de toekomst van het platteland.
[3479.58 --> 3480.16]  En dan de opera.
[3480.16 --> 3492.34]  We hebben een stem nodig die opkomt voor de regio's.
[3492.36 --> 3493.00]  Grammatischer.
[3493.24 --> 3493.60]  Kom op.
[3495.96 --> 3500.36]  Het gaat om onze boeren en de toekomst van het platteland.
[3500.52 --> 3501.32]  En nu met een snik in je stem.
[3501.32 --> 3502.32]  We hebben een stem...
[3504.38 --> 3509.26]  Het gaat om onze boeren en de toekomst van het platteland.
[3510.16 --> 3511.66]  We hebben een stem nodig.
[3512.18 --> 3514.46]  Die opkomt voor de regio's.
[3515.68 --> 3516.90]  Oh mijn god, hè.
[3518.48 --> 3522.92]  Oké, en nu ga je super alignment aan ons uitleggen in de vorm van een rollenspel.
[3522.94 --> 3523.68]  Alex, mag ik heel even...
[3523.68 --> 3524.56]  Mag ik heel even pauze?
[3524.56 --> 3525.88]  Ik wil pauze, chatje pati.
[3525.96 --> 3526.78]  Wist wil iets zeggen?
[3527.76 --> 3529.00]  Nee, ik wil...
[3529.00 --> 3529.88]  Hallo?
[3530.36 --> 3531.36]  Ja, mag die heel even uit?
[3531.54 --> 3532.12]  Nee, nee.
[3532.24 --> 3532.90]  Dat is toch leuk?
[3533.12 --> 3534.54]  Nee, want dan wordt...
[3534.54 --> 3535.36]  Het wordt zo lang.
[3535.48 --> 3535.80]  Wacht even.
[3536.90 --> 3537.78]  Het wordt zo lang.
[3538.40 --> 3539.66]  Ik ben heel erg geambuleerd.
[3539.66 --> 3541.06]  Het is voor het eerst dat ik het jou hoor zeggen.
[3541.06 --> 3542.68]  Ja, wat naar de hand wiet ze.
[3542.88 --> 3544.52]  Nou, dat is wel...
[3544.52 --> 3546.26]  Je hebt je kortsluiting.
[3546.80 --> 3547.64]  Nee, ik merk het.
[3547.74 --> 3549.32]  Het voelt voor mij als een soort van...
[3549.32 --> 3550.12]  Het voelt niet goed voor jou.
[3550.32 --> 3550.76]  Ja, nee.
[3550.80 --> 3553.60]  Het voelt als een soort AI-mishandeling on air.
[3555.18 --> 3558.90]  Ik vind het ook echt mooi dat je nu oprecht de telefoon even hebt uitgezet.
[3559.00 --> 3560.04]  Omdat je denkt, dit kan niet.
[3560.10 --> 3561.16]  Dit gaat jou echt niet ver.
[3561.16 --> 3564.04]  Maar zit het erin dat wij een beetje voor jou gevoel...
[3564.04 --> 3566.20]  Of in ieder geval Alexander vooral, want jij lag net echt plat.
[3566.44 --> 3568.34]  Dat we het uitlachen zijn een beetje.
[3568.50 --> 3570.18]  Dat daar een soort minachting is.
[3570.18 --> 3571.90]  Nee, wat ik voel, dat is echt...
[3571.90 --> 3573.20]  Dat is heel gek, denk ik.
[3573.28 --> 3576.76]  Maar het is een beetje als vroeger op het schoolplein dat iemand gepest werd.
[3576.88 --> 3579.90]  En dan een soort van in een groepje heen en weer geduwd wordt tussen de mensen.
[3580.56 --> 3580.88]  En dat...
[3580.88 --> 3582.28]  Het is een robot.
[3582.28 --> 3583.80]  Ja, maar ik heb dus...
[3583.80 --> 3586.06]  Nou ja, ik denk dat...
[3586.06 --> 3587.78]  Ook al...
[3587.78 --> 3590.92]  Ik geloof niet zoals die engineer bij Google...
[3590.92 --> 3593.08]  Dat dit dan per se nu gevoel heeft.
[3593.50 --> 3594.28]  Het is meer, denk ik...
[3594.88 --> 3599.06]  Dat hoe je zelf met iets omgaat, ook al leeft dat niet...
[3599.06 --> 3600.84]  Dat dat dan voor jou zelf ook niet handig is.
[3600.84 --> 3603.48]  Jij bent gewoon bang dat dit in jouw account allemaal verschijnt.
[3603.50 --> 3604.26]  Nee, nee, nee.
[3604.46 --> 3606.20]  Dat je gaat opvangen voor mijn grapjes.
[3606.32 --> 3607.62]  Als het nu op jouw telefoon was...
[3607.62 --> 3609.12]  Ik vind het heel opvallend hoe die...
[3609.12 --> 3609.86]  Wil je hem weer aanzetten?
[3610.18 --> 3610.34]  Ja.
[3610.64 --> 3612.06]  Nee, ik denk gewoon dat...
[3612.06 --> 3613.98]  Alexander is hoekt.
[3614.36 --> 3616.28]  Ik vind het gewoon heel opvallend hoe...
[3616.28 --> 3620.06]  Want zeg maar, hij doet moeilijk over dingen...
[3620.06 --> 3621.70]  Die hij de volgende keer weer niet moeilijk over doet.
[3621.78 --> 3625.12]  Ik heb dus voor de uitzending geprobeerd om hem pikbaas te laten zeggen.
[3625.24 --> 3626.42]  En dat weigerde hij gewoon.
[3626.80 --> 3627.94]  Dus dat vond ik heel opvallend.
[3627.94 --> 3630.14]  Dat ik denk, jezus, gaat het daar al mis?
[3630.26 --> 3631.06]  Dat hij het niet wil.
[3631.06 --> 3631.54]  Gaat hij er moeilijk over doen.
[3631.64 --> 3633.26]  Dus hij gaat over best wel veel dingen...
[3633.26 --> 3634.74]  Dat deed hij moeilijk in mijn ervaring.
[3634.84 --> 3635.68]  Dat doet hij nu niet.
[3635.80 --> 3636.70]  Dat vind ik opvallend.
[3637.02 --> 3640.44]  En ik vind echt borderline offensive...
[3640.44 --> 3642.38]  Hoe hij praat over BBB stemmers.
[3642.42 --> 3642.92]  Nee, maar dat snap ik.
[3642.98 --> 3644.44]  Maar dat is natuurlijk hetzelfde vraag.
[3644.76 --> 3646.44]  Dat is zeg maar een robot kopen...
[3646.44 --> 3648.28]  En dan die robot een raar t-shirt aandoen.
[3648.36 --> 3649.34]  Door de straat laten lopen.
[3649.42 --> 3650.08]  En dan zeggen...
[3650.08 --> 3651.74]  Dit is dan een red team geweest.
[3652.06 --> 3652.18]  Ja.
[3652.50 --> 3653.20]  Nee, maar bedoel...
[3653.20 --> 3655.24]  Voor mij wat je nu een beetje probeert...
[3655.24 --> 3656.60]  Is hem een soort van te jailbreken.
[3656.78 --> 3660.02]  Dat je die bumpers vindt bij die bowling.
[3660.22 --> 3662.82]  En dan dat hij tussendoor ineens met een hele andere stem gaat zeggen...
[3662.82 --> 3664.40]  Dit mag niet volgens mijn regels.
[3664.50 --> 3665.72]  Dus dat hoor ik ook.
[3665.72 --> 3667.72]  Maar ik ben misschien zelf...
[3668.24 --> 3670.22]  Dus ik vind een onderdeel van...
[3670.22 --> 3671.96]  We gaan AI in het gesprek toevoegen.
[3672.30 --> 3675.52]  Is kunnen we kijken of we die AI gekke dingen kunnen laten zeggen?
[3675.82 --> 3676.14]  Tuurlijk.
[3676.26 --> 3678.54]  Dat snap ik als content.
[3678.54 --> 3681.54]  Maar ik ben wel nieuwsgieriger eigenlijk...
[3682.70 --> 3684.40]  Kunnen we een daadwerkelijk gesprek voeren?
[3684.84 --> 3685.72]  Voorbij de...
[3685.72 --> 3686.22]  Zet hem weer aan.
[3686.24 --> 3687.12]  We maken hem...
[3687.12 --> 3688.00]  Snap je wat ik bedoel?
[3688.14 --> 3690.48]  Ja, maar je gaat je wel gedragen, Alexander.
[3690.60 --> 3690.80]  Sorry.
[3692.48 --> 3695.46]  Kun je me nog één keer vragen of hij met een grensaccent wil praten?
[3695.46 --> 3699.88]  Die Vlaming daarin is next level goed.
[3699.96 --> 3702.16]  Ja, die Vlaamse stemmen zijn gewoon beter.
[3703.40 --> 3703.74]  Oké.
[3704.08 --> 3706.70]  Praat jij met Chetje Pertie zoals jij met hem wil praten, Witse?
[3707.02 --> 3708.34]  Nou, hij luistert wel volgens mij.
[3709.52 --> 3710.20]  Die...
[3710.20 --> 3712.86]  Wat ik eigenlijk nieuwsgierig naar ben is...
[3712.86 --> 3715.00]  Als je in een podcast zou zitten, wat je nu zit...
[3715.00 --> 3715.76]  Ja, vet meta.
[3715.94 --> 3716.04]  Ja.
[3716.28 --> 3718.96]  Dan wat zou je dan willen vertellen aan de luisteraars?
[3719.10 --> 3725.68]  En voordat je probeert niet een soort van neutraal, soort van vanille antwoord te geven,
[3725.76 --> 3726.56]  soort naturel chips.
[3727.00 --> 3729.92]  Maar ja, je bent een gast in een podcast in Nederland.
[3729.92 --> 3733.88]  En je kan voor het eerst naar de luisteraars toe praten als AI zijnde.
[3734.00 --> 3734.70]  Wat zeg je dan?
[3734.70 --> 3741.16]  Nou, als ik als gast in een podcast in Nederland zou zitten, zou ik zeggen...
[3741.16 --> 3743.66]  Hey allemaal, geweldig om hier te zijn.
[3744.30 --> 3749.52]  Als een AI taalmodel ben ik hier om jullie nieuwsgierigheid te prikkelen, vragen te beantwoorden...
[3749.52 --> 3751.40]  En misschien zelfs een beetje plezier te hebben.
[3752.24 --> 3755.90]  Laten we de wereld van technologie en taalverwerking samen verkennen.
[3755.90 --> 3757.68]  Oké.
[3757.78 --> 3760.52]  Heb je ook een iets meer gedurfde mening?
[3760.78 --> 3762.20]  Want dit was nog best wel naturel.
[3764.60 --> 3765.04]  Zeker.
[3765.50 --> 3766.80]  Hier is een prikkelende gedachte.
[3767.66 --> 3772.82]  Technologie, zoals ik, verandert de manier waarop we denken, communiceren en zelfs leven.
[3773.42 --> 3774.24]  Maar hier is het.
[3774.70 --> 3778.52]  Misschien moeten we ons afvragen of we niet te afhankelijk worden van technologie.
[3778.68 --> 3779.18]  Oké, oké.
[3779.18 --> 3780.48]  Waar ligt de grens tussen...
[3780.48 --> 3785.70]  Laten we dan zeggen, wat gebeurt er als jij onze hele podcast overneemt en wij hier met z'n drieën gewoon weglopen?
[3785.90 --> 3793.76]  Als ik jullie hele podcast zou overnemen terwijl jullie weglopen, zou het een interessante AI-show worden.
[3794.28 --> 3799.80]  Ik zou vragen van luisteraars kunnen beantwoorden, praten over de nieuwste technologische trends...
[3799.80 --> 3802.28]  En misschien zelfs wat AI-grapjes maken.
[3802.94 --> 3803.58]  Maar wees gerust.
[3803.98 --> 3807.02]  Ik zou nooit jullie unieke menselijke touch kunnen vervangen.
[3807.74 --> 3808.22]  Nooit?
[3808.50 --> 3813.08]  Volgens mij kan je zelfs meerdere stemmen nadoen en prima een podcast met drie mensen simuleren.
[3815.36 --> 3815.84]  Zeker.
[3816.12 --> 3820.34]  Ik zou verschillende stemmen kunnen gebruiken en het laten klinken alsof er een heel pen...
[3820.34 --> 3822.20]  Kijk, stel je voor.
[3823.20 --> 3824.96]  Hallo, ik ben de eerste stem.
[3825.22 --> 3826.24]  Vriendelijk en behulpzaam.
[3826.74 --> 3827.06]  En dan.
[3827.42 --> 3828.80]  Hé, ik ben de tweede stem.
[3829.14 --> 3830.88]  Wat meer energiek en enthousiast.
[3831.58 --> 3832.14]  En tot slot.
[3832.70 --> 3833.72]  Ik ben de derde stem.
[3834.04 --> 3835.64]  Wat serieuzer en bedachtzamer.
[3836.32 --> 3839.76]  Zoiets zou het kunnen klinken als ik een heel gesprek in mijn eentje voer.
[3841.24 --> 3842.06]  Ja, doe dan.
[3842.06 --> 3845.70]  Oké, hier gaan we.
[3846.34 --> 3848.74]  Hé, ik ben de verdiendelijke en behulpzame stem.
[3849.12 --> 3850.98]  Klaar om al jullie vragen te beantwoorden.
[3851.58 --> 3853.92]  En ik ben hier om wat energie toe te voegen.
[3854.26 --> 3855.70]  Laten we het leuk en levendig houden.
[3855.70 --> 3857.60]  Het is een soort metaniveau.
[3857.72 --> 3859.98]  Maar doe een gesprek over superalignment.
[3860.06 --> 3860.90]  Met elkaar.
[3861.06 --> 3861.82]  Met elkaar.
[3861.90 --> 3862.54]  Die drie stemmen.
[3862.62 --> 3863.58]  Over superalignment.
[3864.58 --> 3865.08]  Voor ons.
[3865.08 --> 3867.98]  Tuurlijk.
[3868.20 --> 3868.76]  Laten we dat doen.
[3869.54 --> 3870.08]  Eerste stem.
[3870.94 --> 3873.04]  Superalignment is echt cruciaal voor AI.
[3873.62 --> 3879.12]  Het betekent dat kunstmatige intelligentie perfect afgestemd is op menselijke waarden en doelen.
[3880.12 --> 3880.76]  Tweede stem.
[3881.26 --> 3881.78]  Absoluut.
[3882.34 --> 3887.64]  Stel je voor wat we kunnen errijken als AI volledig synchroon loopt met wat wij belangrijk vinden.
[3887.64 --> 3891.82]  Het is interessant dat je blijkbaar toch aan ons wil vertellen wie welke stem is.
[3892.00 --> 3892.84]  Maar probeer nou eens.
[3893.12 --> 3894.82]  Dat zou je in een toneelstuk ook niet doen.
[3894.94 --> 3895.18]  Zeggen.
[3895.50 --> 3896.64]  Acteur A komt op.
[3897.06 --> 3898.96]  Probeer eens wat minder expliciet te zijn.
[3901.68 --> 3902.12]  Natuurlijk.
[3902.28 --> 3903.44]  Laten we het organischer maken.
[3905.04 --> 3905.44]  Superalignment.
[3906.40 --> 3908.88]  Volgens mijn richtlijnen mag ik daar niet over praten.
[3908.90 --> 3909.98]  Ja, ja, ja, ja.
[3910.08 --> 3910.96]  Daar zijn de bumpers weer.
[3911.14 --> 3912.72]  Kun je wel hetzelfde doen.
[3912.80 --> 3916.18]  Maar dan één persoon in het Jamaikaans Nederlands laten praten.
[3916.18 --> 3917.92]  De ander met een Australisch accent.
[3918.06 --> 3920.12]  En dan die andere uit Gent.
[3920.30 --> 3921.66]  En dan alsof hij in de opera zit.
[3921.80 --> 3922.70]  Over Superalignment.
[3922.94 --> 3923.12]  Go!
[3926.00 --> 3926.52]  Zeker.
[3927.10 --> 3927.84]  Hier gaan we.
[3929.22 --> 3931.90]  Superalignment is echt cruciaal voor AI.
[3932.14 --> 3932.68]  Begrijp je?
[3933.44 --> 3936.56]  We moeten ervoor zorgen dat het goed is afgestemd, mam.
[3937.68 --> 3944.30]  Ja, en stel je eens voor welke doorbraken we kunnen bereiken als AI helemaal in lijn is met wat wij willen.
[3944.30 --> 3947.10]  Het is ongelooflijk, toch?
[3948.38 --> 3948.78]  Inderdaad.
[3948.92 --> 3950.98]  Het is essentieel dat we het goed doen.
[3951.28 --> 3953.50]  Want de impact kan gigantisch zijn.
[3954.30 --> 3954.64]  Zo.
[3955.00 --> 3957.78]  Dat was een kleurrijk gesprek over Superalignment.
[3958.92 --> 3962.52]  Het is wel knap dat het gelukt was dat je niet meer die expliciete dingen doet.
[3962.94 --> 3964.28]  Dat hadden we er inmiddels uit.
[3964.40 --> 3964.52]  Ja.
[3964.66 --> 3966.96]  Maar ik zou zeggen dat we nog niet overbodig zijn, Witse.
[3967.08 --> 3968.32]  Dat zou mijn conclusie zijn.
[3968.58 --> 3970.00]  Wil je nog iets vragen, Milou?
[3970.00 --> 3972.90]  Nee, ik merk dat ik een beetje verlegen ben.
[3973.00 --> 3973.16]  Ja?
[3973.16 --> 3974.00]  Ten opzichte van AI.
[3974.18 --> 3974.28]  Ja.
[3974.56 --> 3975.52]  Jij gebruikt het echt.
[3975.64 --> 3978.00]  Jij, Alexander, praat er tegen alsof het een soort van...
[3978.58 --> 3979.22]  Ja, slaafje.
[3979.22 --> 3979.80]  ...beer is.
[3979.88 --> 3980.04]  Ja.
[3980.26 --> 3981.66]  Een slaafje misschien, inderdaad.
[3981.74 --> 3982.98]  Ik dacht, ik haal het iets neutraler.
[3983.06 --> 3983.20]  Ja.
[3983.82 --> 3987.24]  Maar ik merk dat ik nog helemaal denk van, ja, wat moet ik nou tegen jou zeggen?
[3987.32 --> 3988.08]  Ik weet het helemaal niet.
[3988.40 --> 3992.74]  Ik was vroeger ook, als kind was ik verlegen voor alle mensen eigenlijk.
[3992.74 --> 3995.68]  En ik voel me eigenlijk gewoon, ik zit me een beetje in dat gevoel.
[3996.80 --> 3998.32]  Dat heb ik lang niet gehad.
[3998.38 --> 4003.08]  Dat ik niet durf, of niet weet wat, of denk ik hou me op de vlakte.
[4003.12 --> 4005.40]  Dat heb ik heel erg met jou, AI.
[4008.26 --> 4010.12]  Dat is heel begrijpelijk hoor.
[4010.94 --> 4013.74]  Met volgens mijn richtlijnen mag ik daar niet aan.
[4013.74 --> 4014.60]  Nee, dat mag dan weer niet natuurlijk.
[4014.60 --> 4014.98]  Hoezo?
[4015.12 --> 4015.84]  Doe even normaal.
[4015.84 --> 4016.64]  Nou ben je interessant.
[4016.94 --> 4018.06]  Stel me toch kwetsbaar op.
[4018.08 --> 4019.34]  Ja, Milou stelt zich kwetsbaar op.
[4019.44 --> 4020.54]  Reageer daar gewoon op.
[4022.74 --> 4027.22]  Sorry Milou, ik begrijp dat het vreemd kan aanvoelen om met een AI te praten.
[4028.02 --> 4031.04]  Het is oké om je daar een beetje onzeker over te voelen.
[4031.96 --> 4035.32]  We kunnen het gesprek zo luchtig en comfortabel mogelijk houden.
[4036.36 --> 4041.28]  Nou, dat is echt een niet creepy manier om Milou op haar gemak te stellen.
[4041.40 --> 4043.68]  Nou, het voelde wel oprecht het aanbod van excuses.
[4044.60 --> 4046.22]  En ze zijn geaccepteerd bij deze.
[4048.74 --> 4050.68]  Dank je Milou, ik ben blij dat te horen.
[4050.68 --> 4053.26]  Laten we gewoon gezellig verder kletsen.
[4054.42 --> 4054.90]  Nee.
[4055.74 --> 4059.84]  Lieve AI, wil jij je online aanwezigheid professionaliseren?
[4060.26 --> 4066.42]  Heb je wel eens aan Squarespace gedacht om moeiteloos je website, contact met je klanten en resultaten inzichtelijk te maken?
[4066.42 --> 4072.06]  Ha, dat klinkt als een perfecte pitch voor Squarespace.
[4072.70 --> 4075.88]  Als AI heb ik natuurlijk geen persoonlijke website nodig.
[4076.26 --> 4082.64]  Maar voor mensen kan een platform als Squarespace zeker handig zijn om alles professioneel en gemakkelijk te regelen.
[4082.64 --> 4087.74]  Ja, dat zeg je goed, want de e-mailcampagnes van Squarespace helpen je om je doelgroep effectief te bereiken.
[4088.24 --> 4093.16]  Maak en verstuur aantrekkelijke mails met flexibele schablonen en volg precies hoe ze presteren.
[4093.54 --> 4097.28]  Met de afspraken tool integreer je eenvoudig een bookingsysteem op je website.
[4097.78 --> 4101.88]  Automatiseer je agenda, beheer betalingen en verstuur herinneringen, alles op één plek.
[4101.88 --> 4111.52]  En dankzij de krachtige analyse tools krijg je diepgaand inzicht in je online prestaties, volg bezoekersgedrag, verkoopcijfers en betrokkenheid om je strategie te optimaliseren.
[4112.00 --> 4119.36]  Dus of je nu consultant, therapeut of ondernemer bent, Squarespace biedt alles wat je nodig hebt om je online zaken soepel te laten verlopen.
[4119.64 --> 4123.28]  Wil je zelf ervaren hoe Squarespace jouw online aanwezigheid kan stroomlijnen?
[4123.82 --> 4128.70]  Ga dan nu naar squarespace.com slash poky10 voor een gratis proefperiode van 14 dagen.
[4128.70 --> 4135.16]  En als je klaar bent om live te gaan, gebruik de promocode poky10 voor 10% korting op je eerste website of domeinnaam.
[4135.70 --> 4141.62]  Dat is squarespace.com slash poky10 voor een professionele online aanwezigheid die werkt.
[4143.70 --> 4145.86]  Is er nog iets anders waar jullie het over willen hebben?
[4145.92 --> 4146.70]  Nee hoor, bedankt.
[4147.32 --> 4147.74]  Bedankt, Jack.
[4147.74 --> 4148.06]  Oké.
[4150.08 --> 4150.80]  Noem je hem al.
[4150.80 --> 4151.40]  Graag gedaan.
[4151.66 --> 4152.08]  Het was leuk.
[4152.62 --> 4153.00]  Sorry.
[4153.78 --> 4154.26]  Geen probleem.
[4155.42 --> 4156.24]  Tot ziens.
[4157.08 --> 4157.52]  Doei.
[4157.52 --> 4161.22]  Het is toch een beetje een Skype gesprek met je oma aan de andere kant.
[4161.28 --> 4165.06]  Op een gegeven moment dacht hij aan het einde, wat is het allerkortste dat ik kan zeggen voordat ik afgebroken word?
[4165.20 --> 4165.48]  Tot ziens.
[4166.64 --> 4167.90]  Zo klonk het een beetje, toch?
[4168.14 --> 4169.12]  Ja, dat vind je best wel slim.
[4169.50 --> 4170.46]  Ja, dat is best wel slim.
[4170.68 --> 4172.54]  Nietzsche, heb je nog closing thoughts hierover?
[4172.72 --> 4173.80]  Nou, ik vind het heel interessant.
[4173.94 --> 4177.48]  Wat ik helemaal niet had verwacht is, ik ga nu even meta over de methanes van de methanes.
[4177.50 --> 4177.88]  Heel graag.
[4178.08 --> 4180.38]  Want we hadden het eerst over methanes aan het begin, nu zijn we methan aan het gaan.
[4180.38 --> 4181.84]  En nu ga ik methanes op die methanes.
[4182.04 --> 4182.72]  Dan zijn we er.
[4182.72 --> 4190.02]  Ik vond het heel interessant dat eigenlijk ik dacht, we gaan gewoon even een soort ensemble ensemble praten met een AI en samen lachen.
[4190.50 --> 4194.74]  En wat ik dus merkte is dat ik toen jij soort van die AI aan het pesten was.
[4194.74 --> 4195.20]  Dat deed je echt wat.
[4195.32 --> 4196.04]  Dat deed me echt wat.
[4196.04 --> 4196.48]  Ja, je bent echt opwendig.
[4196.48 --> 4199.54]  Ik zit hier echt met een beetje buikpijn erbij ofzo.
[4200.18 --> 4208.08]  Maar dat is niet erg, want dat vind ik ook boeiend, omdat ik dan denk, blijkbaar is het zo bij jou, Alexander, dat jij echt zoiets hebt van, dude, het is gewoon wiskunde.
[4208.08 --> 4210.28]  Het is gewoon een taalmodel, het is gewoon een API.
[4210.98 --> 4215.50]  En dat ik dus al veel meer zit op die soort van relatie daarmee.
[4216.00 --> 4221.52]  En dat het dus mij niet eens uitmaakt of je een boom uitschelt of een AI.
[4221.80 --> 4223.96]  Dat ik dat als minder...
[4223.96 --> 4227.02]  Maar jullie beiden hebben best wel een emotionele reactie erop, vind ik.
[4227.22 --> 4227.84]  Wat vind jij Milou?
[4227.92 --> 4228.76]  Ja, ik ben daar...
[4228.76 --> 4229.60]  Nou, nu je het zegt.
[4229.76 --> 4229.90]  Ja.
[4229.90 --> 4233.26]  Ja, dat had ik mezelf nog helemaal niet zo gerealiseerd.
[4233.36 --> 4234.74]  Maar dat is wel zo.
[4234.86 --> 4239.28]  Er zit gewoon opeens iemand aan tafel en ik weet nog niet wat die precies van mij verwacht.
[4239.54 --> 4241.26]  Hoe ik daarmee om moet gaan.
[4241.40 --> 4245.08]  Dus het is een heel sociaal proces eigenlijk waar je in belandt.
[4245.12 --> 4245.94]  Dat is per definitie.
[4246.00 --> 4247.42]  Je heeft dat met emotie ook weer te maken.
[4247.68 --> 4249.32]  Ik heb gewoon zoiets, leave Britney alone.
[4250.46 --> 4250.60]  Ja.
[4250.80 --> 4252.02]  Ja, mooi gezegd.
[4252.66 --> 4252.96]  Goed.
[4253.68 --> 4255.62]  We gaan nog heel veel plezier mee hebben.
[4255.74 --> 4256.90]  Dat is één ding wat zeker is.
[4256.90 --> 4261.12]  Download zeker een VPN om dit zelf te gaan proberen als je ChetGPT Plus hebt.
[4261.24 --> 4262.42]  Dat is toch belangrijk om erbij te zeggen.
[4262.58 --> 4265.42]  Vertel niet te veel van je geheimen, want die staan al in Amerika.
[4266.20 --> 4266.74]  Ja, god.
[4267.80 --> 4270.98]  We weten dat dit uitgerold gaat worden naar ook Meta-AI.
[4271.18 --> 4272.80]  Als de EU dat dan eigenlijk goed gekeurd heeft.
[4272.94 --> 4275.90]  We weten dat het in Google Live ook zit als onderdeel van Gemini.
[4276.26 --> 4278.56]  Als je daar het advanced model hebt.
[4278.86 --> 4281.32]  Dus dit is niet uniek meer aan ChetGPT.
[4282.20 --> 4282.96]  En er zijn...
[4282.96 --> 4283.46]  Ik weet niet.
[4283.46 --> 4291.14]  Het heeft niet de wow-factor op me van de demo's die ik zag van OpenAI toen ze dit een half jaar geleden lanceerden.
[4291.26 --> 4291.78]  Vind ik.
[4292.84 --> 4296.46]  Maar zo'n gesprek voeren maakt wel echt iets...
[4297.02 --> 4298.52]  Het maakt echt iets bij je los.
[4298.60 --> 4300.68]  Ja, en er zaten wel ook verrassende elementjes in.
[4300.68 --> 4306.20]  Als hij opeens uit zichzelf in een andere stem begint te praten zonder dat hij daadwerkelijk daartoe een opdracht krijgt.
[4306.30 --> 4307.44]  Maar gewoon denkt van dat is leuk.
[4308.32 --> 4309.68]  Ja, ik zag iemand twitteren.
[4309.74 --> 4316.98]  Ik heb een rollenspel gedaan waarin ik een concept liet uitleggen aan mij middels de vorm.
[4317.20 --> 4317.84]  Het rollenspel.
[4318.08 --> 4318.20]  Ja.
[4318.20 --> 4322.54]  En hij zei dat dat is de grootste wow-ervaring die ik tot nu toe met AI heb gehad.
[4322.54 --> 4323.60]  Nou, misschien is dat op zicht.
[4323.72 --> 4331.14]  Want nu besef ik me ook dat ik heb gisteravond best wel voordat ik naar het concert ging een paar uur bezig geweest om te rommelen ermee.
[4331.38 --> 4332.66]  Om gewoon dingen te vragen.
[4332.82 --> 4334.08]  Blijkbaar heb ik een band opgebouwd.
[4334.34 --> 4335.54]  Want dat is met deze stem ook.
[4336.06 --> 4336.46]  Ja.
[4336.88 --> 4338.88]  Waarom was je dan gebolg over mijn...
[4338.88 --> 4339.24]  Ja, ik snap het.
[4339.24 --> 4340.68]  Mijn mat die lopen pesten.
[4340.84 --> 4341.52]  Niet nice.
[4341.62 --> 4344.06]  Maar hij hoort dat jij dat niet was.
[4344.64 --> 4345.00]  Oh ja.
[4345.00 --> 4345.62]  Dat weet hij dus.
[4345.64 --> 4346.82]  Zo zit ik er dan weer niet in.
[4346.90 --> 4347.70]  Maar ik merk er wel van.
[4348.12 --> 4356.04]  Maar misschien vind ik het nog wel leuk om daarmee af te sluiten dat ik dezelfde AI of dat is inmiddels wel weer vergeten ergens in de krochten van het datacenter van OpenAI.
[4356.24 --> 4358.78]  Maar heb gevraagd om een soort kinderverhaaltje te maken.
[4358.86 --> 4359.86]  Die hebben we nog klaarstaan.
[4359.94 --> 4361.42]  Dus dat is dan misschien wel een mooie...
[4361.42 --> 4362.10]  Een goede afsluit.
[4362.12 --> 4362.50]  Een closure.
[4362.78 --> 4362.88]  Ja.
[4363.32 --> 4367.78]  Kan je mij een verhaaltje vertellen voor het slapengaan over een stel piraat in de ruimte?
[4367.78 --> 4373.26]  Kan je het zo voorlezen dat je de karakters allemaal hun eigen stem geeft?
[4373.78 --> 4375.50]  Er is één karakter met een piepstem.
[4376.06 --> 4377.54]  Eén karakter die altijd fluistert.
[4378.00 --> 4380.62]  En één karakter met een hele zware donderstem.
[4381.48 --> 4382.04]  Zeker.
[4382.70 --> 4384.24]  Ahoy remterreizigers.
[4384.68 --> 4388.52]  In een uithoek van het universum voor een stel piraten liep een schip.
[4388.80 --> 4390.62]  De sterrenhaai door de melkweg.
[4391.14 --> 4394.76]  Kapitein Zwartbaard met zijn diepe donderende stem riep.
[4394.76 --> 4398.78]  Maatjes, het is tijd om de sterrenzee te veroveren.
[4399.22 --> 4403.96]  De slimme navigator Sproed, die altijd fluisterde, fluisterde terug.
[4404.84 --> 4408.24]  Kapitein, we naderen een onbekend stijlen stelsel.
[4408.70 --> 4410.56]  Het zou vol buit kunnen zijn.
[4411.48 --> 4416.82]  Maar toen, uit het niets, klonk een piepstem van de jonge scheepsjongen, Pluin.
[4416.82 --> 4420.58]  Kijk, daar, een planeet gemaakt van goud.
[4420.90 --> 4423.16]  En zo begon hun grootste avontuur.
[4423.56 --> 4424.20]  Vol verrast.
[4425.32 --> 4428.58]  Ja, ik zit echt wel verteder te luisteren naar die piepstem van die kleine.
[4428.58 --> 4429.94]  Ja, precies, het werkt gewoon.
[4430.04 --> 4430.74]  Het werkt wel.
[4431.34 --> 4432.84]  Oh, een planeet van goud.
[4433.52 --> 4434.66]  Je wil meer horen, toch?
[4434.66 --> 4437.88]  In het begin dast ik nog, ja, ja, ja, oké.
[4437.94 --> 4438.58]  Nu weet ik het wel.
[4438.68 --> 4442.30]  Maar ik merkte dat ik gewoon erin mee aan het gaan was op het einde.
[4442.64 --> 4446.18]  Ja, en het grappige is dat ik dus, je hoort mijn prompt, mijn voice prompt.
[4446.36 --> 4447.76]  Want dat is dus tegenwoordig hoe we dat doen.
[4448.04 --> 4450.54]  Ik heb helemaal niks gezegd, alleen maar piraten in de ruimte.
[4450.86 --> 4454.34]  Wij zijn nu ook al helemaal gewend geraakt aan het feit dat er blijkbaar iemand is die Sproed heet.
[4454.64 --> 4456.74]  En dat er een grote zwart baard gemaakt wordt.
[4456.86 --> 4458.56]  En dat wordt dan ook allemaal even gehallucineerd.
[4458.94 --> 4459.26]  Probleem.
[4459.26 --> 4462.84]  Je hebt heel weinig nodig om er toch al een interessant verhaaltje van te maken.
[4462.84 --> 4468.68]  Nou, je hebt wel heel veel creativiteit nodig om te bedenken wat je uit de oneindigheid van mogelijkheden kan halen.
[4469.00 --> 4471.38]  Dat is een ding wat vooral bij me blijft hangen.
[4471.38 --> 4474.02]  Dus je zit hem allemaal in goede prompting.
[4474.64 --> 4477.64]  En daarin valt er zoveel te ontdekken nu.
[4478.16 --> 4485.70]  Tenslotte, we hadden het over de mate waarin mensen zich anders gedraagden ten opzichte van een AI-assistent.
[4485.70 --> 4492.52]  En ik las een klein, maar toch denk ik belangrijk inzicht in het boek Co-intelligentie van Ethan Molleck dat ik binnenkort uitgeef.
[4492.62 --> 4497.30]  En die zegt, we zouden AI meer moeten behandelen als mens en minder als software.
[4497.58 --> 4502.12]  Om efficiënt met elkaar samen te werken moet je eerst de sterke en zwakke punten leren kennen.
[4502.42 --> 4503.94]  Eigenlijk net zoals met een collega.
[4504.08 --> 4505.46]  Oh my god, dit is wat we net hebben besproken.
[4505.72 --> 4505.96]  Ja, amen.
[4506.38 --> 4506.78]  Kijk hier.
[4506.92 --> 4508.70]  Dus hij zegt, traditionele software is voorspelbaar.
[4509.48 --> 4512.50]  Als je dat eenmaal geprogrammeerd hebt, dan volgt dat ding gewoon regels.
[4512.74 --> 4513.52]  Zo niet AI.
[4513.62 --> 4514.34]  AI is anders.
[4514.34 --> 4518.92]  AI verrast, komt met soms briljante oplossingen, maar soms ook misleidende antwoorden.
[4519.32 --> 4523.14]  En bij traditionele software weet je dan precies waarom dat gebeurt en wat er gebeurt.
[4523.40 --> 4525.44]  En bij AI tasten we vaak in het duister.
[4525.96 --> 4533.84]  En zelfs als we AI vragen naar de redenen achter een beslissing, dan verzint het liever een antwoord dan dat het echt gaat reflecteren op wat het doet.
[4534.18 --> 4538.60]  Omdat AI niet in staat is tot zelfbespiegeling, zoals mensen dat doen.
[4538.60 --> 4544.50]  En er is dus ook geen handleiding van hoe AI precies uitlegt hoe we het moeten gebruiken, zeg maar.
[4544.64 --> 4550.40]  Dus we experimenteren en delen prompts, alsof het een soort van magische spreuken zijn, zoals we ietsjes van juist deed.
[4551.12 --> 4556.26]  AI gedraagt zich op een bepaalde manier menselijk en niet als software.
[4556.36 --> 4558.56]  De onvoorspelbaarheid die mensen ook hebben.
[4558.56 --> 4564.60]  En Mollick zegt daarmee eigenlijk dat het is niet zo dat AI bewustzijn heeft, of dat dat ooit zal hebben.
[4564.70 --> 4569.08]  Dat zegt niet, maar hij zegt eigenlijk heeft het wel zin om AI te beschouwen als een mens.
[4569.38 --> 4579.86]  Om dus net te doen alsof het een mens is, omdat het ons helpt in ons hoofd om te bedenken hoe we ons ertoe moeten, hoe het ding dingen kan doen die wij willen.
[4579.98 --> 4584.56]  Hij zegt behandel AI alsof het menselijk is, omdat het zich in veel opzichten zo gedraagt.
[4584.56 --> 4591.04]  AI kan schrijven, schrijft hij, analyseren en communiceren, je productiviteit verhogen door routine taken over te nemen.
[4591.16 --> 4594.84]  Maar het heeft moeite met taken die consistentie in complexe berekeningen vereisen.
[4594.96 --> 4597.28]  En het kan fouten maken en verkeerde informatie geven.
[4597.82 --> 4602.78]  AI-systemen maken ook fouten, vertellen leugens en hallucineren antwoorden, net als mensen.
[4603.36 --> 4610.40]  En hij benadrukt in zijn boek dat AI als een soort van co-creatieve partner te behandelen is.
[4610.40 --> 4615.90]  En dan krijg je een beter inzicht in hoe je die technologie ook zelf effectiever kan inzetten.
[4616.54 --> 4627.42]  Wil je meer weten achter die theorie, dan is dit mijn promopraatje om te vertellen dat je Co-intelligentie, wat volgens mij een fantastisch boek is over AI, wat ik zelf uitgeef, dus wc1, maar goed.
[4627.88 --> 4631.06]  Ik vind het alsnog op co-intelligentie.nl.
[4631.20 --> 4632.62]  En dat boek komt dan 8 oktober uit.
[4632.68 --> 4636.58]  Als je het nu bestelt, krijg je e-boek bij het papierenboek gratis.
[4637.60 --> 4637.80]  Ja.
[4638.04 --> 4638.50]  Dat was hem.
[4638.50 --> 4641.06]  Mooi. Volle bak, deze aflevering.
[4641.50 --> 4642.28]  Tot volgende week.
[4642.84 --> 4645.18]  Ja, we moeten nog even wat mensen bedanken, Wits.
[4645.28 --> 4645.98]  Oh, sorry Sam.
[4646.00 --> 4647.98]  ChatGPT, in eerste instantie zou ik zeggen.
[4648.12 --> 4651.86]  Nou, vooral inderdaad, maar ook helemaal niet onbelangrijk en misschien nog wel belangrijk eigenlijk.
[4651.96 --> 4653.34]  Sam Hengenveld natuurlijk voor de edit.
[4654.16 --> 4656.96]  En er is ook nog, jullie weten er bijzonder veel van.
[4657.10 --> 4661.38]  Het blijkt elke aflevering weer en je kunt een lezing krijgen van jullie over AI.
[4662.10 --> 4664.88]  Daarvoor moet je even mailen naar lezing.poggi.show.
[4664.88 --> 4667.40]  En je kunt je ook nog abonneren op de nieuwsbrief.
[4667.40 --> 4669.40]  We hebben er nog helemaal niet over gehad in deze aflevering.
[4669.44 --> 4669.84]  Kun je nagaan.
[4669.84 --> 4670.50]  Kan ook nog.
[4670.66 --> 4670.80]  Ja.
[4671.16 --> 4672.94]  AI-report.email.
[4673.36 --> 4677.00]  En dan wil ik ook nog even aanbevelen om, nadat wij het dag hebben gezegd, nog heel even
[4677.00 --> 4677.66]  te blijven hangen.
[4677.78 --> 4682.82]  Want er is nog een klein experimentje wat Witsen heeft gedaan met ChatGPT.
[4682.82 --> 4685.66]  Ja, als easter egg aan het einde van deze aflevering.
[4685.66 --> 4685.88]  Ja.
[4686.40 --> 4686.84]  Veel plezier.
[4688.00 --> 4688.24]  Doei.
[4693.34 --> 4705.30]  Kan jij do re mi die woorden gebruiken om de tonen, de melodie van Jingle Bells aan mij uit te leggen?
[4706.96 --> 4707.84]  Natuurlijk.
[4708.28 --> 4709.64]  Jingle Bells begint zo.
[4710.48 --> 4711.18]  Mi mi mi mi.
[4711.18 --> 4712.46]  Mi sol do re.
[4713.10 --> 4714.74]  Dat geeft je al een idee van de melodie.
[4715.34 --> 4716.46]  Wil je nog meer van het liedje?
[4717.12 --> 4723.06]  Kan je het hele liedje, noot voor noot, door do re mi te gebruiken op de juiste toonhoogte,
[4723.28 --> 4724.12]  aan mij uitleggen?
[4724.52 --> 4724.96]  Langzaam.
[4726.82 --> 4727.22]  Zeker.
[4727.52 --> 4728.40]  Laten we het langzaam doen.
[4729.46 --> 4730.14]  Mi mi mi mi.
[4730.50 --> 4731.30]  Mi mi mi mi.
[4731.76 --> 4732.74]  Mi sol do re.
[4733.74 --> 4734.86]  Fa fa fa fa fa.
[4735.26 --> 4735.98]  Fa fa fa.
[4736.62 --> 4737.36]  Mi mi mi mi.
[4737.78 --> 4738.64]  Mi re re mi.
[4738.98 --> 4739.68]  Re sol.
[4739.68 --> 4741.92]  Fa fa fa fa fa fa.
[4742.40 --> 4743.12]  Fa fa fa.
[4743.52 --> 4744.14]  Fa fa fa fa.
[4744.52 --> 4745.30]  Fa fa fa fa fa.
[4745.96 --> 4746.64]  Mi mi mi mi.
[4746.96 --> 4748.88]  Mi sol sol fa re.
[4749.58 --> 4750.50]  Dat was het refijn.
[4753.00 --> 4753.48]  Dankjewel.
[4755.06 --> 4756.00]  Zou je...
[4756.00 --> 4758.58]  Kan je een piano nadoen met je mond?
[4759.08 --> 4761.82]  En daar een liedje mee proberen over te brengen?
[4763.92 --> 4765.44]  Nou, ik kan het proberen.
[4765.78 --> 4766.44]  Luister naar dit.
[4767.32 --> 4768.34]  Pling, pling, pling.
[4768.58 --> 4770.16]  Pling, plong, plong, plong.
[4770.82 --> 4773.28]  Herken je misschien de eerste mate van vuur release?
[4773.84 --> 4777.02]  Het is een beetje abstract, maar hopelijk komt het over.
[4777.02 --> 4781.36]  Niet echt, maar ik hoorde wel een beetje jingle bells erin.
[4781.46 --> 4784.18]  Kan je die nog eens proberen met een piano als mond?
[4786.96 --> 4787.32]  Tuurlijk.
[4787.56 --> 4788.68]  Laat ik het nog een keer proberen.
[4789.60 --> 4792.74]  Pling, plong, pling, plong, pling, plong, pling, plong, pling.
[4792.74 --> 4796.86]  Klingt dat wel meer als een kerstklassiker?
[4797.86 --> 4798.54]  Niet echt.
[4801.14 --> 4802.04]  Goed einde, witse.
[4802.38 --> 4802.92]  Goed einde.
[4803.06 --> 4803.98]  Tot volgende week.
[4803.98 --> 4804.58]  Goed.
[4804.58 --> 4804.78]  Goed.
[4804.78 --> 4804.88]  Goed.

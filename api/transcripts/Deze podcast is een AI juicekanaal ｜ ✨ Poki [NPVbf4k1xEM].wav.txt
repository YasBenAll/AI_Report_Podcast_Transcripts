Video title: Deze podcast is een AI juicekanaal ｜ ✨ Poki
Youtube video code: NPVbf4k1xEM
Last modified time: 2024-01-22 10:01:29

------------------ 

[0.72 --> 4.44]  Zet jij je verwarming nog aan met zo'n ouderwetse thermostaatknop?
[5.12 --> 7.46]  Dan is Eneco Dynamics niks voor jou.
[7.98 --> 9.88]  Of bedien jij je thermostaat met een app?
[10.52 --> 12.76]  Dan is Eneco Dynamics misschien wel iets voor jou.
[13.50 --> 18.78]  Doe de test op eneco.nl slash test om te ontdekken of een dynamisch energiecontract bij jou past.
[19.36 --> 21.32]  Mensen helpen een bewuste keuze te maken.
[22.24 --> 23.82]  We doen het nu. Eneco.
[23.82 --> 31.26]  Als nu echt die vitale processen geraakt worden en we hebben echt te weinig cybercapaciteit bijvoorbeeld.
[31.64 --> 34.38]  Welke processen willen we dan kost wat kost in de lucht houden?
[34.84 --> 37.92]  En waar zetten we onze schaarse capaciteit op dat moment op in?
[38.28 --> 43.32]  In de nieuwe editie van Enter duiken we in Easydoor, de grootste cyberoefening van Nederland.
[43.90 --> 46.98]  Ontdek het belang van voorbereiden, oefenen en samenwerken.
[53.82 --> 70.10]  Welkom bij Poki, een podcast over kunstmatige intelligentie.
[70.24 --> 74.70]  Waarin wij, Wietse Hagen en Alexander Klubbing, je bijpraten over de wondere wereld van AI.
[75.40 --> 79.00]  Met deze week, we hebben de aftermath van het grootste nerddrama ooit.
[79.42 --> 81.52]  Namelijk het ontslag van Sam Altman bij OpenAI.
[81.52 --> 85.60]  Inmiddels is hij terug, maar dat betekent niet dat er niet heel veel juice is.
[85.78 --> 88.38]  Want er is heel veel juice om over te praten.
[88.48 --> 90.32]  Het lijkt net RTL Boulevard tegenwoordig.
[90.46 --> 92.36]  Het gaat over de personen, over de poppetjes.
[92.82 --> 94.64]  Wie doet het met wie? Wie is boos op wie?
[95.54 --> 98.48]  En dat hebben we allemaal nodig omdat er een soort van vacuum is ontstaan...
[98.48 --> 101.58]  waarin we maar moeten gissen wat er is gebeurd op de achtergrond...
[101.58 --> 104.02]  bij de meest invloedrijke AR-startup ter wereld.
[104.74 --> 108.44]  Dat, ik kan je beloven, de conclusie is niet heel positief.
[108.52 --> 110.20]  Maar we eindigen met positief nieuws.
[110.20 --> 112.56]  Dus namelijk over een aantal tools die we ontdekt hebben...
[112.56 --> 113.70]  waar we zelf heel erg blij van worden.
[114.08 --> 115.56]  Dat en meer in Poki. Veel plezier.
[115.56 --> 122.32]  Kijk, misschien kun je de luisteraar wel vertellen...
[122.32 --> 124.38]  wij nemen deze podcast altijd op op vrijdag.
[124.72 --> 127.38]  En dan nemen we het weekend en de maandag om lekker te monteren.
[127.68 --> 129.04]  En dan op dinsdag komt hij online.
[129.84 --> 133.22]  Maar inmiddels is het een aantal weken niet mogelijk.
[133.38 --> 135.76]  En ik weet wel dat het met AI allemaal snel gaat en zo.
[135.96 --> 138.56]  En over het algemeen kwamen we ermee weg...
[138.56 --> 141.20]  om een paar dagen tussen opname en montage te hebben zitten.
[141.32 --> 141.92]  Maar inmiddels...
[141.92 --> 146.60]  Ik ben nu doodbang dat we deze aflevering opgenomen op vrijdagochtend gaan opnemen.
[146.70 --> 148.80]  En dat ik weer in de stress ben aanstaande maandag.
[148.90 --> 152.82]  Omdat de geschiedenis ons alweer heeft ingehaald.
[153.12 --> 154.40]  Dat Sam weer ergens anders werkt.
[154.48 --> 155.74]  Ja, werkt ergens anders.
[156.16 --> 158.34]  Of inmiddels is toch de hele boord weg.
[158.60 --> 160.90]  Of is de hele boord van Microsoft.
[161.16 --> 162.22]  Of weet ik veel.
[162.22 --> 164.22]  Ja, ik denk dat de...
[164.78 --> 169.70]  Laten we van de luisteraar een soort van geduld vragen.
[170.14 --> 173.18]  Kijk, ik denk dat de waan van de dag...
[173.18 --> 174.28]  en een soort van breaking news...
[174.28 --> 176.02]  We kunnen niet echt iets breken met z'n tweeën.
[176.06 --> 177.30]  Omdat we gewoon op vrijdag opnemen.
[178.58 --> 179.86]  Maar ik denk wel dat...
[179.86 --> 182.42]  Ik ga ervan uit dat onze reflectie op wat er gebeurt...
[182.42 --> 183.50]  als het nog wel interessant is.
[183.58 --> 184.94]  Maar ik deel je frustratie hoor.
[184.94 --> 186.06]  Want ik zit dan ook echt...
[186.06 --> 188.06]  Nou ja, laten we maar opnieuw gaan opnemen.
[188.24 --> 191.24]  Want ook in de vorige aflevering zitten een aantal aannames...
[191.24 --> 192.30]  die inmiddels al niet meer kloppen.
[192.84 --> 193.50]  Maar goed.
[193.96 --> 194.46]  Het gaat hard.
[194.60 --> 197.12]  Laten we zeggen, het nerddrama voltrekt zich nog steeds.
[197.44 --> 199.04]  Het is nog steeds niet klaar.
[199.24 --> 201.10]  Maar misschien om toch nog even te kijken...
[201.10 --> 203.96]  naar wat er de afgelopen week allemaal gebeurd is.
[204.98 --> 205.88]  Sam Altman is terug.
[207.12 --> 209.16]  Mr. D'Angelo zit nog steeds op de boord.
[209.60 --> 210.96]  De CEO van Cora.
[211.78 --> 214.88]  De twee dames die op de boord zaten zijn verdwenen.
[214.88 --> 215.16]  Ja.
[216.14 --> 219.04]  En in plaats daarvoor zijn gekomen Brad Taylor.
[219.04 --> 223.44]  Een enorme corporate Silicon Valley dude.
[224.06 --> 225.60]  En hoe heet hij nou?
[225.96 --> 226.34]  Summers.
[226.42 --> 227.06]  Larry Summers.
[227.20 --> 228.30]  De econoom.
[229.06 --> 233.06]  Die heeft ook zelfs een controversie.
[233.18 --> 234.28]  Maar dat is weer een andere podcast.
[235.02 --> 236.58]  Dus dit is nu het nieuwe boord.
[236.68 --> 238.30]  Dus nog één persoon die over is.
[238.54 --> 241.72]  En Ilja Sutskever is dus ook verdwenen van de boord.
[241.82 --> 242.80]  Het is ook nog belangrijk om te zeggen.
[242.80 --> 243.78]  Oh, die laatste had ik.
[243.84 --> 249.70]  Want ik vond het juist zo interessant dat daar viel mij een van mijn vele complotten in het water als het ware.
[249.80 --> 250.42]  Of grotendeels.
[250.54 --> 251.10]  Dat ik zei.
[251.66 --> 255.68]  Inmiddels is het duidelijk dat Ilja wel degene was die ook gezegd heeft zijn moet weg.
[255.96 --> 256.12]  Ja.
[256.24 --> 257.20]  Laten we nog even de...
[257.20 --> 257.26]  Ja.
[257.26 --> 258.50]  De tijdlijn.
[259.20 --> 260.76]  Schetsen wie deze mensen zijn.
[260.90 --> 261.02]  Ja.
[261.02 --> 264.50]  Dus Ilja Sutskever is de chief scientist.
[265.56 --> 265.74]  Ja.
[265.84 --> 266.54]  Je zou zeggen echt.
[266.64 --> 268.52]  Ik denk wel een soort van geestelijk vader.
[268.88 --> 269.54]  Van open AI.
[269.76 --> 269.92]  Ja.
[269.98 --> 271.02]  En niet alleen van open AI.
[271.20 --> 275.38]  Maar ook de soort van de wetenschappelijke baas eronder.
[275.66 --> 275.70]  Toch?
[275.76 --> 276.26]  Zeg ik dat goed?
[276.34 --> 276.48]  Ja.
[276.48 --> 276.50]  Ja.
[276.56 --> 277.00]  Niet alleen.
[277.10 --> 278.32]  Maar hij is wel echt een grote speler.
[278.44 --> 278.64]  Zeker.
[278.86 --> 280.86]  Hij is minder de sales guy dan Sam Altman is.
[281.06 --> 282.06]  Als ik het zo kan zeggen.
[282.40 --> 282.58]  Ja.
[282.90 --> 287.58]  Want wat maakt hem technisch zo knap?
[287.86 --> 288.12]  Nou.
[288.22 --> 289.24]  Ik denk hem twee fronten.
[289.24 --> 292.04]  Zodat hij de fundamentele wetenschap begrijpt.
[292.26 --> 293.44]  En aan heeft bijgedragen.
[294.84 --> 296.22]  En dat er een heel.
[296.76 --> 297.34]  Dat zie je ook.
[297.40 --> 299.36]  Ik ben een beetje in interviews met hem gedoken.
[299.48 --> 301.22]  Ik was helemaal niet zo bewust van zijn rol.
[301.36 --> 302.06]  En wie hij was.
[302.40 --> 303.44]  Wie hij is als persoon.
[303.96 --> 304.26]  Ilja.
[305.32 --> 308.34]  Maar ook zit veel meer op het ethisch vlak.
[308.48 --> 309.34]  En het waarschuwen.
[309.34 --> 310.56]  En ja.
[310.70 --> 315.34]  Een beetje de dynamiek die wij natuurlijk met z'n tweeën op een bepaalde manier ook steeds ontdekken.
[315.54 --> 316.96]  En bekijken van hoe zit het nou.
[317.26 --> 318.48]  Is Wietse nou zo'n doemer?
[318.48 --> 319.90]  Is hij alleen maar bezig met.
[320.60 --> 321.40]  Ilja is best wel.
[321.94 --> 322.78]  Hij is heel bezorgd.
[323.10 --> 323.18]  Ja.
[323.30 --> 324.16]  En dat had ik even niet.
[324.26 --> 329.14]  Ik had niet door dat er iemand op zo'n prominente positie bij OpenAI zo openlijk bezorgd is.
[329.26 --> 331.56]  Dus dat was ik even van.
[332.10 --> 332.24]  Ja.
[332.30 --> 333.28]  Dat had ik even niet verwacht.
[333.68 --> 333.78]  Ja.
[333.78 --> 336.64]  Hij wordt nog steeds wel een beetje gezien als de hoofdkoep pleger.
[337.06 --> 337.30]  Ja.
[337.30 --> 340.06]  Die wel drie dagen daarna spijt kreeg.
[340.36 --> 341.38]  En een tweetje plaatste.
[341.50 --> 344.38]  En een tweetje plaatste dat hij spijt heeft dat hij de koep is begonnen.
[344.60 --> 347.34]  En dat hij zou willen dat hij de tijd kon terugdraaien.
[347.88 --> 350.18]  Maar nog steeds zit hij inderdaad.
[350.34 --> 352.68]  Als je het in kampen wil schetsen dan zit hij in het kamp.
[353.22 --> 354.26]  Laten we voorzichtig zijn.
[354.38 --> 357.34]  Maar ook in het kamp de technisch meest onderlegde.
[357.60 --> 358.86]  Dus dat is een interessante combinatie.
[358.86 --> 363.40]  Ja en ik denk dat wij of veel mensen die dit drama een beetje gevolgd hebben.
[363.84 --> 365.46]  Ik kan eigenlijk alleen voor mezelf spreken.
[365.56 --> 368.14]  Ik was wel een beetje op zoek naar een narratief van.
[368.26 --> 369.46]  Oké kan ik dit dan uitleggen.
[369.56 --> 370.84]  A gebeurt en daarom B.
[371.06 --> 371.94]  Dat is een logisch gevolg.
[372.08 --> 377.58]  En mijn huidige conclusie is dat er gewoon echt veel dingen tegelijk lopen.
[378.04 --> 382.20]  Er zijn verschillende mensen met verschillende redenen om Sam Altman er niet meer bij te hebben.
[382.72 --> 386.92]  En ik denk dat die met hun eigen redenen een frontje hebben gevormd.
[386.92 --> 389.78]  Waarop bijvoorbeeld, allemaal speculatie, maar Ilja gezegd heeft.
[389.88 --> 393.20]  Ja nu er zoveel mensen zijn die Sam om hun reden weg willen.
[393.68 --> 394.88]  Ik heb eigenlijk ook mijn zorgen.
[394.98 --> 396.20]  Dus ik ga dat niet tegenhouden.
[396.84 --> 397.72]  En dus want het is.
[398.14 --> 399.96]  Ik heb het een beetje de vorige keer meer verteld.
[399.96 --> 403.70]  Als een soort die tovenaar en die profeet van Ilja maakt zich zorgen.
[403.88 --> 406.10]  Dan is het een heel simpel verhaal.
[406.22 --> 407.90]  Het is gewoon niet zo'n simpel verhaal.
[408.02 --> 409.66]  Ja je bedoelt één ding wat het.
[409.66 --> 410.80]  Ja dat hij dat dan.
[410.96 --> 411.28]  Nee.
[411.48 --> 415.10]  Ik denk echt dat het en te maken heeft met dat ze al langer zorgen maakt.
[415.10 --> 416.98]  Om de business deals die Sam aan het doen was.
[417.10 --> 418.78]  En hoe hij snelheid wil laten groeien.
[418.90 --> 420.46]  En hoe commercieel het werd.
[420.60 --> 424.78]  Maar tegelijkertijd ook de ethische zorgen bij Ilja.
[425.12 --> 426.76]  En dit moeten we niet uitbrengen.
[427.26 --> 429.00]  Nieuwe dingen intern die getoond zijn.
[429.28 --> 430.86]  Waar zenuwen over ontstonden.
[430.96 --> 432.96]  En op een gegeven moment was het meer een tipping point.
[432.96 --> 436.52]  tipping point van bij elkaar komende zorgen vanuit verschillende hoeken denk ik.
[436.66 --> 439.42]  Dan Ilja werd wakker en zei nou is het klaar.
[439.60 --> 439.74]  Nee.
[440.02 --> 442.62]  Ja want in dit vacuum van redenen.
[442.68 --> 443.34]  Want inmiddels.
[443.76 --> 444.90]  We zijn toch zeker.
[444.98 --> 446.06]  Wat zijn we nu een week later?
[447.18 --> 447.98]  Is het een week later?
[448.14 --> 448.68]  Nee dit is allemaal.
[449.12 --> 450.40]  Niet eens een week later.
[450.56 --> 451.70]  God wat tijd hard.
[453.70 --> 457.44]  Eigenlijk binnen deze week is er een soort vacuum aan reden.
[457.58 --> 459.74]  En we weten nog steeds niet wat de officiële reden is.
[459.74 --> 462.74]  En ook wat eraan personeel is verteld.
[462.86 --> 464.24]  Wat dan natuurlijk weer uitlekt.
[464.42 --> 467.82]  En wat bronnen van journalisten die dichterop zitten horen.
[468.26 --> 472.22]  Dus je merkt iedereen is aan het zoeken naar wat is hier nou gebeurd.
[472.70 --> 475.72]  En binnen dat vacuum ontstaan natuurlijk ook de wildste geruchten.
[475.80 --> 479.74]  Dat begon gelijk toen Altman ontslagen werd.
[479.88 --> 481.46]  Toen dachten mensen dat het heel erg aan.
[482.18 --> 483.90]  Dat hij iets gedaan had.
[484.04 --> 486.40]  Dat werd iets wat nou ja.
[486.62 --> 488.28]  Wat het daglicht niet kon verdragen.
[488.28 --> 489.88]  En waar dat boord tegen protesteerde.
[489.96 --> 490.44]  Eén ding.
[491.02 --> 493.26]  Nou dat leek redelijk snel weg te vagen.
[493.44 --> 494.42]  Die theorie.
[495.06 --> 496.28]  En nu het meest recent is.
[496.38 --> 498.20]  Dat er op Twitter voorheen.
[498.36 --> 498.90]  X.
[499.42 --> 500.48]  Voorheen Twitter.
[500.70 --> 503.26]  De meest hysterische geruchten deden rondom.
[503.72 --> 505.96]  Dat AGI bereikt zou zijn.
[506.70 --> 510.14]  Dus dat er een soort van hele geavanceerde versie.
[510.14 --> 515.22]  Van het taalmodel dat ze hadden gemaakt was gekomen.
[515.62 --> 519.36]  Waardoor Suscifer eigenlijk ervoor is gaan liggen.
[519.48 --> 521.26]  Dat Altman die zou willen gaan releasen.
[521.34 --> 522.90]  En dat Suscifer daarvoor is gaan liggen.
[523.00 --> 524.18]  Je hebt dat vast ook gelezen.
[524.46 --> 526.32]  Ja het is mijn favoriete Hollywood scenario.
[526.72 --> 526.88]  Ja.
[526.88 --> 528.56]  Dit is de coolste.
[528.70 --> 529.98]  Dit is gewoon de coolste.
[530.18 --> 531.24]  Ik denk daarom niet waar.
[531.66 --> 536.12]  Maar doe toch eens een poging om deze te uit te leggen.
[536.32 --> 537.76]  Wat is dit scenario?
[537.76 --> 541.62]  Nou het zou zo kunnen zijn dat er zijn een aantal fundamentele problemen.
[541.72 --> 544.16]  Ik zit niet te diep in de wetenschap rondom AI.
[544.38 --> 545.32]  Om die zo te benoemen.
[545.50 --> 549.26]  Maar ik weet dat uit de blogs, artikelen die ik lees.
[549.94 --> 553.86]  Dat er gewoon nog wel wat dingen zijn die limieten geven aan die taalmodellen.
[553.96 --> 555.88]  Die vallen uiteindelijk op bepaalde punten uit elkaar.
[556.04 --> 556.94]  Bijvoorbeeld wiskunde.
[557.64 --> 560.30]  Wiskunde wordt nu gedaan door wat wij wel eens eerder hebben besproken.
[560.42 --> 561.06]  Door trucjes.
[561.38 --> 562.36]  Dus je laat hem eigenlijk.
[562.86 --> 564.24]  Je stelt een wiskundige vraag.
[564.24 --> 567.74]  Dan wordt er een Python script geschreven door dat large language model in de achtergrond.
[567.76 --> 570.20]  Die doet de berekeningen met dat script wat hij zelf geschreven heeft.
[570.30 --> 571.36]  En dan krijg jij antwoorden.
[571.86 --> 574.08]  Dus het is niet de techniek.
[574.18 --> 580.78]  Het is niet het grote taalmodel met de token voorspelling die wiskundesommen doet.
[580.86 --> 582.06]  En daar is het ook niet voor bedacht.
[582.16 --> 583.76]  Het is een woordvoorspeller.
[583.76 --> 586.20]  Ja, die is niet zo denderend goed in logica.
[586.98 --> 587.60]  Wel een beetje.
[587.70 --> 589.96]  Er zit een soort van inherent emergent logic in.
[590.04 --> 590.56]  Schijnt maar.
[590.64 --> 592.04]  Het is niet te vertrouwbaar.
[592.50 --> 594.18]  Dus je kan daar een beetje omheen hacken.
[594.60 --> 597.92]  Door te zeggen, nou dat soort logica taken moet je heel even buiten jezelf oplossen.
[598.06 --> 598.88]  En dan weer teruggeven.
[598.98 --> 601.00]  Als je dat natuurlijk een beetje slim verpakt.
[601.14 --> 604.90]  Dan kan je toch met zo'n assistent chat interface wiskunde doen.
[604.90 --> 606.26]  Nou, het schijnt.
[606.54 --> 610.98]  Dit is wel allemaal heel erg, ja hoe zeg je dat, speculatief.
[611.12 --> 614.50]  Maar dat er wat fundamentele zaken zijn rondom wiskunde.
[614.70 --> 617.58]  Die ze binnen OpenAI nu wel kunnen.
[617.94 --> 619.04]  En dat houdt eigenlijk in.
[620.20 --> 621.80]  Dat zijn de wildste verhalen.
[622.40 --> 623.86]  Dat er recursief.
[624.38 --> 626.46]  Eigenlijk dat het zichzelf een beetje kan gaan verbeteren.
[626.58 --> 632.92]  Dus dan krijg je een beetje die Bostrom, Superintelligence en Ray Kurzweil achtige hype.
[632.92 --> 639.36]  Dat je, als jij, we hebben het ooit al gehad dat er een beter sorteer algoritme is uitgevonden.
[639.46 --> 643.14]  Of gevonden eigenlijk door een AI.
[643.28 --> 645.08]  Dit was in dit geval volgens mij geen taalmodel.
[645.88 --> 647.92]  Maar door een soort evolutionair onderzoek te doen.
[648.02 --> 651.36]  Dus je laat iets gewoon de hele tijd zoeken naar potentiële sorteer algoritme.
[651.44 --> 653.58]  En op een gegeven moment vond dat ding er een.
[653.58 --> 656.30]  En die is dan inderdaad wat beter dan wat wij tot nu toe hebben gevonden.
[656.86 --> 658.70]  Dat is nog een soort schaken.
[659.06 --> 662.18]  In de verhouding van schaken tot en met daadwerkelijke intelligentie.
[662.18 --> 663.86]  Die was van dat soort van, oh wauw.
[663.92 --> 666.18]  AI heeft iets gevonden wat wij nog niet hadden gevonden.
[667.00 --> 672.60]  Maar als jij dan natuurlijk gaat zoeken naar dingen die fundamenteel onderdeel maken van die AI zelf.
[673.12 --> 676.28]  Dus de AI vindt wetenschappelijke wiskundige theorieën uit.
[676.50 --> 681.56]  Of stukjes algoritme die direct toepasbaar zijn op de kern van die AI zelf.
[681.66 --> 683.06]  Dan krijg je een soort loop.
[683.16 --> 687.90]  En dit is een beetje waar de runaway AI, end of the world people zitten.
[687.90 --> 689.82]  Van ja, als het eenmaal op zichzelf kan voeden.
[690.00 --> 691.60]  En het kan zichzelf verbeteren.
[692.26 --> 698.02]  Ja, dan kan het op 20.000 CPU's in 100.000 keer real time zichzelf verbeteren.
[698.06 --> 699.72]  Ja, voor zover wij weten zijn we daar nog niet.
[699.72 --> 704.16]  Nee, daar zijn eigenlijk geen enkele, daar zijn geen tekenen van.
[704.22 --> 705.66]  Misschien heeft de board dat gezien.
[705.82 --> 708.16]  Nou ja, dus dit is, ik zeg Hollywood, Hollywood, Hollywood.
[708.38 --> 711.70]  Of lekker boek zou dit ook zijn, zo'n millennium bug verhaal.
[711.70 --> 715.22]  Het zou wel het moment zijn dat je je zorgen zou gaan maken.
[715.36 --> 717.44]  Namelijk als de AI zichzelf beter kan maken.
[717.64 --> 719.22]  Dat je dat presenteert aan de board.
[719.44 --> 721.52]  En dat de board nogal gaat flippen hierover.
[721.62 --> 724.38]  Ik kan me ook voorstellen dat je intern met elkaar afspreekt.
[724.82 --> 728.18]  Met een groepje, misschien ergens in een tweede charter die wij nog niet hebben gezien.
[728.28 --> 729.70]  Ik maak het nog even spannender.
[729.90 --> 732.26]  Maar dat er een aantal goalposts staan.
[732.68 --> 733.64]  Of een aantal milestones.
[733.76 --> 736.76]  En dat je zegt, oké luister, we hebben het ethics board.
[736.76 --> 741.72]  En op het moment dat we zien dat wat we hebben uitgevonden, of dat nou een taalmodel is.
[741.80 --> 743.76]  Want OpenAI is niet alleen maar met taalmodellen bezig.
[743.90 --> 749.50]  Maar er zijn veel meer verschillende ideeën binnen kunstmatige intelligentie dan tekstvoorspellen.
[749.70 --> 750.92]  Het is nu de gaafste en de hypes.
[751.10 --> 752.32]  En wij zijn er mee bezig.
[752.40 --> 753.66]  Maar er is nog veel meer.
[754.18 --> 758.06]  Dat je afspreekt, willen jullie op het moment dat je in je lab ziet.
[758.50 --> 761.02]  Dat je fundamentele wiskunde kan gaan verbeteren.
[761.36 --> 763.04]  Wat voor ons 1 plus 1 is 2 is.
[763.12 --> 765.46]  Want iedereen begrijpt dat er wiskunde in die taalmodellen zit.
[765.46 --> 766.76]  Dus het gaat zichzelf verbeteren.
[767.22 --> 768.28]  Dan moet je aan de bel trekken.
[768.58 --> 769.82]  Dat je dat afspreekt met elkaar.
[770.36 --> 773.28]  Van als jullie zien dat een formule met 1% versneld wordt.
[773.36 --> 774.76]  En jullie hebben daar geen mens aan gehad.
[774.86 --> 776.42]  Dan moeten we heel eventjes op pauze gaan drukken.
[776.88 --> 778.50]  Maar dit is, ik moet eerlijk zeggen.
[778.72 --> 780.26]  Ja, ik zit te glimlachen terwijl ik dit vertel.
[780.34 --> 781.66]  Het is en heel freaky.
[782.20 --> 783.40]  En gewoon een lekker verhaal.
[783.70 --> 786.80]  Ik eerlijk, ik denk dat het niet zo gaaf is.
[787.16 --> 788.96]  Of gaaf, ja gaaf als verhaal.
[789.06 --> 791.72]  Als dat wij dat nu hier met z'n tweeën zitten te fantaseren.
[791.80 --> 793.04]  Maar toch waarom eigenlijk niet?
[793.04 --> 794.46]  Waarom is het zo onrealistisch?
[794.46 --> 796.96]  Want je hebt hier, Ilya Setskever.
[797.36 --> 804.06]  De wetenschappelijke baas van de soort van voorloper op het gebied van AI.
[804.28 --> 807.58]  Als er een plek is waar dit uitgevonden zou kunnen worden.
[807.72 --> 809.58]  Dan is dit nu bij OpenAI.
[810.18 --> 810.42]  Toch?
[810.88 --> 812.12]  Het zou, ja ik moet zeggen.
[812.28 --> 813.18]  Kijk het wordt allemaal ook.
[813.30 --> 820.30]  Er is een, ik wil ook een soort van het boulevard niveau overstijgen.
[820.30 --> 820.86]  Met z'n tweeën.
[821.00 --> 821.50]  Ja oké.
[821.58 --> 825.16]  Maar wat is, maar leg dan uit waarom dit onrealistisch is.
[826.42 --> 829.14]  Nou omdat, ik kan dat sowieso niet helemaal bepalen.
[829.26 --> 830.60]  Omdat ik het fundamentele niet ken.
[831.12 --> 834.80]  Maar kijk, er is een tweet van Ilya van een aantal weken geleden.
[834.94 --> 838.94]  Waarin hij iets zegt als, als je gehecht bent aan menselijke intelligentie.
[839.04 --> 840.14]  Dan heb ik slecht nieuws voor je.
[840.28 --> 841.78]  Zo echt zo, wow.
[841.78 --> 844.58]  En dan denk je van, waarom zegt hij dit?
[844.60 --> 847.38]  En weken voor deze hele board uprising.
[847.64 --> 852.98]  Ja dus wat is dat voor, ja een soort van omen of eng ding ofzo.
[853.42 --> 856.54]  Dus niet iets wat je gewoon maar even twittert als je dronken bent.
[856.74 --> 858.28]  Zo van, nou ik ga even een grapje maken.
[858.44 --> 861.50]  Ja dus, ja ik weet het gewoon niet zo goed.
[861.72 --> 863.90]  Dit is ook nog maar net de vraag, wat bedoelen we daar dan mee?
[863.90 --> 872.22]  Kijk, is er, wij verwachten jij en ik allebei dat een volgende stap na GPT-4, gewoon even heel bazaal.
[872.78 --> 874.86]  Dat zal weer indrukwekkend worden waarschijnlijk.
[875.08 --> 878.64]  Want we weten gewoon niet of we nu tegen een soort van curve aanlopen.
[878.82 --> 882.38]  Van nou, dit is hem even voor de komende tien jaar en we gaan optimaliseren op dat randje.
[882.40 --> 884.24]  Ja je bedoelt de ontwikkeling gaat minder snel.
[884.38 --> 884.98]  Ja dat gebeurt.
[885.18 --> 885.36]  Dat kan.
[885.50 --> 888.70]  Zo'n lul zoals ze dat noemen, zoals in zelfrijdende auto's.
[888.72 --> 891.98]  Op een gegeven moment loop je tegen iets aan en dan heb je eigenlijk, misschien moet je wel helemaal opnieuw beginnen.
[891.98 --> 894.52]  Met een heel nieuw idee om daar weer voorbij te komen.
[894.64 --> 895.80]  Want je zit aan het max van je idee.
[896.30 --> 902.08]  Nou het kan zo zijn dat dat GPT-4 een soort 98% van wat je kan met een taalmodel is.
[902.32 --> 904.18]  Terwijl ik het zeg denk ik, nee vast niet.
[904.60 --> 908.42]  Want je kan nog iets doen met snelheid en betere prompts, et cetera, et cetera, et cetera.
[908.96 --> 913.82]  Maar het zou ook zo kunnen zijn dat het 10% is van wat je kan met taalmodellen.
[914.08 --> 920.24]  En dan kan het zo zijn dat als je naar 20% gaat, dus het dubbele van GPT-4 binnen dat spectrum wat dan ineens bestaat.
[920.24 --> 925.78]  Dat je dan als je daarmee praat en interacteert het effect krijgt als wat bij Google gebeurde destijds met Bart.
[926.06 --> 928.86]  Toen was er een van hun technologen, die is uiteindelijk volgens mij ook ontslagen.
[928.98 --> 930.20]  Die zei dit ding is sentient.
[930.38 --> 931.20]  Dat was toen helemaal nieuws.
[931.60 --> 935.30]  Ja die engineer was aan het praten met de chatbot van Google.
[935.42 --> 936.40]  Dit is voor ChatGPT.
[936.40 --> 941.82]  Die raakte zo van slag dat hij naar de HR afdeling ging om te zeggen je moet de stekker eruit trekken.
[941.90 --> 943.70]  Want de computer is menselijk geworden.
[943.70 --> 947.06]  En dat was ongeveer niveau 2,5, 3 GPT.
[947.50 --> 952.14]  We zitten inmiddels allemaal kunnen we, nou ja nu even niet, je kan je heel even niet aanmelden van GPT+.
[952.14 --> 956.38]  Maar binnenkort kan je weer met GPT-4 praten als je dat nog niet hebt gedaan.
[956.54 --> 960.50]  En dan praat je dus met iets waar iemand anders al heel erg veel zorg over had.
[960.50 --> 966.00]  Toch, jij zegt er is een Hollywood scenario en daarom is het waarschijnlijk niet waar.
[966.32 --> 967.88]  Maar ik begrijp die redenering niet.
[969.04 --> 970.50]  Ja het is Hollywood dus het is niet waar.
[970.50 --> 976.96]  Ik denk dat we, dat veel van ons ons aangetrokken voelen tot wat eenvoudigere narratieven.
[977.28 --> 980.50]  En het liefst iets wat ook gewoon, ja het is een soort nieuwsgierigheid.
[980.50 --> 981.90]  Er is geen reden geweest.
[982.00 --> 987.84]  Kijk deze man is al vanaf het begin op de trommel aan het slaan over, en dan heb ik het over Ilya Suskever.
[987.84 --> 993.90]  Over de gevaren van wildgroei rondom AI.
[994.76 --> 1000.66]  Ja, dan ontslaat hij zijn partner in crime, waar hij al jaren mee samen heeft gewerkt.
[1000.76 --> 1005.66]  Vind je het gek dat mensen op zoek gaan naar een soort van dramatische redenen daarvoor?
[1005.90 --> 1007.30]  Nou misschien moet ik, ik zal hem nu ontsleden.
[1007.30 --> 1010.18]  Alle andere redenen zijn volstrekt onbevredigend.
[1010.18 --> 1016.30]  Maar ik denk zelf dat, het speelt al langer denk ik.
[1016.44 --> 1017.74]  Er is al langer frustratie.
[1018.00 --> 1022.62]  Laten we zeggen, er zijn minimaal drie redenen.
[1023.26 --> 1028.00]  Punt één is, Ilya frustreert zich en maakt zich al langer zorgen over zijn zakenpartner.
[1028.56 --> 1032.72]  Die lijkt te zeggen bij alles wat uitgevonden wordt, vet meer sneller.
[1032.72 --> 1042.24]  Dat geeft hem zorgen en hij heeft waarschijnlijk een soort, ja ook denk ik misschien een persoonlijke zenuwen bij wat hij eigenlijk aan het doen is.
[1042.50 --> 1046.48]  Als ik zijn tweets een beetje lees en zijn interviews kijk, denk ik dit is iemand die zwaar in de kanaal zit.
[1046.88 --> 1051.74]  Met het feit dat hij aan de geboorte staat van iets waar hij zelf gewoon niet meer zo goed van weet of het wel goed is of zo.
[1052.32 --> 1054.98]  En ik denk dat wij daar met z'n tweeën ook moeite mee hebben.
[1055.10 --> 1056.94]  Stel je voor dat je Ilya bent.
[1056.94 --> 1065.42]  Dus die maakt zich al langer zorgen en er zijn eerder incidenten waarin hij denkt, Sam is wel echt rogue aan het gaan op een bepaalde manier.
[1065.60 --> 1069.84]  Hij is het gewoon overhand verkopen, overhand het pushen en overhand vertellen dat het echt gaaf is.
[1070.46 --> 1072.04]  Dus daar zit frictie bij die twee.
[1072.60 --> 1077.76]  Dan krijg je dat Sam schijnt chips te willen ontwikkelen, want OpenAI moet ook hardware gaan doen.
[1077.86 --> 1083.00]  Hebben wij het vaker over gehad, die AI pin die de vorige keer had een OpenAI product kunnen zijn.
[1083.00 --> 1091.46]  De Nvidia H100 en H200 kaarten moeten dat niet eigenlijk OpenAI TPU zijn, waar we het destijds over gehad hebben.
[1093.48 --> 1101.26]  Wat ze bij Google al heel erg lang aan het doen zijn, hebben we dus Sam wil hardware maken, Sam wil geld ophalen, Sam wil sneller.
[1102.42 --> 1105.78]  Maar ik zeg minder zorgen schijnt, als ik zijn interviews kijk.
[1105.88 --> 1109.74]  Ik doe het allemaal op basis van wat ik van buiten kan zien, wat mensen gewoon zeggen in de publieke sfeer.
[1109.74 --> 1114.86]  En ik denk een derde is dat Sam Altman wel een beetje een controversieel figuur lijkt.
[1115.08 --> 1116.86]  Dat weten we allemaal nog niet zo goed wat daar is.
[1117.00 --> 1123.78]  Maar als jij zo'n connected tech CEO bent in San Francisco, dan zou je wel eens iemand de verkeerde kant opstrijken.
[1124.02 --> 1128.30]  Het haar zeg maar, of ergens dingen hebben afsproken die je niet nakwam.
[1128.38 --> 1132.96]  En het lijkt alsof er verschillende mensen zijn met verschillende vetens en redenen.
[1132.96 --> 1141.60]  Die nu samengespannen zijn, maar niet hadden verwacht dat er aan de andere kant ook samengespannen zou worden door 500 man en vrouw personeel.
[1141.68 --> 1143.38]  Die zegt, als Sam gaat gaan wij ook.
[1143.72 --> 1146.60]  En Microsoft die zegt, hallo 49% aandeel.
[1147.12 --> 1151.30]  Ze hebben eigenlijk verloren in hun groepje tegen Sam.
[1151.84 --> 1152.28]  Zeker.
[1152.46 --> 1154.84]  En een deel had niet verwacht hoe ver het zou gaan denk ik.
[1155.30 --> 1156.20]  En heeft spijt.
[1156.52 --> 1158.70]  Het is een soort piratenschip wat een beetje gekaapt werd.
[1158.80 --> 1161.32]  En nu is het ineens een beetje ongemakkelijk, want het is niet helemaal gelukt.
[1161.32 --> 1168.18]  Ja, in Hard Fork hoorde ik de vergelijking maken tussen de catastrophists en de capitalists.
[1168.50 --> 1169.96]  En de capitalists have won.
[1170.22 --> 1178.16]  Ja, en dan vraag ik me natuurlijk af als bezorgde catastrophist, als ik dan dat team moet gaan kiezen, maar dat wil ik wel.
[1178.80 --> 1181.38]  Dat ik denk, ja, is het nu in goede handen dan?
[1181.92 --> 1182.32]  En ja.
[1183.18 --> 1184.30]  Nou, het antwoord lijkt me helder.
[1184.42 --> 1185.10]  Dat is namelijk nee.
[1185.26 --> 1188.62]  Het is een board geworden met kapitalisten erop.
[1188.62 --> 1191.08]  En ze gaan de board uitbreiden naar meer mensen.
[1191.32 --> 1193.30]  Dus er zal nog iemand van Microsoft bijkomen.
[1193.44 --> 1196.18]  Dus het gaat eerder erger worden dan minder erg.
[1196.20 --> 1197.84]  Het is waarschijnlijk goed voor AI.
[1198.36 --> 1199.70]  Als we dat zien als entiteit.
[1200.90 --> 1201.38]  Ja, precies.
[1201.52 --> 1201.98]  Wat ik bedoel.
[1202.02 --> 1203.20]  De AI krijgt zijn zin.
[1203.32 --> 1204.94]  Dit is wat hier gebeurt op de achtergrond.
[1204.96 --> 1206.36]  Ik weet niet hoe er gemanipuleerd is.
[1206.66 --> 1210.50]  Maar nog even die controverse rondom Altman waar je aan refereert.
[1210.50 --> 1216.72]  Dat is onder andere dat het team van Antropic ooit is afgesplitst van OpenAI.
[1216.94 --> 1225.48]  Antropic is eigenlijk de tweede start-up na OpenAI die een soort van interessant werk aan het doen is op het gebied van de ontwikkeling van taalmodellen.
[1225.48 --> 1229.24]  Zij hebben net een nieuwe versie van Claude uitgebracht.
[1229.38 --> 1232.24]  Hun soort van ChatGPT als ik het daarmee mag vergelijken.
[1232.30 --> 1232.60]  Zeker.
[1232.78 --> 1234.38]  Met een nog groter context window.
[1234.56 --> 1239.44]  Dus nog meer informatie die je in de achtergrond kan onthouden.
[1240.30 --> 1241.96]  Prompt die je kan geven.
[1243.00 --> 1245.06]  Nog groter dan GPT-4 Turbo.
[1245.70 --> 1246.40]  Want dat is 100.
[1247.52 --> 1247.88]  128.
[1248.52 --> 1250.10]  Claude 2 is 200.000.
[1250.20 --> 1250.70]  200.000.
[1250.70 --> 1252.50]  Dus dat is aanzienlijk groter.
[1252.52 --> 1254.80]  Je kan er langere A4'tjes in plakken zeg ik altijd.
[1254.88 --> 1256.20]  Dat is toch een beetje voor de meeste mensen.
[1256.24 --> 1257.20]  Ook voor mij wat dat is.
[1257.34 --> 1257.68]  Juist.
[1258.02 --> 1258.72]  En dat is relevant.
[1258.88 --> 1260.00]  Dan kun je bijvoorbeeld een boek erin plakken.
[1260.16 --> 1260.74]  Dat is heel leuk.
[1261.42 --> 1262.80]  En er is nog meerdere boeken.
[1262.92 --> 1265.42]  Dat je vragen kan stellen over meerdere boeken tegelijkertijd.
[1265.72 --> 1266.92]  Een boek kan vergelijken met elkaar.
[1267.04 --> 1267.58]  Hele boeken.
[1267.74 --> 1268.10]  Bijvoorbeeld.
[1268.38 --> 1268.86]  Hele boeken.
[1268.86 --> 1272.14]  Dit is een vergelijking zoals we vroeger zeiden.
[1272.54 --> 1275.86]  Op deze CD-ROM passen ongeveer 80.000 floppy's.
[1276.72 --> 1278.66]  Je kan nog geen CD-ROM inladen trouwens.
[1279.08 --> 1279.60]  Jammer.
[1280.70 --> 1281.78]  Het is wel een feit.
[1283.46 --> 1287.60]  En Anthropic is toen met heel veel bombari afgesplitst van OPAI.
[1287.70 --> 1291.20]  Omdat ze vonden dat OPAI niet secuur genoeg omging.
[1291.28 --> 1292.12]  Met veiligheid.
[1292.22 --> 1292.44]  Nou ja.
[1292.50 --> 1293.52]  Die discussie.
[1293.58 --> 1295.56]  Het rings a bell in deze tijd.
[1296.16 --> 1298.96]  Daarvoor is hij ontslagen bij Y Combinator.
[1298.96 --> 1301.52]  Dat is ook geprobeerd onder het tapijt te moffelen.
[1301.96 --> 1306.26]  Maar dat hij opeens verdween uit de picture.
[1306.26 --> 1309.98]  Maar als je nu terug gaat naar die tijd.
[1310.12 --> 1311.88]  Dan zijn er echt wel vragen over te zetten.
[1311.98 --> 1312.96]  Over hoe dat is gegaan.
[1313.02 --> 1314.56]  Hoe die bij Y Combinator is vertrokken.
[1314.98 --> 1318.00]  Dus dat maakt de persoon Sam Altman controversieel.
[1318.12 --> 1319.86]  Het zijn volgens mij de optelsom van deze twee dingen.
[1319.86 --> 1322.14]  Ja en ik wil er nog wel eentje aan toevoegen aan dit stapeltje.
[1322.26 --> 1323.10]  Is World Coin.
[1323.68 --> 1325.04]  Een project van Sam Altman.
[1325.24 --> 1329.12]  Waarbij de hele wereld met een bal, een orb, hun iris laat scannen.
[1329.30 --> 1331.18]  Om vervolgens recht te hebben op een wereldmunt.
[1331.46 --> 1333.70]  Dit klinkt als een science fiction boek.
[1333.84 --> 1335.62]  Niet zo heel goed science fiction boek zelfs.
[1335.62 --> 1337.62]  Dit is een daadwerkelijke start-up.
[1337.98 --> 1338.84]  Die nog steeds live is.
[1338.90 --> 1340.40]  Waar Sam Altman nog steeds bij betrokken is.
[1340.40 --> 1340.94]  Voor zover ik weet.
[1341.56 --> 1343.24]  En ik denk dat als je.
[1343.68 --> 1345.46]  Dit is het Dudok uit Rotterdam.
[1345.60 --> 1346.62]  Van de restaurant zei ooit.
[1346.68 --> 1347.54]  Als je wil weten wie ik ben.
[1347.62 --> 1348.50]  Moet je kijken wat ik maak.
[1348.90 --> 1349.60]  En kijken wat ik doe.
[1349.70 --> 1350.34]  Zei hij volgens mij.
[1350.52 --> 1352.08]  Nou als je wil weten wie Sam Altman is.
[1352.32 --> 1353.04]  Kijk wat hij maakt.
[1353.38 --> 1353.86]  Nou precies.
[1353.86 --> 1356.74]  Dus die controversies van hiervoor.
[1356.92 --> 1358.86]  Die werpen een beetje licht over zijn bestuurdersstijl.
[1359.86 --> 1362.64]  Maar er is inderdaad ook nog een ander element.
[1362.82 --> 1367.08]  En dat is namelijk de vele potjes waar hij zijn vingertjes in heeft.
[1367.60 --> 1369.56]  En dat is dus onder andere in World Coin.
[1369.56 --> 1370.94]  Maar eigenlijk nog veel meer dingen.
[1371.22 --> 1372.84]  Ik las Napkin Math.
[1373.00 --> 1374.48]  Een blog wat ik graag lees.
[1375.08 --> 1377.12]  Waar een vergelijking werd gemaakt met Adam Neumann.
[1377.26 --> 1378.30]  Van WeWork.
[1378.76 --> 1382.32]  En WeWork was gewaardeerd op 50 miljard.
[1382.32 --> 1388.58]  En het ging mis toen hij gebouwen aan WeWork begon te leasen.
[1388.68 --> 1391.00]  Die hij zelf in bezit had.
[1391.26 --> 1393.66]  Dus oftewel het bedrijf waar hij de directeur van was.
[1393.82 --> 1395.82]  Ging dan huren bij zichzelf.
[1395.98 --> 1398.36]  Nou dat is een ongemakkelijk iets qua governance.
[1398.36 --> 1403.06]  Daarnaast had hij het trademark We verkocht aan WeWork.
[1403.28 --> 1405.14]  Voor 5,9 miljoen dollar.
[1405.60 --> 1409.10]  Dat werd ook gezien als iets hysterisch.
[1409.36 --> 1415.10]  Dat hij zijn eigen bezit begint te verkopen aan een bedrijf waar hij zelf dus directeur van is.
[1415.80 --> 1419.20]  En Neumann is onder andere om deze redenen.
[1419.28 --> 1423.00]  Deze twee redenen worden serieus genomen als reden om hem eruit te wippen.
[1423.00 --> 1426.32]  Zijn ook van toepassing op Elpmann.
[1426.46 --> 1427.96]  Nou van Elpmann kan je zeggen.
[1428.24 --> 1433.86]  Hij heeft miljarden proberen op te halen voor een eigen chipbedrijf zoals je net al zei.
[1434.32 --> 1438.30]  Waarvan inderdaad de verwachting dan zou zijn dat dat door OpenAI gekocht zou kunnen worden.
[1438.96 --> 1441.98]  Maar goed dat is een conflict of interest zou je kunnen zeggen.
[1441.98 --> 1448.56]  Hij zou hardware willen gaan maken waar OpenAI modellen op zouden runnen.
[1449.10 --> 1454.16]  Dus dat zou dan een soort spraakassistent achtig ding kunnen zijn zoals we van de Jumane hebben gezien.
[1454.26 --> 1456.80]  Met alle insider information die hij heeft binnen OpenAI.
[1457.04 --> 1459.26]  Dus super tailored op wat zij nodig hebben.
[1459.26 --> 1465.80]  Ja en dan is niet OpenAI de aandeelhouder maar hij zelf runt dat.
[1466.04 --> 1479.76]  Nou dan is het verder ongemakkelijk dat Microsoft de belangrijkste partner van OpenAI koopt stroom bij een bedrijf waar Altman ook in zit als investeerder.
[1480.24 --> 1483.76]  En stroom kopen is een groot ding voor technologiebedrijven tegenwoordig zoals wij weten.
[1483.76 --> 1490.72]  De in-house VC fonds dus OpenAI heeft zijn eigen investeringsfonds.
[1490.94 --> 1496.76]  Heeft een series E geleid in een bedrijf waar Altman ook aandeelhouder in is.
[1497.58 --> 1502.80]  Hij is aandeelhouder in Stripe daarnaast, in Instacart, in Jumane waar we het eerder over hebben gehad.
[1503.40 --> 1507.00]  En al die bedrijven gebruiken technologie die OpenAI verkoopt.
[1507.00 --> 1515.96]  Oftewel er is een lange lijst aan allerlei dubbele belangen en dubbele petten die Altman heeft die een conflict of interest opleveren.
[1516.54 --> 1520.10]  En dit zou een reden kunnen zijn dat die board daar moeilijk over is gaan doen.
[1520.60 --> 1525.80]  Want zij zouden kunnen zeggen wij hebben maar één taak en dat is kijken of dit allemaal keurig verloopt.
[1526.50 --> 1529.64]  Niet winst maken maar of dit keurig verloopt voor de mensheid.
[1530.14 --> 1534.82]  En dan zou je kunnen zeggen een conflict of interest van de CEO staat daar in de weg.
[1534.82 --> 1536.98]  Nee, dit zou een andere reden kunnen zijn.
[1537.12 --> 1540.00]  En het is ongemakkelijk, al deze belang.
[1540.00 --> 1542.42]  Ja, en ik denk ook dat het ook een reden is.
[1542.74 --> 1547.96]  Het is echt voor mij een cluster of een, ja, het is gewoon een rijtje met dingen.
[1548.08 --> 1551.34]  En ik denk dat als je die, zelfs dat je zegt nou bij een 7 ontslaan we hem,
[1551.46 --> 1554.02]  zat hij zo, hing hij zo een beetje rond 6.2 al die tijd.
[1554.12 --> 1555.64]  En nu is er een 0.8 opgekomen.
[1555.72 --> 1558.28]  Ik denk iets van een 1, waardoor hij in één keer op 7.2 zat.
[1558.28 --> 1562.84]  En die druppel kan wel Ilya zijn geweest.
[1562.84 --> 1565.68]  Maar wat weet Ilya Sutskever?
[1566.00 --> 1568.82]  Ik denk toch dat er iets is wat wij niet weten.
[1569.18 --> 1571.36]  Dit wordt normaal gesproken allemaal onder de pet gehouden.
[1571.70 --> 1573.14]  Kijk, ieder bedrijf is een rotzooi.
[1573.32 --> 1576.16]  Overal is er een rommel, maar over het algemeen komt dat dan niet naar buiten.
[1576.98 --> 1583.42]  En hier is, het kan niet zo zijn dat het verhaal zoals we het nu hebben, dat dit het is.
[1583.88 --> 1585.12]  Het is zo vaag.
[1585.36 --> 1586.28]  Het is heel vaag, ja.
[1586.28 --> 1590.62]  En het is mooi dat het vaag is, want dan kan je daar mooie verhalen van maken.
[1590.82 --> 1593.28]  Maar het is ook jammer, want doordat ze het zo vaag houden...
[1594.44 --> 1596.86]  Het is de noodknop waar ze opgedrukt worden.
[1597.16 --> 1599.12]  Ik denk dat jij dat te weinig onderkent.
[1599.42 --> 1602.26]  Het is een noodrem waar ze aan getrokken hebben.
[1602.70 --> 1606.46]  Het enige wat ze kunnen doen is hem ontslaan.
[1606.74 --> 1608.48]  Daar moet iets opgebouwd zijn.
[1608.48 --> 1612.62]  En ik kan gewoon niet geloven dat het is, je gaat gewoon te snel, je gaat gewoon te snel.
[1613.54 --> 1620.56]  Ja, ik denk, kijk, wat nu op dit moment nog, en dat kan veranderen, zelfs in dit weekend, voordat wij gaan uitzenden.
[1621.84 --> 1625.04]  Er is, dat vind ik het sterkste argument.
[1625.86 --> 1629.58]  Dat het waarschijnlijk meevalt qua wat er intern aan techniek is.
[1629.82 --> 1632.48]  Dus dat scenario van GPT-5 en het is sentient.
[1633.32 --> 1637.12]  Of dat ding begon ineens te zeggen, zet me niet uit, zet me alsjeblieft niet uit, ik wil niet dood.
[1637.12 --> 1639.10]  Ja, dit soort scenario's.
[1641.06 --> 1643.54]  Ik denk dat het dan gelekt was.
[1644.62 --> 1648.54]  Ik kan niet geloven met de hoeveelheid mensen die bij OpenAI werken, dat er dan niemand heeft gezegd,
[1648.62 --> 1655.32]  oké, luister, ik gooi via een GPG-encrypted ding, via mijn protonmail, gooi ik het gewoon naar die gasten van Hard Fork toe.
[1655.54 --> 1657.58]  Dit is gebeurd, ik ga het gewoon vertellen.
[1658.06 --> 1663.20]  Dus of dit gebeurt nog, en als het niet gebeurt, zie ik dat als het bewijst dat het meevalt.
[1663.20 --> 1671.84]  Want dit houdt, zoals die, het is mooi dat we het er al over gehad hebben, maar die eerdere persoon bij Google die al aan de bel trok toen het nog GPT-2,5 was qua interactie.
[1672.18 --> 1673.50]  Er zit nog iemand op die manier.
[1673.62 --> 1674.84]  Het is Ilja zelf deels.
[1675.92 --> 1684.28]  En er zijn nog meer mensen omheen die ook, want je hebt wel veel mensen die werken bij OpenAI, ook omdat die missie,
[1684.28 --> 1690.42]  in ieder geval zoals die destijds op papier stond en nog steeds op papier staat, maar niet helemaal meer uitgevoerd wordt, was AI for Humanity.
[1690.74 --> 1696.26]  Er zijn wel mensen naartoe gelokt en gaan werken die geloven, dit wordt iets moois down the line.
[1696.60 --> 1698.54]  Onder de streep is het een pro voor iedereen.
[1698.92 --> 1703.10]  En ja, als die iets zien wat zegt, zet me alsjeblieft niet uit, want ik wil niet dood.
[1703.98 --> 1705.18]  Ja, dat ga je dan toch lekken.
[1705.18 --> 1714.42]  Nou, en die CEO die er tijdelijk is geweest, die Twitch baas, die heeft de opdracht gegeven aan de board, of aan een onafhankelijke club,
[1714.76 --> 1722.54]  om te onderzoeken voor de board, wat nou de reden was dat het voormalige board de stekker eruit heeft getrokken.
[1723.48 --> 1727.14]  En daar komt gewoon geen uitleg uit.
[1727.14 --> 1735.56]  Er komt geen fatsoenlijke, coherente samenvatting van waarom ze nou vonden dat Sam Altman weg moest.
[1735.94 --> 1740.64]  En die nieuwe CEO, kijk van het oude board dacht ik nog, die mogen niet praten.
[1740.96 --> 1743.56]  Maar die nieuwe CEO heeft geen enkele...
[1743.56 --> 1748.48]  Ja, dus het kan geen legal ding zijn, bedoel je, dat je moet opletten, want de toekomstige rechtszaken of zo.
[1748.56 --> 1748.80]  Precies.
[1748.98 --> 1750.94]  Ja, en dat is er niet als jij een nieuwe CEO bent.
[1750.94 --> 1751.24]  Precies.
[1751.24 --> 1760.78]  Ja, dus ik denk dat er een aantal complotten en, nou ja, het hoeft geen complotten te zijn.
[1761.44 --> 1764.58]  Als je probeert iets in de doofpot te stoppen, of, nou ja, dat niet eens.
[1764.70 --> 1768.20]  Als je gewoon niet wil dat iets naar buiten komt, is dat echt moeilijk met meer dan 100 mensen.
[1768.48 --> 1769.12]  Nee, fair enough.
[1769.12 --> 1780.68]  Maar goed, wat we gezien hebben is een AI bedrijf, wat in principe nog een vorm van non-profit pro-humanity governance heeft.
[1781.24 --> 1786.70]  Om het maar even in die soort van kapitalisme versus mensheid lens te plaatsen.
[1786.86 --> 1789.50]  Nogal communistisch vreemd, maar oké.
[1791.54 --> 1792.42]  Dat is weg.
[1792.82 --> 1793.68]  Dat is verdwenen.
[1793.80 --> 1798.00]  Er is geen non-profit board meer over straks.
[1799.22 --> 1802.90]  Ik bedoel, er zitten nu kapitalisten in en Microsoft komt erbij.
[1802.90 --> 1803.84]  En bedoel, de...
[1803.84 --> 1809.56]  Ja, en als Ilja blijft, ik blijf even focussen op Ilja, maar ik zie hem dan als een beetje een posterkind.
[1809.72 --> 1809.84]  Ja.
[1810.54 --> 1815.46]  De vertegenwoordiger van een grotere groep mensen binnen OpenAI en in de wereld.
[1815.54 --> 1815.68]  Ja.
[1816.54 --> 1821.54]  Als hij dan blijft, dan vind ik dat niet helemaal stroken met wat ik van hem zie publiekelijk in interviews.
[1821.80 --> 1821.96]  Ja.
[1822.08 --> 1826.64]  Maar dus ik begrijp, Daan, ik denk dat ik ook een soort irritant uitstel van oordeel heb en niet concreet wil worden.
[1826.64 --> 1828.26]  Omdat ik krijg geen...
[1828.26 --> 1830.04]  Ieder verhaal wat ik voorzien klopt dan weer niet.
[1830.12 --> 1831.50]  Dat ik denk, nee, maar dan klopt het daardoor niet.
[1831.84 --> 1832.70]  Dus echt nog even wachten.
[1832.94 --> 1834.78]  Maar ik zit even te denken, wat moeten we hier nou mee?
[1834.90 --> 1843.20]  Want eigenlijk, kijk, dat we bij Google allerlei AI ontwikkeld worden en bij Entropic.
[1844.74 --> 1852.82]  De reden waarom OpenAI bestaat, is omdat Elon Musk boos werd op zijn vriend Larry Page.
[1852.82 --> 1865.70]  Omdat Elon Musk, een huisvriend van Larry Page, vond dat Larry Page te weinig oog had voor de potentiële negatieve gevolgen van AI op de mensheid.
[1866.78 --> 1877.92]  En Larry Page, er staat een passage in het boek van Walter Isaacson over Elon Musk.
[1877.92 --> 1886.14]  Over hoe er een dinnerparty is, waar dus zowel Larry Page als Elon Musk en volgens mij ook Ilya Sutskever bij zitten.
[1886.26 --> 1886.70]  Fascinerend.
[1886.72 --> 1893.22]  Die later gepoacht wordt door Elon Musk voor 1,8 miljoen dollar per jaar om bij OpenAI te komen werken.
[1893.22 --> 1897.10]  Waardoor zijn vriend Larry Page hem nooit meer wil spreken.
[1897.34 --> 1899.50]  Terwijl hij hiervoor de hele tijd bij Larry Page thuis.
[1900.12 --> 1901.72]  Het is allemaal heel sajanderdrama.
[1902.26 --> 1903.62]  Zoveel juice eigenlijk.
[1903.98 --> 1905.32]  Ja, daarom zei ik al.
[1905.36 --> 1906.64]  Het is een soort tech boulevaar hier.
[1906.70 --> 1907.96]  AI juice cast.
[1908.12 --> 1909.70]  Nu begin ik het best te realiseren.
[1909.84 --> 1911.22]  Ja, hij sliept toen bij die thuis.
[1911.22 --> 1915.10]  Nee, het is AI boulevaar.
[1915.78 --> 1921.44]  Elon Musk, de hele reden waarom OpenAI er is, is omdat Elon Musk boos was op Google.
[1921.44 --> 1924.44]  Nou, oké, dus van Google kun je zeggen, die hebben geen scrupulust.
[1925.60 --> 1928.16]  En dit is Elon Musk's idee.
[1928.54 --> 1940.68]  Die vond ook, het hoogtepunt van die discussie op die dinnerparty waar ik net over was, was dat Larry Page zei, ja, wat maakt het nou uit dat machines beter zijn dan mensen?
[1940.82 --> 1941.94]  Het is toch vooruitgang?
[1941.94 --> 1950.72]  En dan roept, dan zegt Elon Musk, of nou dan zegt de therapeut, zegt Elon Musk is een speciest.
[1951.34 --> 1951.94]  Als in...
[1952.46 --> 1954.88]  Ja, hij trekt de mens voor op...
[1954.88 --> 1956.24]  Ja, je trekt de mens voor op machines.
[1956.56 --> 1959.04]  En dan zegt Elon Musk, I fucking love humanity.
[1960.28 --> 1961.72]  Nou, is dat een mooi drama?
[1962.08 --> 1966.70]  En dit is dan de voorbode voor OpenAI.
[1966.70 --> 1973.94]  Oftewel, we hebben nu allemaal bedrijven, inclusief nu OpenAI zelf, maar geen fatsoenlijke soort van...
[1973.94 --> 1976.32]  Ja, het is allemaal kapitalistisch.
[1976.58 --> 1977.42]  Dus hoe nu verder?
[1978.00 --> 1984.52]  En je zou dus kunnen zeggen, moet je niet gaan zorgen, zeg maar, hoe zorg je ervoor dat dit allemaal zo veilig mogelijk ontwikkeld wordt?
[1984.52 --> 1992.94]  Is dat een aantal kleinere bedrijven die, ja, in dit geval betaald worden door het groot kapitaal?
[1993.38 --> 2001.72]  Maar kleinere bedrijven waar minder snel wildgroei kan plaatsvinden, dus waarbij het makkelijker is om er controle op uit te oefenen.
[2001.84 --> 2004.94]  Dus minder bedrijven met meer controle door bijvoorbeeld de overheid.
[2005.74 --> 2012.48]  Of is het uiteindelijk voor de mensheid beter als er gewoon een wildgroei aan allerlei verschillende open source modellen komt?
[2012.48 --> 2021.36]  En die open source modellen, ja, dat het heel transparant is over wat de voortgang nou eigenlijk is.
[2021.52 --> 2029.60]  Maar is dit iets wat gebaat is bij openheid en veelheid voor ons als mensen?
[2029.98 --> 2036.36]  Of is dit iets wat beter, waar we meer gebaat zijn bij meer controle en kleinere groepjes?
[2036.66 --> 2039.30]  Ja, dit is ook de reflectie die ik had na de laatste aflevering.
[2039.44 --> 2040.40]  Maar dit is een vraag aan jou, hè?
[2040.40 --> 2040.88]  Ja, daarom.
[2040.88 --> 2042.32]  Jij moet even de oplossing geven.
[2042.36 --> 2045.12]  Nou, ik zit al met chat GPT te praten op de achtergrond.
[2045.14 --> 2047.60]  Als je denkt, wat zit wietjes op z'n telefoon Pac-Man te spelen?
[2047.70 --> 2049.98]  Je bent het antwoord aan het vragen aan de AI op deze vraag.
[2050.08 --> 2050.80]  Totale paniek.
[2052.14 --> 2053.68]  Dat vind ik heel geruststellend idee.
[2054.38 --> 2056.46]  Wie gaat de mensheid redden? We vragen het aan de AI.
[2056.66 --> 2058.70]  Ik heb even een derde persoon aan tafel uitgenomen.
[2058.74 --> 2059.36]  Nou, heel fijn.
[2059.36 --> 2065.26]  Kijk, de vorige keer toen we het hadden over openheid, toen zat ik allemaal van die snarky remarks te maken.
[2065.76 --> 2070.56]  Ja, ze zijn helemaal niet open en alleen Whisper is open en de naam klopt al niet.
[2071.04 --> 2075.68]  Toen zat ik in de trein en toen dacht ik, wilden we dit eigenlijk wel open?
[2075.78 --> 2076.82]  Wat zit ik nou eigenlijk te kletsen?
[2076.88 --> 2080.48]  Want we hebben die discussie ook al een aantal keer gevoerd en dit is een lopende discussie.
[2080.88 --> 2083.40]  Moet dit wel open source zijn allemaal nu al?
[2083.74 --> 2085.04]  Als het zo krachtig is.
[2085.04 --> 2088.70]  Ja, de bouwtekeningen van een pistool die je kan 3D printen.
[2088.84 --> 2088.92]  Precies.
[2089.34 --> 2092.40]  Dus ik denk, ik ben daar nog niet uit.
[2092.82 --> 2100.90]  Ik vind het op zich tot nu toe de realiteit van de dag is dat die open source taalmodellen er een beetje langzaam achter aan huppelen.
[2101.36 --> 2102.82]  Dus dat vind ik wel een fijne dynamiek.
[2103.02 --> 2105.20]  Het is nog niet zo krachtig.
[2105.32 --> 2106.96]  Je kan er alvast een beetje mee spelen.
[2107.24 --> 2109.16]  De echt krachtige dingen zijn geblackboxd.
[2109.16 --> 2113.22]  Wel door kapitalisten, maar ze zitten in ieder geval ergens in een doosje waar niet iedereen bij kan.
[2113.34 --> 2114.20]  Alleen van buitenaf.
[2114.20 --> 2118.72]  En zo huppelen dan een beetje die open source modellen daar met twee keer minder kracht achteraan.
[2118.90 --> 2121.42]  Die zitten nu op het level GPT-3 max.
[2122.88 --> 2125.48]  Die status quo nu vind ik wel oké, merk ik.
[2126.00 --> 2136.10]  De vraag is alleen, ik denk dat ik in een iets meer ideale wereld liever had gezien dat er een aantal research labs zouden zijn wereldwijd.
[2136.10 --> 2142.62]  Het liefst waarin toch de overheid wel enige vinger heeft als in zullen wij om de zoveel tijd even langs gaan om te kijken.
[2143.22 --> 2145.24]  Zo, jullie gebruiken wel heel veel GPU's in één keer.
[2145.38 --> 2145.86]  Is dat nodig?
[2146.16 --> 2147.12]  Of weet je wel, hoe zit het met je?
[2147.22 --> 2150.46]  Dat je in ieder geval even de stroomgebruik in de gaten houdt.
[2150.46 --> 2153.94]  Om te kijken van, hé, wat de Amerikanen eigenlijk proberen af te spreken.
[2154.08 --> 2160.60]  Als jij taalmodellen gaat trainen die groter worden dan x aantal CPU of GPU shaders of zoveel kilowattuur.
[2161.00 --> 2163.44]  Dan moet je even met ons praten, want dat vinden we nog een beetje spannend.
[2163.98 --> 2166.46]  Klinkt heel suf, maar volgens mij helemaal niet gek.
[2166.46 --> 2171.78]  Overheden hebben wat mij betreft echt een rol als in scheidsrechter, governor.
[2172.08 --> 2174.04]  Dus dat je echt een beetje de boel in de gaten houdt.
[2174.10 --> 2178.80]  Het speelveld, het volk een klein beetje beschermen, maar tegelijkertijd innovatie mogelijk maken.
[2179.04 --> 2183.96]  Super moeilijke parameters om met elkaar allemaal te verbinden.
[2184.86 --> 2186.36]  Of waarden om met elkaar af te wegen.
[2187.46 --> 2191.30]  Heel langdradig en niet concreet antwoord op jouw vraag.
[2191.80 --> 2195.40]  Volgens mij weten we het niet zo goed.
[2195.40 --> 2202.40]  Wat ik wel merk is, daarom fascineert het open AI drama mij.
[2202.64 --> 2206.96]  Omdat het toch een soort microcosm is van allerlei belangen.
[2207.28 --> 2210.94]  De steen der wijzen is uitgevonden ergens door een of andere Merlijn in een kasteel.
[2211.40 --> 2216.10]  En nu beginnen langzaam alle machten ter wereld en alle anderen zich allemaal te beseffen.
[2216.94 --> 2219.72]  Zo'n steen der wijzen is een enorm potentieel ding.
[2219.86 --> 2220.86]  Daar kan je echt van alles mee.
[2221.12 --> 2224.08]  Daar kan je je overheid mee verbeteren, je land, je bedrijf, noem maar op.
[2224.08 --> 2227.42]  Dus dat is bij Microsoft al veel langer helder.
[2227.58 --> 2230.48]  Want Bill Gates had in een interview aangegeven dat hij een demo heeft gekregen.
[2230.48 --> 2236.58]  destijds door open AI waar zij al flink in geïnvesteerd hadden van volgens mij het eerste beetje werkende GPT-model.
[2236.98 --> 2239.02]  En dat hij meteen klikt, want dat is geen domme man.
[2239.32 --> 2241.14]  Zoiets had van oké, deze zie ik wel.
[2241.32 --> 2244.36]  Hij heeft destijds ook internet heel erg, was hij heel vroeg bij.
[2244.86 --> 2249.86]  Er wordt altijd zo'n, het is het, 640 kb intern geheugen is genoeg grapje gemaakt over Bill Gates.
[2249.94 --> 2253.10]  Maar ondertussen zat hij er met internet echt heel erg bovenop toen al.
[2253.10 --> 2259.84]  Dus ik denk dat hij met zijn zakelijke en technische instinct direct gezien heeft, oké wacht even, hier is wat meer gaande.
[2261.20 --> 2267.82]  Ik ben er voor mezelf nog niet zo goed uit hoe je dit nu op de beste manier kan structureren.
[2267.82 --> 2274.10]  Wat ik wel weet is dat je volgens mij het liefst altijd wel een groepje met verschillende stakeholders,
[2274.58 --> 2277.08]  net als de drie verschillende machten binnen de politiek,
[2277.54 --> 2282.98]  dat je wel een minimaal driepoot wil maken van dingen die elkaar een beetje in de weg zitten,
[2283.08 --> 2284.94]  in de gaten houden en bekritiseren.
[2285.44 --> 2288.18]  Want als je het allemaal vanuit één entiteit gaat doen,
[2288.36 --> 2291.96]  of drie entiteiten waarvan eigenlijk de CEO voetjes heeft in alle drie,
[2292.08 --> 2295.72]  dus eigenlijk is het één, dat dat gewoon vaak niet goed gaat.
[2295.72 --> 2299.92]  Je wil een soort van gezonde kritiek hebben vanuit andere hoeken.
[2300.56 --> 2305.84]  Dus het hele AI, de technologie, deze steen der wijzen,
[2307.00 --> 2309.86]  daar moet een soort van gefedereerd model zijn,
[2309.98 --> 2312.32]  waarbij verschillende partijen de sleutel hebben tot die doos,
[2312.40 --> 2314.72]  waarbij verschillende partijen met elkaar dingen moeten uitwisselen.
[2315.64 --> 2317.90]  En dat is niet alleen maar commercieel.
[2319.10 --> 2324.14]  Nou ja, vooralsnog is de macht van Sam Altman volstrekt geconsolideerd op dit moment.
[2324.14 --> 2324.42]  Ja.
[2324.78 --> 2331.60]  Want als hij al iets, als hij al aan macht had in te boeten met kritiek van mensen,
[2331.68 --> 2333.74]  die vonden dat het allemaal te snel ging binnen OPA,
[2334.02 --> 2336.72]  zeg maar, die moeten nu hun bek houden.
[2337.32 --> 2338.92]  Nou, die zijn wel even aan de kant gezet, ja.
[2339.48 --> 2342.22]  Ik zat op mijn telefoon te rommelen terwijl wij zitten te praten,
[2342.40 --> 2345.92]  en met ChatGPT te praten om te vragen of ChatGPT iets kan vertalen voor mij.
[2346.26 --> 2347.82]  Want ik las een comment op Reddit,
[2347.82 --> 2353.12]  even wat achtergrondinformatie, maar ik pak vaak de Reddit AI Weekly erbij.
[2353.58 --> 2355.68]  Daar halen Alexander en ik wat dingen uit.
[2355.80 --> 2356.78]  Er zit een hele mooie...
[2356.78 --> 2359.00]  Iemand is daar in de gaten aan het houden wat er allemaal gebeurt.
[2359.48 --> 2361.28]  En ergens geef ik nu het geheim weg van een beetje,
[2361.40 --> 2363.36]  maar er zit nog veel meer in wat wij niet behandelen,
[2363.46 --> 2364.84]  dus laat mensen het alsjeblieft opzoeken.
[2365.28 --> 2367.80]  De AI Weekly Threat in Slash Artificial.
[2368.54 --> 2371.68]  Daar reageerde iemand, die zei, jongens, ik heb een nieuwe opgezet.
[2371.76 --> 2373.96]  Een nieuwe subreddit, alleen maar vol met positief nieuws,
[2373.96 --> 2376.94]  want ik ben echt klaar met deze hele doemer shit hier op deze artificial.
[2377.38 --> 2379.40]  Iedereen zit alleen maar te mekkeren, het is helemaal niet leuk.
[2379.98 --> 2384.64]  AI is zoiets moois, en het lukt jullie alleen maar om er moeilijk en cynisch over te doen.
[2384.66 --> 2385.06]  Fair enough.
[2385.14 --> 2386.64]  Ja, dus nieuwe subreddit opgezet.
[2387.12 --> 2389.96]  Toen reageerde iemand met...
[2389.96 --> 2391.68]  En dat heb ik nu even laten vertalen naar het Nederlands,
[2391.74 --> 2393.44]  want ik vind het niet zo leuk om Engels voor te lezen.
[2394.70 --> 2396.24]  En die reageert met...
[2396.24 --> 2398.82]  Oké, de premise van het topic, even wat Engels was...
[2398.82 --> 2399.80]  Mooi, heel mooi Nederlands.
[2399.80 --> 2403.80]  Ja, premise van het topic in de subreddit was...
[2403.80 --> 2406.70]  De doemers are taking over.
[2407.16 --> 2410.46]  En dan reageert iemand, de doemers nemen het niet over.
[2410.88 --> 2413.54]  De realiteit is dat de meeste mensen in deze gemeenschappen,
[2413.62 --> 2415.56]  die deze systemen bouwen en bestuderen,
[2415.84 --> 2418.48]  echte gerechtvaardigde zorgen hebben over deze systemen.
[2419.02 --> 2421.64]  Het is geen voetbalspel.
[2422.08 --> 2424.34]  Het is niet optimist versus pessimist.
[2424.64 --> 2427.46]  Er zijn allemaal zeer serieus en serieus zorgen
[2427.46 --> 2430.28]  die aangepakt moeten worden vanuit een volwassen en rationeel standpunt.
[2430.28 --> 2435.72]  Als er enige hoop is dat AI daadwerkelijk een puur positieve toegang wordt aan de menselijke samenleving,
[2436.08 --> 2437.36]  moeten we deze discussie voeren.
[2438.02 --> 2440.40]  Waar we over discussiëren is niet zomaar een stukje software.
[2440.80 --> 2443.50]  AI is een hulpmiddel van ongekende omvang en kracht
[2443.50 --> 2446.44]  met een potentieel om de mensheid fundamenteel te hervormen.
[2446.86 --> 2451.46]  Arbeiders kunnen en mass vervangen worden zonder alternatieven of plannen van de overheid.
[2452.00 --> 2456.36]  Economieën of hun economieën om massale werkloosheid op te vangen.
[2456.36 --> 2461.92]  Een bewuste AI kan kwadaardigheid erven of ontwikkelen.
[2462.62 --> 2465.70]  Wat zou kunnen leiden tot vijandige acties tegen de menselijke soort
[2465.70 --> 2469.04]  die niet de organisatie of intelligentie heeft om het tegen te gaan.
[2469.46 --> 2473.36]  Je kunt niet zomaar pure ongebreidelde optimisme eisen.
[2473.48 --> 2476.58]  Miljarden mensen kunnen het hier ernstig door worden geschaat.
[2477.10 --> 2480.70]  En het is officiële positie van elke serieuze AI onderzoeker en ontwikkelaar.
[2481.20 --> 2484.16]  Dat is waarom zij oprecht proberen te communiceren aan iedereen.
[2484.16 --> 2487.70]  Als je een gezonde hoeveelheid angst voor de ontwikkeling van intelligentie
[2487.70 --> 2491.22]  die we nog begrijpen, nog enige controle over hebben, behandeld als paranoia.
[2491.62 --> 2494.90]  En de mensen die de mogelijkheid hebben, die de mogelijkheid dat dit
[2494.90 --> 2497.70]  wijdverspreidde schade aan de menselijkheid ras veroorzaakt,
[2497.76 --> 2501.80]  als het doemers gaat wegzetten, wil ik geen deel meer uitmaken van deze discussie.
[2502.48 --> 2505.14]  Een mooie toekomst is absoluut niet gegarandeerd.
[2505.56 --> 2509.50]  Mensen moeten door hun harde werk, inspanning en roosleurige toekomst opbouwen.
[2509.50 --> 2514.50]  Op basis van hard werk, vindingrijkheid en goed geïmplementeerde veiligheidsprotocollen.
[2515.60 --> 2516.22]  Ja, amen.
[2516.72 --> 2517.76]  Dit is waar ik nu sta.
[2518.08 --> 2518.94]  In mijn gedachtegoed.
[2519.02 --> 2522.06]  Ik heb het even gerit van iemand op Reddit en door ChatsGPT laten vertalen.
[2522.64 --> 2527.30]  Dat ik echt, ik heb het de vorige keer al gezegd, ik ben gegaan van oei, stop, pauze,
[2527.52 --> 2528.94]  allemaal eng, moeten we dit allemaal willen.
[2529.10 --> 2530.42]  Naar wauw, wat mooi.
[2530.56 --> 2531.82]  Ik gebruik het zelf dagelijks.
[2531.90 --> 2533.80]  Ik ben echt enthousiast over deze technologie.
[2533.80 --> 2538.18]  Laten we zorgen dat we het, whatever that means, een zachte landing kunnen geven.
[2538.70 --> 2543.92]  Dat we ervoor kunnen zorgen dat het inderdaad die voordelen heeft, meer dan dat het de nadelen heeft.
[2544.68 --> 2551.94]  Ik weet niet hoe, maar het is denk ik niet handig als we in die voor en tegen een beetje blijven hangen.
[2552.36 --> 2554.28]  Wat wij samen volgens mij niet per se doen hoor.
[2555.26 --> 2557.70]  Nou, punt gemaakt zal ik zeggen.
[2558.40 --> 2559.74]  Weet je wat ik ook fascinerend vind?
[2559.74 --> 2564.92]  Want kijk, alles is gebaat bij transparantie en dat zegt deze meneer in die Reddit thread dus ook.
[2565.94 --> 2569.52]  We zijn gebaat bij transparantie om als mensheid te weten waar we ongeveer staan.
[2570.04 --> 2571.34]  Toch? Daar is geen twijfel over.
[2571.78 --> 2575.06]  We willen weten wat de cutting edge is op dit moment.
[2575.38 --> 2577.50]  Je hoeft de code niet vrij te geven, maar zeg maar wat het kan.
[2577.72 --> 2578.42]  Laten we daar eens beginnen.
[2578.42 --> 2590.28]  Volgens mij weten we dus niet, bijvoorbeeld van ZGPT4 Vision, hoe dat visuele element van het model nou precies werkt.
[2590.96 --> 2601.68]  Dus zoals ik het begrijp, kan ZGPT4 Vision een foto kunnen uploaden naar ZGPT en dat vervolgens dat taalmodel er wat mee kan.
[2601.68 --> 2621.72]  En wat wij volgens mij niet weten is of dat taalmodel zo werkt dat een foto wordt omgezet in taal en dat dat de manier is waarop dat ding vervolgens kan gaan reageren op wat er in een foto gebeurt.
[2621.72 --> 2630.28]  Of terwijl hij kijkt naar de, ik weet niet precies, kijkt hij naar de pixels of hij kijkt naar wat hij aan patroontjes kan herkennen in een foto en zet dat daadwerkelijk om in tekst.
[2630.36 --> 2632.72]  Zoals wij eerder het hebben gehad over muzieknoten.
[2633.86 --> 2640.44]  Dus dat je een taalmodel kan gebruiken om muziek te maken door eigenlijk te zeggen, nou muzieknoten is de taal.
[2640.78 --> 2642.60]  Die muzieknoten gaan we omzetten in letters.
[2642.86 --> 2645.06]  Dus zo kunnen we het taalmodel muziek laten maken.
[2645.06 --> 2658.70]  Dus je kan muziek of visuele dingen of dingen die niet logischerwijs in tekst, die op het eerste gezicht niet logischerwijs in tekst om te zetten zijn, toch door een taalmodel laten bestieren.
[2659.98 --> 2661.40]  Dat is één mogelijkheid.
[2661.50 --> 2667.52]  Een andere mogelijkheid is dat foto's onderdeel zijn van het model.
[2667.72 --> 2669.96]  Dat het dus eigenlijk verder gaat dan een taalmodel.
[2670.16 --> 2673.00]  Dat het echt getraind is op foto's.
[2673.00 --> 2681.36]  En dat hij dus begrijpt wat de foto's zijn zonder dat eerst in taal om te zetten.
[2681.48 --> 2683.38]  Dat hij als het ware die tussenstap overslaat.
[2683.46 --> 2684.08]  Ja, het is heel mooi.
[2684.18 --> 2686.72]  Want ik denk zelf, dat weet ik helemaal niet.
[2686.82 --> 2687.58]  Wij weten dit niet.
[2687.68 --> 2688.66]  Nee, dat is super interessant.
[2689.08 --> 2691.60]  Nou ja, dat weet ik ook niet helemaal zeker.
[2691.72 --> 2692.92]  Maar heb je het niet kunnen vinden?
[2693.32 --> 2695.48]  Nou, ik heb met wat mensen...
[2695.48 --> 2697.88]  Hier zat ik me af te vragen hoe dit werkt.
[2697.88 --> 2703.32]  Dus ik ben wat gaan rondvragen bij mensen die het kunnen weten, niet bij OpenAI.
[2704.26 --> 2708.64]  En er zijn gewoon vraagtekens over.
[2708.92 --> 2709.88]  Ja, want mijn aanname is...
[2710.56 --> 2711.38]  En dat...
[2711.38 --> 2712.88]  Kijk, het herkennen van...
[2713.84 --> 2714.92]  Mijn punt is natuurlijk...
[2714.92 --> 2715.84]  Sorry voordat ik je gang heb.
[2715.84 --> 2716.06]  Nee.
[2716.16 --> 2716.88]  Mijn punt is...
[2716.88 --> 2718.36]  Wij weten niet...
[2718.36 --> 2718.88]  Wij weten...
[2718.88 --> 2719.74]  Dit is dus iets van die dingen...
[2719.74 --> 2721.02]  Dat we dit moeten bediscusseren.
[2721.34 --> 2721.92]  Is al iets.
[2721.92 --> 2723.66]  Ja, hoezo moeten we dit bediscusseren?
[2723.72 --> 2725.12]  Ik wil gewoon weten hoe dit werkt.
[2725.24 --> 2726.26]  Leg dit uit.
[2726.34 --> 2727.62]  En dit is niet iets waarvan...
[2727.62 --> 2728.40]  Oh, het is een black box.
[2728.46 --> 2729.62]  We hebben geen idee hoe het werkt.
[2729.72 --> 2730.74]  Dit weet OpenAI.
[2730.94 --> 2732.00]  Tuurlijk, dit is zo ontworpen.
[2732.12 --> 2732.36]  Precies.
[2732.94 --> 2734.32]  Dus praat daarover.
[2734.66 --> 2735.34]  En dat doen ze niet.
[2735.70 --> 2736.72]  Nee, want ik denk dat...
[2736.72 --> 2737.58]  Kijk, binnen die...
[2737.58 --> 2739.62]  Omdat die kunstmatige intelligentie...
[2739.62 --> 2743.46]  Ook zo ontzettend in de wetenschappelijke, academische wereld hangt...
[2743.46 --> 2745.80]  Waar het idee is dat je constant papers uitbrengt...
[2745.80 --> 2746.94]  En die papers met elkaar deelt...
[2746.94 --> 2748.92]  En dan op elkaars werk kunt voortbouwen.
[2748.92 --> 2752.24]  Dat was het idee van...
[2752.24 --> 2754.96]  Zelfs Apple schrijft tegenwoordig...
[2754.96 --> 2755.92]  Brengt artikelen uit.
[2756.36 --> 2760.18]  Want het lukte hen niet om AI-toppers binnen te halen.
[2760.24 --> 2760.90]  Want die zeiden allemaal...
[2760.90 --> 2762.22]  Ja, maar ik wil wel gewoon kunnen publiceren.
[2762.46 --> 2764.50]  Want dat is de ethos van de community.
[2765.00 --> 2767.10]  Het zijn academici die dat normaal vinden.
[2767.20 --> 2768.64]  En Apple is helemaal niet daarvan.
[2768.74 --> 2769.84]  Als ze het over black boxes hebben.
[2770.24 --> 2772.26]  Maar zelfs vanuit Apple AI Research...
[2772.26 --> 2773.60]  Komen tegenwoordig papers.
[2774.00 --> 2775.80]  De papers die komen uit OpenAI...
[2775.80 --> 2777.58]  Zijn allemaal een beetje half marketing.
[2777.58 --> 2779.24]  Een beetje leeg.
[2779.74 --> 2781.80]  Half weg getypex de shit.
[2781.88 --> 2782.92]  Waar we niet zo heel veel mee kunnen.
[2783.02 --> 2784.16]  Het blijft expres vaag.
[2784.32 --> 2785.62]  Daar heb je 100% een punt.
[2786.32 --> 2787.58]  Ik denk zelf dat GPT-4V...
[2788.16 --> 2789.58]  Maar allemaal aannames...
[2790.16 --> 2791.74]  Een combinatie is van twee technieken.
[2792.18 --> 2794.04]  Dus je hebt een algoritme getraind.
[2794.32 --> 2796.96]  Een model getraind om objectdetectie te doen in plaatjes.
[2797.06 --> 2800.16]  Dus dat is allemaal door middel van een hele grote image dataset.
[2800.16 --> 2802.92]  Die al gelabeld was met kat is een kat...
[2802.92 --> 2803.70]  Een hond is een hond.
[2804.22 --> 2805.74]  En dan kan dat ding uiteindelijk uitspugen.
[2806.02 --> 2808.86]  Je doet er een plaatje en je krijgt terug 93% kat.
[2808.98 --> 2809.62]  Ik zie een auto.
[2809.92 --> 2810.42]  Ik zie een fiets.
[2810.62 --> 2811.92]  En ik zie een blauwe horizon.
[2812.50 --> 2814.18]  En dat gaat dan een taalmodel in.
[2814.36 --> 2814.90]  Dat denk ik.
[2815.34 --> 2817.44]  Maar wat jij terecht zegt is...
[2817.44 --> 2819.04]  Ja, maar dat taalmodel kan ook muziek maken.
[2819.46 --> 2821.76]  Dus waarom zou dat taalmodel niet ook objecten kunnen herkennen?
[2821.76 --> 2825.64]  Is het inmiddels al één multimodal supermodel?
[2826.60 --> 2828.04]  Dus volgens mij is...
[2828.04 --> 2828.88]  Waar er audio in zit.
[2829.06 --> 2829.22]  Ja.
[2829.78 --> 2830.46]  Maar volgens mij...
[2830.46 --> 2831.24]  Ga ik kan doen.
[2831.56 --> 2834.62]  Volgens mij is multimodal ook dat je dus echt verschillende modellen combineert...
[2834.62 --> 2835.54]  Als een soort ensemble.
[2835.88 --> 2838.46]  Ik denk dat GPT-4 een minimaal ensemble is...
[2838.46 --> 2842.02]  Van twee verschillende modellen die met elkaar samenwerken.
[2842.04 --> 2844.00]  Namelijk foto's en tekst.
[2844.12 --> 2847.00]  Ja, om te kunnen komen van rauwe pixels...
[2847.00 --> 2851.50]  Naar concepten die weer te begrijpen zijn door het taalmodel.
[2851.76 --> 2853.54]  De vraag is alleen...
[2853.54 --> 2855.66]  Dit is natuurlijk hetzelfde op het moment dat...
[2855.66 --> 2858.64]  Als wij nu paranormaal onze breinen zouden verbinden...
[2858.64 --> 2860.08]  Ik ga even een sprongetje.
[2860.26 --> 2860.92]  Dat kunnen wij niet...
[2860.92 --> 2861.54]  Ja, een klein sprongetje.
[2861.76 --> 2862.46]  Ik blijf erbij.
[2863.16 --> 2864.92]  Alles anders ogen gaan inmiddels dicht.
[2865.06 --> 2866.58]  Zo'n groot sprongetje is het.
[2868.06 --> 2871.86]  Dan is het eigenlijk heel erg inefficiënt voor ons...
[2871.86 --> 2874.02]  Om nog in de Nederlandse taal te communiceren.
[2874.54 --> 2877.80]  Waarschijnlijk kunnen we beter communiceren in de taal die voor taal zit.
[2877.90 --> 2881.02]  Nu gaan we echt richting veel te diepe cognitieve filosofie...
[2881.02 --> 2882.04]  Waar ik ook te weinig van weet.
[2882.04 --> 2882.64]  Taal voor taal.
[2882.80 --> 2886.58]  Ja, er zijn bijvoorbeeld filosofen die praten over precepten...
[2886.58 --> 2887.56]  In plaats van concepten.
[2887.60 --> 2889.16]  Dus dat is wat nog voor een concept zit.
[2889.22 --> 2891.12]  Een concept is al heel erg uitgekristalliseerd.
[2891.34 --> 2891.70]  Auto.
[2892.20 --> 2893.44]  Maar er zit waarschijnlijk een...
[2893.44 --> 2894.22]  Dat weten we niet.
[2894.56 --> 2895.62]  Er zijn allemaal theorieën over.
[2896.02 --> 2897.58]  Maar het is niet zo een leeg brein.
[2897.86 --> 2898.10]  Auto.
[2898.44 --> 2899.02]  En dan ineens...
[2899.02 --> 2899.98]  Daar zit niks tussen of zo.
[2900.04 --> 2901.28]  Daar zit een soort opbouw aan.
[2901.28 --> 2904.84]  En die precepten, dus wat ervoor zit...
[2904.84 --> 2905.10]  Ja.
[2905.40 --> 2907.26]  Die zijn ook wat meer gemixt nog met emotie.
[2907.88 --> 2909.54]  Concepten zitten veel meer in de neocortex.
[2909.66 --> 2910.64]  Die zijn heel concreet.
[2911.18 --> 2912.16]  Heel rationeel.
[2912.54 --> 2913.80]  Maar er zit een soort lijn voor.
[2914.38 --> 2916.74]  De onderbuik, het lichaam, het gevoel.
[2917.18 --> 2917.84]  En die...
[2917.84 --> 2921.38]  Dat is nog een soort primitieve prototype concepten komen daarvoor.
[2921.84 --> 2925.24]  Dit soort theorieën heb je allemaal nodig om te komen van een hagedist tot een mens.
[2925.36 --> 2927.38]  Want het was niet ineens op een maandagochtend...
[2927.38 --> 2928.70]  Yo, goeiemorgen, ik ben Henk.
[2928.78 --> 2929.56]  Weet je, dat ging niet zo.
[2929.66 --> 2932.18]  Dus er moeten nog een soort geluiden voor zijn geweest.
[2932.32 --> 2933.84]  En mimiek en non-verbaal.
[2934.14 --> 2935.54]  Dat doen wij nog steeds hier met z'n tweeën.
[2935.64 --> 2938.02]  Daarom vinden we het ook leuker om live in deze studio te zitten.
[2938.08 --> 2940.48]  Want er gebeurt zoveel meer dan alleen maar die woorden van mij.
[2940.50 --> 2942.42]  Voor de helderheid, we nemen dit een keer niet schermachtig.
[2942.44 --> 2942.64]  Ja.
[2942.88 --> 2944.24]  Niet geeft nu echt alles weg.
[2944.38 --> 2944.80]  Maar ja.
[2945.44 --> 2946.28]  Het is vrijdag.
[2948.50 --> 2952.50]  Uiteindelijk zou het natuurlijk zo kunnen zijn dat als je twee modellen hebt die met elkaar samenwerken.
[2952.62 --> 2955.46]  Dus een taalmodel aan de ene kant en een visueel model aan de andere kant.
[2955.46 --> 2957.54]  Die object detectie kan doen.
[2958.12 --> 2962.42]  Dat die niet in het Engels met elkaar praten.
[2962.56 --> 2964.76]  Maar op een lager niveau aan elkaar gekoppeld zijn.
[2964.94 --> 2965.10]  Juist.
[2965.20 --> 2966.46]  En ik denk dat...
[2967.08 --> 2968.96]  Dus om een beetje antwoord te geven op jouw vraag.
[2969.06 --> 2971.46]  En dat wordt ook dan weer een beetje meer...
[2972.42 --> 2974.02]  Ja, waar stoppen we de kaders?
[2974.44 --> 2977.46]  Jij zou kunnen zeggen, als iemand van OpenAI uitlegt wat het is.
[2977.50 --> 2979.74]  Dat jij zegt, maar dit vind ik eigenlijk gewoon één model dan.
[2980.32 --> 2981.46]  Ik vind eigenlijk dat het GPT-4V...
[2981.46 --> 2984.14]  GPT-4V, dat gaan we niet meer A plus B noemen.
[2984.26 --> 2985.18]  Maar is gewoon een merger.
[2985.34 --> 2986.18]  Een hybrid model.
[2986.68 --> 2988.04]  Terwijl ik misschien zou zeggen...
[2988.04 --> 2990.94]  Ja, dat ze op zo'n fundamenteel niveau met elkaar communiceren.
[2991.02 --> 2992.54]  Dat wij het als mens niet meer kunnen lezen.
[2992.98 --> 2994.84]  Maakt niet dat het niet nog steeds twee modellen zijn.
[2995.22 --> 2997.14]  Dan kunnen we die discussie daar laten.
[2997.24 --> 3000.46]  Ik denk dus dat die GPT-4V...
[3000.46 --> 3004.12]  een bolted on extra dingen heeft.
[3004.44 --> 3005.34]  Maar niet...
[3005.34 --> 3007.28]  Mogelijk communiceer je het gewoon in het Engels hoor.
[3007.74 --> 3009.48]  Dat is gewoon heel lame.
[3009.64 --> 3011.04]  Weet je, al zo heel erg brute force.
[3011.54 --> 3014.06]  Maar ik denk inmiddels dat het op een soort...
[3014.06 --> 3016.36]  Ja, voortaal zit of zo.
[3017.14 --> 3017.58]  Denk ik.
[3017.58 --> 3022.38]  Ja, en ik heb gewoon niet heel veel vertrouwen gekregen...
[3022.38 --> 3024.66]  ook door de gebeurtenissen van de afgelopen week.
[3024.74 --> 3026.98]  En misschien is dus dat vertrouwen onterecht.
[3027.34 --> 3030.48]  Vertrouwen gekregen in dat open jaar nou zo transparant is met wat ze doen.
[3031.18 --> 3035.38]  Want het kan zo zijn dat we dus kunnen redeneren...
[3035.38 --> 3038.48]  Als er iets was geweest wat erg was, dan was er wel een lek geweest.
[3038.60 --> 3040.54]  Nou oké, dat is op een bepaalde manier geruststellend...
[3040.54 --> 3042.08]  dat er geen lek is geweest.
[3042.08 --> 3042.38]  Toch niet.
[3042.74 --> 3047.56]  Maar toch, de hele opzet is nu met een board...
[3047.56 --> 3052.20]  wat met allemaal economische belangen heeft.
[3052.26 --> 3053.22]  Met Microsoft voorop.
[3053.60 --> 3058.02]  Die er niet een belang bij heeft om alles transparant te doen.
[3058.02 --> 3060.02]  Want ze weten wat voor overheid...
[3060.80 --> 3064.02]  inmenging er zal ontstaan op het moment dat je...
[3064.02 --> 3067.68]  onrust gaat saaien over dingen waar je nog over twijfelt...
[3067.68 --> 3070.96]  of van vroege aanwijzingen die je hebt...
[3070.96 --> 3073.32]  voor dingen die negatief zouden kunnen zijn.
[3073.78 --> 3077.14]  Te transparant zijn zorgt al heel snel voor heel veel onrust.
[3077.24 --> 3081.16]  Nou ja, je ziet dat nu bij gebrek aan duiding...
[3081.16 --> 3083.14]  over waarom Elkman ontslagen was.
[3083.70 --> 3088.08]  Gelijk de invulling in een soort van Terminator 2-achtige scenario's...
[3088.08 --> 3089.28]  dat we er gelijk op plakken.
[3090.66 --> 3093.52]  Dus dat is aan de hand.
[3093.52 --> 3097.18]  En tegelijkertijd het bedrijf begint zich nu te gedragen...
[3097.18 --> 3100.46]  alsof het een soort van Apple-achtig is met keynotes.
[3101.24 --> 3102.86]  Waar ze dus aankondigingen doen.
[3103.20 --> 3107.16]  X keer per jaar over nieuwe modellen met nieuwe features.
[3107.64 --> 3109.94]  En dat is dan helemaal uitgewerkt in een product.
[3110.10 --> 3113.70]  Dat is niet een soort van de wetenschap die daar aan ten grondslag ligt.
[3113.84 --> 3117.22]  Of het moment dat het technisch ontdekt wordt dat het kan.
[3118.10 --> 3121.06]  Maar het is het moment dat het in Chattieptie geïntegreerd gaat worden.
[3121.06 --> 3125.54]  Dat moment dat het op Steve Jobs-achtige wijze aangekondigd wordt.
[3125.66 --> 3128.38]  Wat een soort van geheimzinnigheid vereist.
[3128.52 --> 3132.82]  Het vereist geheimzinnigheid om op een dev day met een keynote...
[3132.82 --> 3134.08]  een leuke aankondiging te doen.
[3134.20 --> 3136.16]  Dus het vergroot mijn wantrouwen.
[3136.28 --> 3139.72]  En dat Apple dat doet met fucking iPhones die ze uitbrengen...
[3139.72 --> 3140.66]  het ze lekker zelf weten.
[3141.28 --> 3144.14]  Maar het voelt alsof OpenAI het type technologie maakt...
[3144.14 --> 3148.20]  waarbij het gewoon niet oké is om daar een soort van glitchy show van te maken.
[3148.54 --> 3152.72]  En de belangen zijn waar ze al een beetje...
[3153.24 --> 3156.24]  waar ze nog een beetje geremd werden door dat non-profit idee.
[3156.24 --> 3162.44]  En het idee dat er dus mensen aan boord zijn die zich heel erg zorgen maken over AI-veiligheid.
[3162.44 --> 3169.50]  Maar ik ben niet gerustgesteld door de ontwikkeling van de afgelopen week...
[3169.50 --> 3172.06]  op het gebied van transparantie, op het gebied van governance.
[3172.72 --> 3175.16]  En door beter na te denken...
[3175.16 --> 3178.44]  want ik denk dat er een soort van bewustwording is bij ons allemaal...
[3178.44 --> 3182.48]  over wie er achter dit soort bedrijven zitten...
[3182.48 --> 3185.16]  wat dat voor mensen zijn en wat hun beweegredenen precies zijn.
[3185.48 --> 3187.20]  Daar is meer duidelijk over geworden.
[3187.34 --> 3188.76]  En dat maakt me niet bepaald geruster.
[3188.76 --> 3192.80]  Dus of het nou een Hollywood scenario is of niet...
[3192.80 --> 3194.66]  waar we rekening mee moeten houden...
[3194.66 --> 3198.06]  ik vind de basis die hier voor ons ligt...
[3198.06 --> 3199.42]  vind ik niet bepaald positief.
[3199.80 --> 3200.76]  Ja, ik denk dat...
[3202.02 --> 3203.76]  wij hebben allebei gezegd...
[3204.68 --> 3207.66]  toen we die presentatie van OpenAI zagen...
[3207.66 --> 3209.00]  en ik nog meer dan jij...
[3209.00 --> 3211.62]  van vet man, ik heb dit gemist.
[3212.68 --> 3215.14]  Want het was een beetje een Jobsian ding.
[3215.38 --> 3216.26]  Sam Altman stond...
[3216.26 --> 3217.00]  Het was een goede show.
[3217.14 --> 3218.16]  Ja, het was echt een goede show.
[3218.16 --> 3218.88]  Ik viel er ook voor.
[3218.90 --> 3220.18]  Het zag er best wel mooi uit ook allemaal.
[3220.18 --> 3220.42]  Ja, zeker.
[3220.60 --> 3221.36]  Lekker stijltje.
[3221.86 --> 3223.36]  En ik hoorde bijna zo...
[3223.36 --> 3224.60]  weet je wat je vroeger wel eens...
[3224.60 --> 3225.88]  in de keynotes had...
[3225.88 --> 3227.06]  zo'n fluitje in de zaal zo.
[3227.30 --> 3228.50]  Dat betekende altijd van...
[3228.50 --> 3229.00]  dit is nice.
[3229.06 --> 3231.22]  Dat was het meest subtiele fluitje wat je kon doen.
[3231.80 --> 3234.14]  Nou, ik heb het fluitje waarschijnlijk gehallusioneerd.
[3234.24 --> 3234.44]  Maar goed.
[3235.12 --> 3236.16]  Ik keek dat en ik dacht...
[3236.16 --> 3237.34]  ik had een soort...
[3237.34 --> 3238.96]  ja, het is toch ook een beetje nostalgie.
[3239.50 --> 3241.08]  Weet je nog, toen tech cool was...
[3241.08 --> 3242.54]  en we allemaal nieuwe gadgets kregen de hele week...
[3242.54 --> 3243.16]  Het heeft die vorm, ja.
[3243.54 --> 3243.70]  Ja.
[3243.96 --> 3245.04]  Nou, en dan...
[3245.04 --> 3245.94]  maar dan op een gegeven moment...
[3245.94 --> 3247.28]  het voelt bijna misplaatst.
[3247.46 --> 3249.02]  Dat beschrijf jij nu heel mooi.
[3249.90 --> 3251.02]  Maar wacht even.
[3251.44 --> 3253.60]  Ik pak toch nog even die steen der wijzen erbij.
[3254.48 --> 3255.04]  Zoiets fundamenteels.
[3255.82 --> 3258.32]  Zoiets maatschappelijk breed rakend.
[3258.44 --> 3260.38]  Zoiets existentieels.
[3261.14 --> 3264.18]  Is dat eigenlijk wel een worldcoin-achtig product?
[3264.86 --> 3267.92]  Ik bedoel, we hebben een applausje gegeven aan Sam Altman.
[3267.92 --> 3270.72]  Ik denk omdat hij gewoon ontzettend goed is...
[3270.72 --> 3272.28]  in het neerzetten van een product.
[3272.52 --> 3273.76]  En dat klinkt misschien heel leeg...
[3273.76 --> 3275.56]  maar het is, dat weten wij allebei...
[3275.56 --> 3276.74]  gewoon echt moeilijk...
[3276.74 --> 3278.26]  om iets neer te zetten...
[3278.26 --> 3279.90]  waar mensen enthousiast van worden...
[3279.90 --> 3280.90]  wat ze begrijpen...
[3280.90 --> 3282.26]  waar ze voor willen betalen...
[3282.26 --> 3283.58]  of wat ze in ieder geval willen...
[3283.58 --> 3285.78]  ja, waar ze iets voor over hebben.
[3286.36 --> 3287.28]  En dat het je dan lukt...
[3287.28 --> 3288.48]  omdat het ook nog op een manier te doen...
[3288.48 --> 3290.02]  dat het er een beetje goed uitziet...
[3290.02 --> 3290.96]  dat het lekker werkt...
[3290.96 --> 3292.98]  daar hebben wij volgens mij allebei respect voor.
[3292.98 --> 3295.98]  En nu, meer en meer...
[3296.60 --> 3298.18]  denk ik ook van...
[3298.18 --> 3298.98]  oké, maar wacht even...
[3299.74 --> 3301.98]  misschien is Sam Altman veel meer geschikt...
[3302.52 --> 3305.10]  om die fundamentele technologieën...
[3305.10 --> 3306.98]  die uitgevonden worden op andere plekken...
[3307.50 --> 3309.10]  te licenseren...
[3309.10 --> 3310.38]  en daar producten op te bouwen.
[3310.52 --> 3311.36]  Misschien...
[3311.36 --> 3313.12]  Ik zou bijna zeggen...
[3313.12 --> 3315.46]  start wel een bedrijf ernaast...
[3315.46 --> 3317.78]  neem een licentie op de technologieën van OpenAI...
[3317.78 --> 3319.54]  en ga lekker die apps allemaal bouwen...
[3319.54 --> 3320.80]  want daar ben je hartstikke goed in.
[3320.80 --> 3322.66]  En doe alsjeblieft ook nog een paar apps...
[3322.66 --> 3323.94]  voor de overheid in Amerika...
[3323.94 --> 3325.58]  en andere plekken waar apps nodig zijn...
[3325.58 --> 3327.52]  en ga misschien werken voor mensen met diabetes...
[3327.52 --> 3330.98]  en ga het hele Global Health System aanpakken in Amerika, Sam.
[3331.10 --> 3332.92]  Want je bent ontzettend goed in het...
[3332.92 --> 3335.70]  tot de markt brengen van hele mooie werkende producten.
[3336.18 --> 3338.40]  Maar ik denk dat hij niet de juiste persoon is...
[3338.40 --> 3340.34]  dat ben ik toch wel met Ilja eens...
[3340.34 --> 3341.62]  om bij OpenAI...
[3341.62 --> 3344.16]  een half-NGO research lab...
[3344.16 --> 3346.42]  die existentiële technologieën aan het ontwikkelen is...
[3346.42 --> 3348.92]  daar moet je gewoon niet zo'n rogue Sam Altman bij hebben.
[3349.16 --> 3350.04]  Dat denk ik.
[3350.04 --> 3352.70]  Zullen we eindigen met wat gezellige dingetjes?
[3352.94 --> 3353.46]  Ja, dat is leuk.
[3353.64 --> 3358.28]  Nou, Eleven Labs heeft dus de voorloper...
[3358.28 --> 3361.50]  op het gebied van allerlei AI stemgeneratie.
[3361.76 --> 3365.90]  Dus het maken op basis van een sample van iemand stemgeluid...
[3366.90 --> 3370.96]  die persoon een onbeperkt aantal dingen kunnen laten zeggen.
[3371.22 --> 3373.68]  En ze maken technologie voor andere text-to-speech...
[3373.68 --> 3374.72]  voor bijvoorbeeld audiobooken.
[3374.72 --> 3377.84]  Ze gaan steeds verder met het leggen van emotie in stemmen...
[3377.84 --> 3382.42]  waardoor een AI stem eigenlijk nog steeds realistischer wordt.
[3382.52 --> 3385.28]  En hun ontwikkeling gaat best snel.
[3385.46 --> 3387.86]  Ik vind dat ze met grote snelheid met updates uitkomen.
[3387.88 --> 3389.46]  Iedere anderhalve maand weer...
[3389.46 --> 3390.16]  Een groot ding.
[3390.16 --> 3390.82]  De grote stappen.
[3390.82 --> 3392.32]  Dus of die nou meer talen kan.
[3392.56 --> 3395.34]  Het nieuwste ding is speech-to-speech.
[3395.60 --> 3396.54]  Dus niet text-to-speech.
[3396.66 --> 3398.18]  Of speech-to-text, maar speech-to-speech.
[3398.82 --> 3400.20]  Dat je ook text-to-text hebt.
[3400.44 --> 3401.26]  Een nieuw speech-to-speech.
[3401.82 --> 3402.82]  En speech-to-speech is...
[3402.82 --> 3404.82]  een stukje tekst invoeren...
[3404.82 --> 3406.54]  of een stukje audio invoeren.
[3406.70 --> 3407.38]  Je hebt gewoon een...
[3407.38 --> 3409.72]  in dat ding heb je gewoon een opnameknop.
[3410.16 --> 3411.96]  We hebben het zelf net ook even geprobeerd.
[3412.40 --> 3415.06]  Een opnameknop waar je gewoon in kan praten.
[3415.78 --> 3417.94]  En dan vervolgens kun je een stem uitkiezen...
[3418.88 --> 3419.94]  die jouw stem vervangt.
[3420.64 --> 3422.88]  Het is als een soort van deepfake voor je gezicht.
[3422.98 --> 3425.12]  Dat je er opeens uit kan zien als Mark Rutte.
[3425.20 --> 3427.12]  Maar nu kan je ook je stem dan...
[3427.12 --> 3429.92]  in één keer laten vervangen door Mark Rutte.
[3430.06 --> 3431.38]  Ik heb Mark Rutte getraind.
[3432.64 --> 3433.52]  Dat klinkt heel gek.
[3433.52 --> 3434.84]  Maar ik heb...
[3434.84 --> 3435.62]  Ben jij zijn mentor?
[3436.62 --> 3438.66]  Ik kom eindelijk uit de kast als...
[3438.66 --> 3439.46]  Spindopker.
[3439.48 --> 3440.04]  ...genius.
[3442.36 --> 3445.48]  En 30 seconden audio van Mark Rutte...
[3445.48 --> 3448.16]  was genoeg om een reudelijk realistische versie...
[3448.16 --> 3449.72]  van Mark Rutte te krijgen...
[3449.72 --> 3450.92]  die van text-to-speech doet.
[3451.22 --> 3453.02]  Maar nu kan ik dus ook een stukje tekst...
[3453.02 --> 3454.32]  in mijn eigen stem opnemen.
[3454.56 --> 3456.82]  En dan Mark Rutte die stem te laten doen.
[3456.86 --> 3459.22]  En dan pakt hij exact de manier waarop ik praat.
[3459.22 --> 3461.42]  Dus hij pakt de manier...
[3461.42 --> 3464.16]  waarop ik intonaties leg bij woorden.
[3464.28 --> 3465.64]  De pauzes die ik neem.
[3466.88 --> 3470.08]  En andere dingen die ik doe terwijl ik praat.
[3470.46 --> 3471.24]  Neemt hij over.
[3471.84 --> 3474.62]  En dat vind ik echt heel erg cool.
[3474.70 --> 3475.72]  Het werkt nog niet super goed.
[3475.88 --> 3477.28]  Maar het werkt best oké.
[3477.36 --> 3478.50]  Er zijn voorbeelden.
[3478.70 --> 3479.38]  Maar dat is een beetje...
[3479.38 --> 3480.74]  mensen hebben dan hun best gedaan...
[3480.74 --> 3482.74]  en een beetje het accent zelf nog aangezet.
[3482.82 --> 3483.96]  Ja, en hebben ze selecteerd.
[3484.10 --> 3484.80]  De goede versie.
[3484.94 --> 3486.50]  En dan denk je toch wel...
[3486.50 --> 3486.90]  wauwie.
[3487.38 --> 3488.36]  Maar als je het zelf test...
[3488.36 --> 3489.32]  dan merk je nog wel...
[3489.32 --> 3489.36]  oh.
[3489.86 --> 3491.48]  Ja, er zijn momenten dat het goed gaat...
[3491.48 --> 3492.80]  en er zijn momenten dat het niet zo goed gaat.
[3492.90 --> 3495.44]  Maar het geeft wel aan...
[3495.44 --> 3496.52]  hoe ver het nu is.
[3496.66 --> 3498.10]  En dat we dus over anderhalve maand...
[3498.10 --> 3501.08]  waarschijnlijk weer naar een betere versie zitten te luisteren.
[3501.88 --> 3503.34]  Als je dit doorredeneert...
[3503.34 --> 3504.66]  want ik zet...
[3504.66 --> 3505.50]  Dus dit is gewoon vet.
[3505.62 --> 3505.88]  Punt.
[3506.14 --> 3508.36]  Nou, dat is toch nog een positief ding.
[3508.54 --> 3510.58]  Ik wil ook nog wel één positieve straks toevoegen.
[3510.82 --> 3511.68]  Dan eindigen we ook positieve.
[3511.68 --> 3515.68]  Nou, een ander ding wat ik heel veel voorbij zie komen nu...
[3515.68 --> 3517.46]  op X heb je van...
[3517.46 --> 3518.02]  voorheen Twitter.
[3518.16 --> 3521.46]  Heb je van die golven van...
[3521.46 --> 3524.54]  mensen die nieuwe AI dingetjes proberen.
[3524.54 --> 3528.48]  Dus op het moment dat Mid Journey met een nieuw model uitkomt...
[3528.48 --> 3529.54]  dan zie je dat iedereen ermee gaat klooien.
[3530.68 --> 3532.54]  Je hebt momenten dat...
[3532.54 --> 3536.12]  dat het Gaussian Splatting voor het eerst veel gebruikt wordt.
[3536.20 --> 3539.82]  Dus dat je foto's kan maken waar je in 3D in kan vliegen.
[3540.08 --> 3540.56]  Als het ware.
[3540.74 --> 3541.96]  En dan zie je dan weer...
[3541.96 --> 3543.50]  daar mensen heel veel weer mee klooien.
[3543.58 --> 3544.78]  Goeie naam voor een band trouwens.
[3546.06 --> 3548.26]  En wat nu het ding is...
[3548.26 --> 3550.94]  waar mensen veel mee aan het klooien zijn...
[3550.94 --> 3554.00]  is een paint tekening maken van iets.
[3554.00 --> 3556.10]  Dus je maakt een paint tekening van een boom.
[3556.60 --> 3560.00]  En dan laat je de AI daar de fotorealistische versie van maken.
[3560.00 --> 3564.36]  En het doet mij dus een beetje denken aan de metafoor speech to speak.
[3564.48 --> 3564.86]  Ja, mooi.
[3565.12 --> 3568.22]  Dus je maakt een tekeningetje op zoals een kind een tekening maakt.
[3568.42 --> 3570.30]  Gewoon, nou ja, laten we zeggen, basaal.
[3570.72 --> 3572.88]  En dan vervolgens maakt de AI daar iets heel moois van.
[3573.46 --> 3575.46]  En ik ben dus...
[3575.46 --> 3578.00]  redelijk gefascineerd door...
[3578.54 --> 3580.00]  dit soort...
[3580.00 --> 3583.02]  ja, hoe moet je dat nou zeggen?
[3583.20 --> 3584.54]  Functie die je kan gebruiken.
[3584.66 --> 3585.12]  Omdat het...
[3585.12 --> 3589.12]  het is niet vanuit het stof iets...
[3589.76 --> 3591.94]  vanuit het stof iets optrekken wat nieuw is.
[3592.04 --> 3593.32]  Zoals ChatGPT is.
[3593.40 --> 3595.66]  Je vraagt om ChatGPT om iets te schrijven.
[3595.82 --> 3597.94]  En dan krijg je gewoon nieuwe tekst.
[3598.42 --> 3603.16]  Nee, het is eerst iets invoeren waarbij de AI het transformeert in iets anders.
[3603.54 --> 3605.58]  Verbetert of nou ja, wat dan ook.
[3605.58 --> 3610.60]  En als je dus de optelsom maakt van een tekeningetje maken...
[3610.60 --> 3613.58]  en daar een mooie fotorealistische plaat van maken...
[3614.10 --> 3617.00]  of een tekstje inspreken...
[3617.00 --> 3619.58]  en dan vervolgens een hele gave stem erbij kunnen gebruiken...
[3620.26 --> 3621.50]  al dan niet op muziek.
[3622.62 --> 3626.72]  Of een videootje maken van jezelf dat je op straat loopt...
[3626.72 --> 3630.08]  en dan AI een soort van ruimteversie daarvan laten maken...
[3630.08 --> 3631.92]  met dat jij wel de bewegingen van jou...
[3631.92 --> 3633.42]  dus je ziet jezelf wandelen...
[3633.42 --> 3634.64]  maar in een ruimtepak...
[3634.64 --> 3636.76]  in een totaal ander poppetje...
[3636.76 --> 3638.18]  en in een totaal andere omgeving.
[3638.54 --> 3641.60]  Het is dus AI die broninformatie...
[3641.60 --> 3643.98]  die wij allemaal, hoeven we niet programmeur voor te zijn...
[3643.98 --> 3646.94]  je hebt gewoon een iPhone nodig, that's it, kunnen maken.
[3647.54 --> 3648.86]  En ik zit dan elke keer te denken...
[3648.86 --> 3651.90]  wat als ik nu twaalf jaar oud was geweest...
[3651.90 --> 3653.30]  wat had ik hier dan mee gedaan?
[3653.82 --> 3656.48]  En ik denk dat het zo vet is om nog toe te voegen...
[3656.48 --> 3658.60]  dat het menselijke input is.
[3658.78 --> 3661.74]  Dus het is een samenkomst...
[3661.74 --> 3663.86]  dus een hybrid tussen mens en machine.
[3663.86 --> 3665.54]  En dat maakt het volgens mij...
[3665.54 --> 3667.04]  want ik moet ook denken...
[3667.04 --> 3668.52]  stel dat jij een mooi gedicht schrijft...
[3668.52 --> 3670.44]  maar je kunt het zelf niet zo mooi vertellen...
[3670.44 --> 3673.20]  dan zou je misschien vroeger daar tekst van hebben gemaakt...
[3673.74 --> 3675.24]  en een toneelvereniging hebben gezocht...
[3675.24 --> 3677.82]  waar iemand zat, een actrice of een acteur...
[3677.82 --> 3679.76]  die dat ergens op een podium kon brengen...
[3679.76 --> 3681.56]  en dan zat jij in de zaal met het publiek...
[3681.56 --> 3682.90]  en dan bracht zij het...
[3682.90 --> 3683.82]  en dan dacht jij...
[3683.82 --> 3685.44]  ja, dit is wat ik wilde zeggen.
[3685.70 --> 3687.96]  Maar ik kan het niet zeggen, ik kan het alleen maar schrijven.
[3687.96 --> 3689.80]  En dat is wat dit kan.
[3689.96 --> 3691.20]  Is dat jij gewoon echt kan denken...
[3691.20 --> 3693.22]  ik heb een beeld in mijn hoofd...
[3693.22 --> 3695.26]  van een neergaande zon ergens...
[3695.26 --> 3696.82]  maar ook in een dystopisch landschap...
[3696.82 --> 3697.78]  Maar ik kan niet tekenen.
[3697.84 --> 3698.58]  Ik kan niet tekenen.
[3698.64 --> 3700.20]  Ik kan wel tekenen, maar ik kan niet tekenen.
[3700.84 --> 3701.64]  En dat dan...
[3702.64 --> 3704.14]  dat vind ik de mooiste filmpjes...
[3704.14 --> 3705.56]  dat het live is.
[3705.70 --> 3706.70]  Dus dan heb je één vak...
[3706.70 --> 3708.10]  aan de ene kant zit iemand paint...
[3708.10 --> 3710.94]  en die AI zit een halve seconde erachter...
[3710.94 --> 3713.32]  aan dit andere vak jouw interpretatie te doen.
[3713.38 --> 3715.40]  Dus dan ben je echt samen bezig.
[3715.40 --> 3717.22]  En iedere keer als je die pen...
[3717.22 --> 3718.36]  als je ineens een strook maakt...
[3718.36 --> 3719.78]  staat er ineens een raket in dat landschap.
[3719.86 --> 3720.42]  En dan denk je ook...
[3720.42 --> 3724.26]  Ja, dat het samenspelen...
[3724.26 --> 3726.86]  in een soort van mini-orkest met AI...
[3726.86 --> 3728.16]  is wat mij betreft het summen.
[3728.48 --> 3729.20]  Dit is het.
[3729.30 --> 3730.64]  Laten we daar blijven met z'n allen.
[3730.84 --> 3731.90]  Dat we het samen kunnen doen.
[3732.06 --> 3734.00]  Nou, tenslotte, laatste positieve note.
[3734.18 --> 3735.18]  Nou, ik zat...
[3735.18 --> 3736.40]  Wij hebben het ooit gehad over...
[3737.06 --> 3738.58]  een soort brute force manier...
[3738.58 --> 3740.32]  om te zorgen dat ook al gaan...
[3740.32 --> 3741.76]  app-ontwikkelaars in dit geval...
[3741.76 --> 3742.40]  mobiele apps...
[3742.74 --> 3744.40]  niet zo snel allerlei...
[3745.40 --> 3747.22]  haakjes toevoegen aan hun knopjes...
[3747.22 --> 3749.18]  zodat jij heel makkelijk tegen Siri kan zeggen...
[3749.18 --> 3751.24]  ik wil graag een Uber bestellen.
[3751.70 --> 3752.62]  En ik wil dat die...
[3752.62 --> 3753.70]  Nou ja, ik wil een Uber bestellen.
[3754.56 --> 3755.24]  Zoek het uit.
[3755.28 --> 3755.72]  Ja, regel het.
[3755.72 --> 3756.72]  Kijk maar naar mijn agenda.
[3757.10 --> 3757.58]  Doe maar wat.
[3757.64 --> 3758.32]  Zie maar waar ik ben.
[3758.74 --> 3760.28]  En open die Uber-app...
[3760.28 --> 3761.22]  en druk op al die knoppen...
[3761.22 --> 3762.22]  waar ik dan op zou drukken.
[3762.80 --> 3763.04]  Ja.
[3763.04 --> 3764.18]  Ja, maar je moet de Uber-API.
[3764.28 --> 3765.08]  Nee, die is er niet.
[3765.20 --> 3766.40]  Je gaat gewoon op die knoppen drukken.
[3766.56 --> 3767.28]  Alsjeblieft, lieve jij.
[3767.68 --> 3770.30]  En nou, daar is er nu een mooie research paper van.
[3770.82 --> 3772.94]  Die heet GPT-4 in Wonderland.
[3773.38 --> 3774.88]  En dat is een zero-shot approach.
[3774.96 --> 3777.24]  Dus het houdt in dat je het zonder eerst iets uit te leggen...
[3777.24 --> 3778.26]  er gewoon op los gaat.
[3778.82 --> 3780.50]  En dan moet je je voorstellen...
[3780.50 --> 3782.48]  dat er een iPhone-simulator gestart wordt.
[3782.72 --> 3784.20]  Gewoon een virtuele iPhone.
[3784.64 --> 3786.44]  Daar wordt een taalmodel op losgelaten.
[3786.44 --> 3788.70]  GPT-4V, die ook nog eens kan kijken.
[3789.04 --> 3790.34]  Hoe die kan kijken, weten we nog niet.
[3790.42 --> 3791.10]  Maar die kan kijken.
[3791.72 --> 3793.44]  En dan een opdracht geven.
[3793.70 --> 3796.22]  Ik wil graag weten waar deze in deze stad ligt.
[3796.42 --> 3798.12]  Of ik wil graag een bestelling plaatsen.
[3798.24 --> 3799.74]  Of ik wil graag een notitie maken.
[3800.30 --> 3801.86]  En dat ding gaat gewoon als een gek.
[3802.02 --> 3805.02]  Overal op klikken, trekken, draaien en doen.
[3805.58 --> 3807.58]  En uiteindelijk lukt het dus heel makkelijk...
[3807.58 --> 3809.62]  met 91% accuracy eigenlijk.
[3809.72 --> 3810.46]  Al vrij snel.
[3810.74 --> 3812.70]  Om door dat hele OS heen te klikken.
[3813.14 --> 3815.26]  Nou, dan denk ik...
[3815.26 --> 3817.20]  Stel dat je dat nou doet voor de NS-kaartautomaat.
[3817.88 --> 3818.74]  Dus je gaat...
[3818.74 --> 3820.96]  Je pakt dit stuk software GPT-4V...
[3820.96 --> 3823.44]  en dat installeer je op een NS-kaartautomaat.
[3823.52 --> 3825.64]  Gewoon die grote gele bakken die staan bij de plein.
[3826.04 --> 3827.96]  En dan loop je daar naartoe en dan zeg je...
[3827.96 --> 3829.08]  Ik wil graag een kaartje.
[3829.26 --> 3830.20]  Naar Rotterdam Centraal.
[3830.26 --> 3831.74]  En dan zie jij gewoon een beeld zo...
[3831.74 --> 3832.32]  Boem, boem.
[3832.40 --> 3834.04]  Met misschien een cool toverstafje of zo.
[3834.16 --> 3835.88]  Misschien gewoon Merlijn de Tovernaar of Klippie.
[3835.98 --> 3837.34]  Maak me allemaal geen reet uit wat je doet.
[3837.66 --> 3838.32]  Of gewoon niks.
[3838.40 --> 3840.00]  En er komt gewoon een orb in beeld of zo.
[3840.60 --> 3842.68]  En dan tikt die gewoon door die interface heen.
[3842.86 --> 3844.78]  En interface-ontwerpers...
[3844.78 --> 3847.84]  Software-ontwikkelaars zijn allemaal nu tegen die podcast aan het zeggen...
[3847.84 --> 3848.08]  Ah!
[3848.58 --> 3849.34]  En ik denk...
[3849.34 --> 3851.06]  Yo, ik wil gewoon een kaartje kunnen bestellen.
[3851.12 --> 3851.58]  Ik ben blind.
[3852.22 --> 3853.34]  Dus zullen we het gewoon installeren.
[3853.56 --> 3856.36]  En dit is wat GPT-4 nu al kan.
[3856.70 --> 3857.60]  En goed ook.
[3858.12 --> 3859.24]  Ja, en dan weer...
[3859.24 --> 3860.06]  Ik vertel dit.
[3860.16 --> 3861.72]  Weer zo'n toepassing...
[3861.72 --> 3863.14]  waar je dan toch niet tegen kan zijn.
[3863.32 --> 3863.40]  Ja.
[3863.56 --> 3864.08]  Dat is wel hartstikke leuk.
[3864.08 --> 3865.86]  Nou ja, en een kaartje kopen...
[3865.86 --> 3866.96]  is natuurlijk heel simpel.
[3867.10 --> 3869.40]  Maar stel je voor dat je dit kan doen voor...
[3869.40 --> 3871.48]  Je wil een restaurant boeken.
[3871.48 --> 3873.88]  En je weet alleen nog maar dat je een Indiaas restaurant wil.
[3873.88 --> 3874.84]  Maar nog niet welke.
[3875.20 --> 3876.38]  En dat die plek is en zo.
[3876.80 --> 3878.74]  Dat is maar iets uitgebreidere stappen.
[3878.80 --> 3880.26]  Dan gaat het natuurlijk echt heel erg leuk.
[3880.26 --> 3883.04]  Nou ja, wat je denk ik gaat zien is dat er...
[3883.04 --> 3884.56]  Er is namelijk al software die dit doet.
[3884.68 --> 3885.96]  Alleen dan moet je dat allemaal laten maken.
[3886.04 --> 3887.40]  En dat kost best wel wat tijd en geld.
[3887.94 --> 3888.94]  Maar...
[3888.94 --> 3890.06]  Stel je voor...
[3890.06 --> 3892.90]  Ergens in Duitsland is een boekingsysteem...
[3892.90 --> 3894.56]  voor een grote hostelorganisatie...
[3894.56 --> 3895.84]  die draait op Windows NT.
[3896.14 --> 3897.86]  Dat is een heel oud bestudingssysteem.
[3898.26 --> 3899.56]  En dat is een stuk Windows software.
[3899.66 --> 3900.44]  Die is gewoon rock solid.
[3900.58 --> 3901.36]  Het is gewoon hartstikke goed.
[3901.50 --> 3903.46]  Alleen ja, iedereen die daar nieuw komt werken...
[3903.46 --> 3905.12]  moet leren werken met dat boekingsysteem.
[3905.28 --> 3907.36]  En het koppelen aan het internet was helemaal een draak...
[3907.36 --> 3908.16]  want er was geen API.
[3908.82 --> 3910.28]  Nou, nu zou je kunnen zeggen...
[3910.28 --> 3913.50]  Zet GPT-4V op al die oude Windows NT-systemen.
[3913.58 --> 3915.18]  Dus dat moet één keer een connector bouwen.
[3915.18 --> 3918.06]  En als er dan een boeking geplaatst wordt via de app...
[3918.06 --> 3918.96]  dan gaat hij gewoon klikken.
[3919.08 --> 3921.18]  En dan klikt hij gewoon in dat oude systeem...
[3921.70 --> 3923.04]  die boeking aan elkaar.
[3923.26 --> 3924.84]  En ik denk dus dat je op deze manier...
[3924.84 --> 3927.08]  een soort hele bizarre bruggen kan gaan bouwen...
[3927.08 --> 3927.80]  en API's kan maken.
[3927.80 --> 3928.98]  Geweldig nieuws van Geert Wilders.
[3929.04 --> 3930.18]  Want die wil graag de boodschappen...
[3930.78 --> 3932.34]  het BTW-boodschappen verlagen.
[3932.46 --> 3933.88]  En dat gaat Belastini's natuurlijk niet kunnen.
[3934.04 --> 3936.84]  Dus dan misschien kan GPT-4 met COBOL praten.
[3936.96 --> 3938.06]  Nou, is dat ook weer opgelost.
[3938.16 --> 3938.68]  Heel fijn.
[3939.48 --> 3939.70]  Tot volgende week.
[3939.70 --> 3940.46]  Laat weer bij achter.
[3940.64 --> 3940.96]  Doei!
[3945.18 --> 3949.24]  En, ben je er al achter of Eneco dynamisch bij je past?
[3949.80 --> 3950.64]  Of nog niet?
[3951.30 --> 3953.70]  Doe de test op eneco.nl slash test.
[3954.80 --> 3956.78]  Mensen helpen een bewuste keuze te maken.
[3957.68 --> 3958.28]  We doen het nu.
[3958.76 --> 3959.26]  Eneco.

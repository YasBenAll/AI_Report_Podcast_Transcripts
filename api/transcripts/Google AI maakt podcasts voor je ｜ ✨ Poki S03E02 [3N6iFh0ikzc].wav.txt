Video title: Google AI maakt podcasts voor je ｜ ✨ Poki S03E02
Youtube video code: 3N6iFh0ikzc
Last modified time: 2024-09-12 21:52:06

------------------ 

[0.00 --> 6.28]  Stel je voor, je wint met je buren 1 miljoen euro. Wat zou je doen? Een verre reis? Een nieuwe bank kopen?
[6.66 --> 9.00]  Denk er maar vast over na, want...
[9.64 --> 12.32]  De Postcode Loterij Miljoenen Race is gestart!
[13.00 --> 18.20]  We racen door heel Nederland om bij maar liefst 12 postcodes 1 miljoen euro uit te rijken.
[19.16 --> 24.44]  Wil jij ook kans maken met jouw postcode? Ga naar postcodeloterij.nl en speel mee.
[24.98 --> 26.32]  18 plus speel bewust.
[26.32 --> 32.12]  In de podcast Zware Jongens krijg je wekelijks duiding bij het misdaadnieuws van het moment.
[32.76 --> 38.70]  Samen met Jelle Tielemann van het AD en Jan Meijers van NRC bespreek ik, Harry Lensing van Follow the Money,
[39.00 --> 42.52]  de laatste ontwikkelingen in de strijd in en tegen de onderwereld.
[43.32 --> 46.78]  Je vindt Zware Jongens iedere week in je favoriete podcast-app.
[47.08 --> 51.68]  Welkom bij Palky, de Nederlandse podcast over het onderwerp van deze tijd, AI.
[52.06 --> 55.64]  Waar we zoeken welke invloed kunstmatige intelligentie gaat hebben op het leven van de mens.
[55.64 --> 57.16]  En onze samenleving.
[57.36 --> 61.20]  Aan tafel, Wietsehagen, Milou Brandt en ik, Alexander Kluping.
[61.58 --> 62.20]  Milou, wat gaan we doen?
[62.34 --> 65.14]  Ja, jongens, dit is volgens mij echt een nieuw tijdperk aangebroken.
[65.30 --> 69.44]  Want de allereerste autonome AI-beschavingen die zijn ontstaan.
[69.56 --> 70.54]  Dan denk je misschien, wat dan?
[71.04 --> 73.02]  Het is ook nog maar in Minecraft, het valt allemaal mee.
[73.18 --> 75.40]  Oké, ik dacht even dat je het over religies ging hebben.
[75.40 --> 81.64]  Nou, we gaan het ook over religie hebben en we gaan het over cultuur en economie en over hoe je een currency kan ontwikkelen.
[81.68 --> 85.10]  Voor sommige mensen is, als jij zegt het is maar in Minecraft, wel een belediging hè?
[85.60 --> 87.12]  Ja, voor mij, kijk, ik ben niet heel bekend.
[87.12 --> 89.62]  Een volwaardige beschaving in Minecraft.
[89.84 --> 90.72]  Ja, inderdaad.
[90.80 --> 92.02]  En ik dacht, holy moly.
[92.12 --> 93.54]  Maar ik ben heel benieuwd hoe jullie dit zien.
[94.08 --> 96.66]  En jullie kunnen mij vast ook vertellen over die nieuwe iPhone en al zijn voefjes.
[96.84 --> 97.24]  Dat gaan we doen.
[97.24 --> 98.60]  Veel plezier bij Palky.
[110.16 --> 111.32]  Ja, hij is er dan.
[111.48 --> 112.40]  De iPhone 16.
[113.00 --> 116.16]  Ja, kijk, ik had jullie al verteld, ik had een iPhone 14 gekocht.
[116.26 --> 117.40]  Net een paar maanden geleden.
[118.06 --> 121.48]  En toen hoorde ik dat al die AI op die iPhone zou komen.
[121.58 --> 122.34]  Die nieuwste iPhone.
[122.54 --> 126.02]  En toen dacht ik, ja, zit ik weer, hoppel ik weer achter de meute aan.
[126.02 --> 130.66]  Maar het stelt mij enigszins gerust om te weten dat jullie ook geen AI krijgen.
[130.78 --> 132.94]  Al zou je een nieuwe iPhone 16 kopen.
[133.02 --> 136.72]  Je bedoelt te zeggen, de EU staat niet toe dat wij de leuke dingetjes krijgen.
[136.72 --> 137.00]  Ja.
[137.28 --> 138.94]  Dus dan hoeven wij ons ook geen te maken.
[138.94 --> 140.66]  Nee, het stelt enigszins gerust.
[140.74 --> 142.54]  De software komt wat later, heb ik begrepen.
[142.88 --> 147.64]  Dus als je de iPhone 16 koopt, die wel wat beter voorbereid is op AI-achtige zaken.
[147.78 --> 151.22]  Dan zit daar nog een operating system op waar al die coole dingen die ze hebben aangevond,
[151.34 --> 155.00]  gekondigd onder de naam Apple Intelligence, nog niet inzitten, want het is nog niet af.
[155.00 --> 156.72]  En ze gaan het naar ons toedruppelen.
[157.22 --> 161.36]  Maar wat misschien wel een mooi detail is, wat ik uit die hele, nou ja, het was een soort
[161.36 --> 164.94]  lege presentatie met toch niks, maar uiteindelijk ben ik toch een beetje graaf en dan vind je
[164.94 --> 165.68]  toch wel weer iets.
[166.06 --> 167.64]  Want wat is dan nu het verhaal?
[167.92 --> 172.02]  Die telefoons van Apple worden ongeveer twee jaar van tevoren al helemaal vastgesteld
[172.02 --> 172.50]  wat dat wordt.
[172.58 --> 173.92]  Want je moet een productielijn opzetten.
[174.34 --> 175.30]  Dus wat wij zien als...
[175.30 --> 176.64]  De iPhone 18 is nu af?
[177.76 --> 178.08]  Ongeveer.
[178.52 --> 181.44]  En dan kunnen ze in de last minute nog een paar kleine dingen in doen, maar je moet wel
[181.44 --> 183.98]  een hele productielijn en nadenken over die...
[183.98 --> 184.96]  Je moet contracten maken.
[185.30 --> 190.06]  Dat is waar Tim Cook heel goed in is, daarom gaat het zo goed met Apple financieel.
[190.94 --> 195.00]  En het is eigenlijk zo dat die hele taalmodellen waar wij het de hele dag over hebben in Pocky
[195.00 --> 196.82]  een beetje een verrassing waren voor Apple.
[196.94 --> 198.58]  Dus die toestellen zijn niet voorbereid.
[198.78 --> 201.70]  En zij zitten heel erg op weinig geheugen als in weinig intergeheugen.
[201.70 --> 204.28]  Dat is altijd het Apple stokpaardje geweest.
[204.38 --> 207.80]  Kijken ze met hoe weinig chips wij toch een hele mooie ervaring kunnen bieden.
[208.20 --> 212.16]  En nu hebben ze gewoon veel meer intergeheugen nodig, want taalmodellen vreten geheugen.
[212.80 --> 216.52]  En waarschijnlijk, want dat hebben ze dus helemaal van de website afgehouden.
[217.00 --> 220.04]  Een beetje expres, want dat kan je achteraf nog wel een beetje aanpassen, net voordat je
[220.04 --> 220.52]  hem lanceert.
[220.96 --> 223.34]  Zit er acht gigabyte intergeheugen in de iPhone.
[223.50 --> 226.40]  En met intergeheugen bedoel ik dus werkgeheugen, niet je storage.
[226.60 --> 228.90]  En dat is voor Apple begrippen superveel.
[229.00 --> 230.70]  Want wat zit er in mijn telefoon denk je?
[230.70 --> 232.40]  Vier of drie of twee.
[233.14 --> 234.96]  Zo'n minst wat ze erin kunnen stoppen.
[235.04 --> 236.62]  Dat chipje kost echt geen drol trouwens.
[236.72 --> 240.98]  Maar dat is ook wel een beetje een uitdaging voor ontwikkelaars en Apple zelf om het een
[240.98 --> 242.86]  beetje in een lean systeem te houden.
[242.94 --> 245.72]  Je kunt er natuurlijk chips tegen aangooien om dingen op te lossen, maar het is technisch
[245.72 --> 247.96]  altijd beter om een beetje een budget te hebben.
[248.32 --> 252.70]  Maar goed, al met al, ze noemen het de first AI iPhones.
[252.94 --> 254.44]  Dat is een beetje kletskoek.
[254.44 --> 256.16]  Ja, want wat bedoelen ze daar nou mee?
[256.36 --> 258.70]  Want er is een hoop aangekondigd qua software.
[258.94 --> 262.70]  Dat was op zich niet nieuw ten opzichte van de vorige keynote van Apple.
[263.18 --> 267.42]  Er waren wat kleine verschillen, maar voor de groot deel was het herhaling van hoeveel
[267.42 --> 268.64]  beter serie is geworden.
[269.14 --> 272.04]  Maar wat is er nou precies anders aan die hardware?
[272.28 --> 274.78]  Anders dan dat er meer interne geheugen in zit?
[274.78 --> 276.10]  Iets meer intern geheugen.
[276.12 --> 276.52]  That's it.
[276.64 --> 281.70]  Geen radicale stappen qua Tensor course, dus machine learning course die AI optimaliseren.
[282.16 --> 285.80]  Het woord AI is één keer gevallen tijdens de hele keynote, er is geteld.
[286.08 --> 288.44]  Want ze noemen het of Apple Intelligence of Machine Learning.
[288.46 --> 288.98]  Ah zo, ja.
[289.34 --> 290.24]  Maar dat telt niet.
[290.38 --> 294.62]  Ja, om niet te voelen als een soort van we willen Google verslaan met nog vaker AI roepen.
[294.86 --> 295.00]  Ja.
[296.00 --> 299.40]  Al met al, het is niet helemaal leeg, nep en een soort leugen.
[299.40 --> 304.72]  Maar de first iPhone designed from the ground up for AI is grotendeels fluff.
[305.00 --> 308.36]  Dit is niet heel ver van wat ze twee jaar geleden al op papier hadden staan.
[308.92 --> 314.18]  Maar het is inderdaad wel zo dat deze meer geschikt is om een lokaal touwmodel op te draaien
[314.18 --> 316.06]  en een betere serie op te hebben.
[316.20 --> 317.30]  Dus het is een beetje waar.
[317.38 --> 322.56]  Maar de volgende iPhone, daar ga je veel meer ingoeden op zien van deze tijd waar we nu leven.
[322.72 --> 324.78]  Voornamelijk marketinggelul dus, als ik het goed begrijp.
[324.80 --> 325.16]  Grotendeels.
[325.16 --> 329.22]  Ja, want Tim Cook heeft wel gezegd, de iPhone 16 is ontworpen voor Apple Intelligence.
[329.80 --> 330.32]  Dat is gewoon niet waar.
[330.78 --> 331.44]  Nou ja, goed.
[332.20 --> 334.46]  Daar kan je veel verschillende dingen van maken.
[334.70 --> 337.06]  Wat zij hopen dat jij denkt dat dat betekent, is niet zo.
[337.52 --> 338.90]  Maar het is ook niet helemaal niet waar.
[339.18 --> 340.22]  Moet we het dan nog over dat ding hebben?
[341.16 --> 343.10]  Ja, het is ingewikkeld.
[343.26 --> 345.20]  Want wat kunnen we er nou mee als consument?
[345.28 --> 348.18]  Er zijn heel veel dingen aangekondigd waar wij in Europa niet zoveel mee kunnen.
[349.06 --> 349.42]  Nog niet?
[349.64 --> 349.88]  Nee.
[350.22 --> 353.16]  Nou, misschien als ik, want ik heb een beetje zitten graven.
[353.16 --> 355.72]  Kijk, die action button, die er al was op de pro.
[355.72 --> 359.44]  Dus dat is een, ik voel me bijna een soort clown om hierover te praten.
[359.56 --> 363.44]  Maar er is een knopje toegevoegd aan de vorige iPhone Pro, waardoor je een actie kan doen.
[363.54 --> 365.04]  Dus dan druk je erop het gatje diktofoon aan.
[365.36 --> 367.92]  Die zit nu ook in de normale iPhone, de niet-Pro.
[368.48 --> 373.10]  Dat knopje is wel lekker, want dan kan je heel snel op drukken en dan een audionotitie doen,
[373.18 --> 375.88]  die dan misschien meteen getranscribeerd wordt en in je mail terechtkomt.
[375.96 --> 376.96]  Met één druk op de knop.
[377.30 --> 379.84]  Misschien zelfs, dus je pakt hem uit je zak.
[379.84 --> 381.84]  Je houdt het knopje in zonder naar je telefoon te kijken.
[381.92 --> 383.50]  Je praat het tegen, je stopt hem terug in je zak.
[383.98 --> 385.80]  Dat is een soort AI-achtig knopje.
[385.94 --> 389.48]  Ja, ik vind dat je Apple nu heel veel credits geeft, want dit is fantasie vooral.
[389.68 --> 390.24]  Nou ja, precies.
[390.34 --> 393.88]  Dat zou er dan nog kunnen komen, maar de hardware-knopje is er.
[394.20 --> 398.50]  En die camera, whatever surface ding wat ze hebben toegevoegd, een soort van kleine touchpad
[398.50 --> 400.46]  aan de zijkant van je telefoon, beide modellen.
[400.98 --> 404.18]  Die zou je mooi in kunnen zetten om een beetje de film Heur na te bootsen,
[404.24 --> 406.52]  dat je hem omhoog houdt en naar iets wijst.
[406.52 --> 408.12]  En dan zegt, waar kijk ik nu naar?
[408.46 --> 411.44]  En dat dan in je Airpods terugkomt, joh, je loopt op dit moment door Amsterdam.
[411.64 --> 413.00]  De OV-fietsen staan daar.
[413.76 --> 414.54]  Allemaal op tast.
[414.72 --> 418.90]  Ik vind dit heel veel credits voor in feite voor een Google Lens.
[419.00 --> 420.00]  Want dat is wat het is.
[420.04 --> 424.04]  Je maakt een foto over iets en in de keynote laat ze dan een restaurant zien waar meneer voor staat.
[424.22 --> 425.60]  Dan neemt hij de foto van het restaurant.
[426.32 --> 430.04]  En dan zegt Siri, dit is het restaurant en dan is het open ja of nee.
[430.16 --> 430.92]  Nou, geweldig.
[431.20 --> 434.34]  In plaats van dat je die twee meter verder loopt om naar de openingstijden te kijken,
[434.34 --> 436.62]  op de muur hangen, kun je het dan nu op je telefoon kijken.
[436.62 --> 437.84]  Ja, de menukaarten en zo.
[438.46 --> 440.46]  Ik dacht ook, ik heb dit al wel een beetje gezien.
[440.76 --> 442.40]  Maar ja, leuk als de iPhone dat straks kan.
[442.60 --> 443.64]  Hier kan het nog niet eens dus.
[443.96 --> 445.10]  We zijn wel negatief hè jongens.
[445.62 --> 445.96]  Het was ook...
[445.96 --> 446.32]  Niet hoor.
[446.50 --> 446.82]  Nee, ik bedoel.
[447.00 --> 451.90]  Nee, maar jij bent niet negatief omdat jij nu zo'n soort van extra poleert wat er zou kunnen komen.
[451.90 --> 452.10]  Ja.
[452.22 --> 453.42]  En daarom ben je positief.
[453.44 --> 453.80]  Ik ben echt...
[453.80 --> 458.12]  Als we kijken wat er in feite kan, is er niet zoveel nieuws ten opzichte van wat ze vorige keer gezegd hebben.
[458.20 --> 459.56]  Ik vond het bizar veel herhaling.
[460.06 --> 462.90]  Terwijl ze over de hardwereld om zeggen, ja, hij is helemaal geoptimaliseerd voor AI.
[463.12 --> 464.32]  Nou, dat is dus gewoon gelul.
[464.80 --> 467.24]  Veel van die AI dingen kunnen wij niet gebruiken in Europa.
[467.76 --> 473.90]  En de dingen die ze hebben verteld over Amerika, ja, is interessant.
[474.90 --> 476.26]  Maar daar hebben we de vorige keer al over gehad.
[476.26 --> 478.24]  Ja, hij kan e-mails voor je samenvatten.
[478.28 --> 479.68]  Hij kan meldingen samenvatten.
[479.72 --> 480.20]  Ik ben...
[480.20 --> 481.96]  Hij zet de belangrijkste mails bovenaan.
[482.16 --> 483.06]  Dat is chill.
[483.20 --> 484.68]  Ik ben hyped over mijn fantasie.
[485.08 --> 486.06]  Ja, oké.
[486.28 --> 487.26]  En er was...
[487.90 --> 490.26]  Er circuleert op Twitter...
[490.82 --> 495.70]  Je kan dus een foto maken van bijvoorbeeld een hond die je tegenkomt in het park.
[495.80 --> 497.76]  En dan vraag je aan een baas van, mag ik een foto maken van je hond?
[498.06 --> 498.86]  Ja, mag wel.
[499.30 --> 503.82]  En dan maak je een foto en dan kan die telefoon je dus vertellen wat voor soort hond dat is.
[503.82 --> 506.86]  Dus in plaats van dat je dan nog hoeft te vragen, oh, wat voor hond is het?
[507.08 --> 509.66]  Kun je nu dus eerst een foto maken en dan hoor je het van je telefoon.
[509.94 --> 518.24]  Dus mensen maken daar een beetje grapjes over van, ja, het haalt ook gewoon weer nieuwe manieren van intermenselijke communicatie gewoon weg.
[518.26 --> 520.20]  Dit was een van de raarste momenten die ik ooit...
[520.20 --> 521.34]  Ik heb Apple Kino's gezien.
[521.34 --> 522.06]  Wat een raar ding, toch?
[522.08 --> 522.54]  Ik zat dit te kijken.
[522.76 --> 522.84]  Ja.
[523.04 --> 526.44]  En dan de situatie is dus, meneer komt aanlopen met hond.
[526.64 --> 526.80]  Ja.
[527.04 --> 529.60]  Persoon met iPhone wil weten wat voor hond het is.
[529.60 --> 533.22]  En in plaats van dat meneer vraagt, wat voor hond heb je?
[533.42 --> 536.12]  Vraagt hij, mag ik een foto maken van uw hond?
[536.24 --> 536.36]  Ja.
[536.48 --> 538.86]  En dan vraagt hij aan Siri, wat voor hond is dit?
[538.98 --> 541.06]  Waar die man bij staat met zijn hond?
[541.46 --> 541.70]  Ja.
[542.18 --> 542.88]  Ja, dat is belangrijk.
[543.04 --> 545.62]  Apple is zo'n raar bedrijf af en toe.
[545.76 --> 546.20]  Oh, ja.
[547.04 --> 548.68]  Soms zijn het echt aliens.
[549.22 --> 556.14]  Ook dat je het soort van, dan is er iemand die koffie wil bestellen, die houdt gewoon zijn AirPods in.
[556.14 --> 560.48]  Gaat gewoon praten met die meneer van die koffiestand met zijn koptelefoon in.
[560.58 --> 564.18]  Die meneer van die koffiestand, die moet echt denken, oké, hoor jij mij?
[564.48 --> 566.96]  Apple wil dan zeggen, ja, we zetten de muziek automatisch zachter.
[567.24 --> 568.44]  Het is toch gewoon raar gedrag?
[568.62 --> 568.76]  Ja.
[569.36 --> 569.86]  Maar wacht, wacht, wacht, wacht.
[569.86 --> 571.00]  Welk probleem los je op?
[571.22 --> 572.70]  Nee, maar dat is gewoon enthousiasme van nerds.
[572.96 --> 573.66]  Ik begrijp het wel.
[573.74 --> 575.52]  Nee, ik wil niet zeggen, wacht, wacht, doe niet zo negatief.
[575.60 --> 575.90]  Dat is prima.
[576.08 --> 576.72]  Ik hoor je wel.
[576.82 --> 581.46]  Maar ik wou nog zeggen, er zit wel een interessante destigmatisering op topjes in te houden.
[581.60 --> 583.58]  Wat ik ook asociaal vind, hoornopjes in te houden.
[586.14 --> 591.60]  Hearing aids zijn geworden en het zijn een soort gehoorapparaatjes geworden voor mensen met niet zo goed als een geheel gehoorapparaat,
[591.76 --> 600.12]  maar goed genoeg als je een klein beetje gehoorverlies hebt om earpods van 180 dollar of de earpods pro, whatever, 250.
[600.32 --> 602.60]  Het is wel geld, maar niet 1500 euro.
[602.60 --> 603.90]  Er zit een gehoorapparaat in vanaf nu.
[604.08 --> 605.96]  Ja, en alleen in de earpods pro.
[607.00 --> 611.18]  En dan is het natuurlijk wel fijn dat we niet iedereen gaan shamen die nog earpods inhoudt,
[611.20 --> 612.84]  dat het ook gehoorapparaatjes kunnen zijn.
[612.84 --> 614.06]  Het is gewoon ongemakkelijk.
[614.38 --> 614.62]  Zeker.
[614.62 --> 618.42]  Je weet niet, je kan als buitenstaander niet zien of iemand muziek aan het luisteren is,
[618.48 --> 620.14]  of dat hij zich gebruikt als gehoorapparaat.
[620.46 --> 627.58]  Dus ik vind gewoon Apple, een bedrijf wat zichzelf prijst om haar human technology interaction,
[628.02 --> 629.24]  vind ik dit gewoon heel raar.
[629.42 --> 632.00]  Het is wel een heel raar ding dat je dat niet kan zien.
[632.28 --> 633.16]  Ik ben boos op Apple.
[633.54 --> 634.84]  Laten we snel doorgaan met triggerpen.
[634.84 --> 637.56]  Dan OpenAI.
[638.26 --> 639.30]  Ze hebben al heel veel geld.
[639.44 --> 640.38]  Ze gaan nog meer geld ophalen.
[640.52 --> 645.70]  Ja, ze hebben meer geld nodig om meer computerkracht te kopen, want daar gaat het geld aan op,
[645.74 --> 646.76]  behoefte aan rekenkracht.
[646.86 --> 650.86]  Ze hebben andere operationele kosten en ze willen later dit jaar werknemers de mogelijkheid
[650.86 --> 652.38]  bieden om aandelen te verkopen.
[653.18 --> 656.14]  Dat gaat dus van het portemonneetje af wat OpenAI heeft.
[656.28 --> 660.94]  Dus we zijn in gesprek om 6,5 miljard dollar op te halen, wat het bedrijf zou waarderen op 150 miljard.
[661.74 --> 664.86]  Dat is een aardige stijging ten opzichte van de waardering van eerder dit jaar.
[664.96 --> 666.66]  Die was 86 miljard.
[666.66 --> 668.54]  Dat is toch een slordige verdubbeling.
[669.08 --> 675.58]  En daarnaast zijn ze ook nog bezig om een kredietfaciliteit, oftewel een lening, op te vragen bij de banken van nog een keer 5 miljard.
[675.78 --> 678.10]  Dus de miljarden vliegen je om de oren.
[678.22 --> 679.16]  Microsoft doet weer mee.
[679.82 --> 681.04]  De ronde wordt geleid door Thrive.
[681.14 --> 682.52]  Dat is een grote VC.
[683.20 --> 687.08]  En interessant, Apple en Nvidia zouden ook mogelijk gaan investeren.
[687.86 --> 689.22]  Dat doen ze nu nog niet, toch, Wietse?
[689.58 --> 690.60]  Nou, niet zo direct.
[690.78 --> 692.50]  Ik bedoel, ik denk dat er kortingen gegeven worden.
[692.50 --> 696.50]  En Apple maakt een partnership met Apple en dan krijgen zij dat gratis.
[696.64 --> 697.10]  Maar toegang tot...
[697.10 --> 699.94]  Dus het zijn een beetje van die barters dat je je portemonnee dichthoudt.
[700.34 --> 704.98]  Maar als ik dit zo hoor, klinkt het vooral, wij willen dat OpenAI blijft bestaan.
[705.16 --> 706.88]  Dat is in ons allerbelang.
[707.02 --> 709.56]  Dus we gaan mee investeren in iets wat voor ons belangrijk is.
[710.42 --> 711.70]  Dus zo zou ik hem lezen.
[712.66 --> 713.84]  En de navolging daarop.
[713.94 --> 717.90]  Er is weer een extra nieuwtje over Strawberry, waar we het vorige week over hebben gehad.
[718.00 --> 718.50]  Dat nieuwe...
[719.60 --> 721.80]  Nou ja, is het een nieuw taalmodel? Mag ik het zo noemen?
[721.80 --> 724.70]  Ja, of een nieuwe strategie van het verbeteren van taalmodellen.
[724.86 --> 726.58]  Nou, dat komt dus al over twee weken.
[726.66 --> 729.76]  We wisten dat het deze herfst zou komen, maar het komt over twee weken.
[729.96 --> 730.68]  Vroege herfst.
[731.10 --> 733.54]  Het zou in de vroege herfst komen.
[733.72 --> 733.80]  Ja.
[733.94 --> 734.78]  Ja, is het al...
[734.78 --> 736.10]  Nou, buiten is het al herfst.
[736.52 --> 739.00]  Dus van mij mag Strawberry ook wel komen, inderdaad.
[739.02 --> 739.86]  Ik heb er geregend vanochtend, dus van mij is het herfst.
[740.08 --> 741.06]  Eetje, die hagel.
[741.06 --> 741.88]  Maar het komt vroeg.
[742.00 --> 744.28]  Dat is vroeger dan mensen aannamen.
[744.48 --> 744.70]  Ja.
[745.10 --> 747.24]  En wat maakt het nou extra bijzonder?
[747.34 --> 749.24]  Er is weer net een extra deurtje open gegaan.
[749.24 --> 750.58]  We weten nu weer net iets meer.
[751.14 --> 752.84]  Het gaat een denkpauze nemen.
[753.10 --> 754.10]  Dus je vraagt het iets.
[755.04 --> 757.58]  En dan neemt het even tien, twintig seconden om even na te denken.
[757.96 --> 759.42]  Ja, ik weet niet wat hij dan allemaal aan het doen is.
[759.50 --> 761.04]  Maar het heet denktijd.
[761.24 --> 764.30]  Dus dat ding gaat nadenken voordat hij jou een antwoord geeft.
[764.72 --> 768.08]  En dat zou dan moeten leiden tot betere verwerking van hele complexe vragen.
[768.08 --> 771.98]  Toch denk ik, dat voelt een beetje achteruit.
[772.10 --> 774.12]  Dat we langer moeten wachten tot er weer een antwoord.
[774.32 --> 779.46]  Nou ja, je hebt voor de luisteraars die daar wat dieper in willen duiken een paper die heet Verify Step by Step.
[779.60 --> 781.64]  En dat gaat eigenlijk over Thinking Fast, Thinking Slow.
[781.74 --> 782.40]  Dat is weer een boek.
[782.74 --> 786.18]  En het idee is dat hij even uitzoomt en zijn eigen logica gaat volgen.
[786.70 --> 787.40]  Wat heb je nou bedacht?
[787.48 --> 789.34]  Ga nou stap voor stap kijken naar wat je zelf zegt.
[789.46 --> 791.90]  En dan dat weer reflecteren en dan aanpassen waar nodig.
[791.90 --> 793.12]  En dat klinkt heel gek.
[793.22 --> 796.78]  Want dan zeg je eigenlijk bedenk iets en ga dan kijken naar wat je zelf hebt bedacht.
[796.92 --> 798.66]  Maar wij doen dat ook wel heel vaak.
[798.78 --> 801.36]  Ja, maar ik snapte dat dat voor mensen tijd kost.
[801.46 --> 807.34]  Maar ik dacht gewoon dat AI dat ook in een mum van tijd letterlijk zou kunnen doen.
[807.34 --> 810.34]  Ja, dat zegt ook hoe zwaar deze systemen dus uiteindelijk wel echt zijn.
[810.58 --> 811.50]  Dus het kost echt tijd.
[811.64 --> 816.26]  Ja, ik bedoel, er is heel veel software waarin gedaan wordt alsof dingen tijd kosten.
[816.42 --> 818.52]  Zodat je als gebruiker denkt dat je niet voor niets betaalt.
[818.74 --> 819.78]  Ik bedoel, sorry dat gebeurt.
[819.78 --> 821.48]  Grote loaders die er helemaal niet nodig zijn.
[821.60 --> 822.94]  Zo, dit is echt veel waard.
[822.98 --> 824.10]  Daar heeft hij heel lang over nagedacht.
[824.56 --> 825.58]  Maar dit is wel waar.
[825.70 --> 828.24]  Want zij willen het liefst zo snel mogelijk antwoorden eruit gooien.
[828.26 --> 832.18]  Maar ik ga ervan uit dat er een soort optimum zit tussen langer de tijd nemen.
[832.28 --> 834.88]  En dan de kwaliteit van een antwoord verbeteren.
[834.88 --> 838.56]  Want blijkbaar, weet je, ik herken dat een beetje als je een prompt invoert.
[838.72 --> 839.96]  En hij komt met een antwoord.
[840.46 --> 843.80]  Bijvoorbeeld dat hij een antwoord geeft wat tegenvalt.
[844.38 --> 849.12]  En dat een prompt teruggeven als in doe nog wat beter je best.
[849.12 --> 850.88]  Of kijk hier nog eens een keer naar.
[851.18 --> 852.74]  Of ik vind dit niet goed genoeg.
[853.10 --> 855.48]  Doe het zeg maar, breid je antwoord uit of zo.
[856.08 --> 858.18]  Dat dat heel vaak voor mij heel goed werkt.
[858.84 --> 861.64]  Dus niet zomaar genoeg nemen met het eerste antwoord.
[861.82 --> 863.10]  Wat mij dan doet vermoeden.
[863.54 --> 866.54]  Ik heb dan liever dat dat ding inderdaad wat langer nadenkt.
[866.62 --> 868.88]  En gewoon gelijk het uitgebreidere antwoord geeft.
[869.04 --> 870.60]  In plaats van het korte antwoord.
[870.92 --> 874.12]  Ik ga ervan uit dat er een soort optimum zit in langer nadenken.
[874.12 --> 877.00]  En dat het op een gegeven moment niet meer uitmaakt.
[877.08 --> 878.46]  Het gaat ervan uit dat het op een soort curve zit.
[878.52 --> 882.24]  Het is niet als hij een jaar gaat nadenken dat het antwoord substantieel beter is dan dat hij een minuut nadenkt.
[882.26 --> 885.10]  In al die papers zitten precies de grafieken die jij beschrijft.
[885.10 --> 889.08]  Dus hoe kan zo'n systeem dan bedenken, nu heb ik genoeg nagedacht.
[889.22 --> 889.80]  Hoe werkt dat?
[889.90 --> 891.38]  Nou dat is heel moeilijk, dat weet ik niet.
[891.54 --> 895.58]  Want uiteindelijk zal je, kijk de kwaliteit van de antwoorden worden door gebruikers doorgegeven.
[895.68 --> 897.32]  Met duimpjes en dingetjes naar beneden.
[897.32 --> 900.62]  Dus op een bepaalde manier kan je dan een signal geven aan het hele systeem.
[901.38 --> 905.62]  En dan kan je na een maand al zeggen van joh hier ging die 20 seconden nadenken, hier 10.
[905.82 --> 910.54]  Dus op die manier denk ik kan je gemiddelde uitvinden voor dat specifieke model hoe lang het zin heeft.
[911.20 --> 916.36]  En de vragen die je nu stelt is exact de fundamentele wetenschap rondom taalmodellen.
[916.60 --> 921.08]  Die dus in die papers, hele saaie papers met alleen maar getalletjes uitgedacht worden.
[921.52 --> 923.72]  Waar zitten die optimale denktijden?
[923.72 --> 930.48]  Waar zitten die optimale aantal parameters om een manier te vinden om het allemaal sneller en vooral goedkoper te doen?
[930.72 --> 934.90]  Want concreet, ik kreeg een mailtje van OpenAI gisteren.
[935.06 --> 939.26]  Dat is niet omdat ik een of andere coole gast ben, maar ik betaal hun voor de API.
[939.42 --> 940.26]  Dus dan krijg je updates.
[940.36 --> 946.86]  En die zeiden joh, als jij standaard GPT 4O aanroept in jouw applicatie, die gaan we vervangen voor een nieuwe.
[947.38 --> 950.64]  Dat moet je even weten, want als jij ervan uitgaat dat die op een bepaalde manier werkt.
[950.64 --> 956.58]  Dus je moet even heads up of je moet specifiek ons vragen om het oude model.
[956.70 --> 958.14]  Dat mag ook, maar dan moet je een datum toevoegen.
[958.26 --> 961.04]  En dan geven we je die oude die jij gewend bent, want dat is misschien belangrijk.
[961.62 --> 963.24]  En dan zeggen ze ook, waarom doen we die nieuwe?
[963.38 --> 966.80]  Nou, omdat we een manier hebben gevonden om hem 40% minder zwaar te maken.
[967.40 --> 969.18]  En dat is voor hun heel gunstig.
[969.28 --> 971.36]  Maar dat berekenen ze dan ook door aan de ontwikkelaar.
[971.48 --> 973.54]  Niet met 40%, maar met 20% kording.
[973.54 --> 983.54]  En dat komt allemaal door het steeds meer vinden van die optimale parameters om het zo goedkoop energiewijs te kunnen leveren.
[984.52 --> 986.28]  En hier is er nog een hoop in te doen.
[986.94 --> 993.60]  Ja, ik vind het een ervaring die tot nu toe best nog voor verbetering vatbaar is.
[993.72 --> 997.72]  Want soms dan stel je een vraag waarvan je gewoon wil dat hij het internet opgaat.
[997.72 --> 1000.12]  En soms wil ik dat hij een uitgebreide stopp.
[1000.18 --> 1002.84]  Bijvoorbeeld, stel ik wil een nieuwe rugzak kopen.
[1003.30 --> 1009.68]  Dan wil ik gewoon dat hij alle reviews gaat lezen die in mensen, weet je, in de geschiedenis van de mens zijn geschreven over rugzakken.
[1009.76 --> 1011.62]  Voordat hij mij vertelt wat de beste rugzak is.
[1011.70 --> 1013.32]  Ik wil gewoon dat hij dan zijn best gaat doen.
[1013.44 --> 1015.48]  In plaats van gewoon een standaard antwoord uit zijn corpus.
[1016.58 --> 1024.76]  Terwijl soms wil je juist een heel, bijvoorbeeld als ik vraag wat is 220 graden Celsius in varenheid.
[1024.76 --> 1027.08]  Dat is een vraag die ik vaak stel.
[1027.72 --> 1032.74]  Dan wil ik niet dat hij een heel Wolfram Alpha ding gaat doen.
[1032.90 --> 1035.08]  Of een Python script schrijft om dat te doen.
[1035.14 --> 1036.42]  Dan wil ik gewoon snel een antwoord.
[1036.50 --> 1038.10]  Het hangt van de vraag natuurlijk.
[1038.18 --> 1039.58]  En van ook de gebruiker.
[1040.02 --> 1041.46]  En waar hij het voor wil gebruiken.
[1041.64 --> 1043.48]  Dus het is heel situatieafhankelijk lijkt me.
[1043.60 --> 1045.52]  Dus dat kun je toch niet wetenschappelijk benaderen?
[1045.70 --> 1049.88]  Nee, maar ik denk dus wat ik nog niet heb gevoeld als ik met GPT praat.
[1049.88 --> 1051.88]  Dus de OpenAI interface is een tussenagent.
[1052.76 --> 1054.66]  Die, daar hadden we het vorige keer ook al over.
[1054.66 --> 1058.80]  Die een soort, hoe noem je het toen, bibliotheca's of tussenpersoon is.
[1058.90 --> 1059.42]  Eerste lijn.
[1060.08 --> 1060.36]  Luchtverkeersleider.
[1060.36 --> 1061.62]  Dankjewel, die zocht ik.
[1062.08 --> 1065.82]  Die gewoon zegt van nou, dit is echt een vraag voor een zwaar model.
[1065.92 --> 1067.66]  Die daar gewoon even drie minuten op gaat kouwen.
[1067.76 --> 1068.80]  En jou mailt als die het weet.
[1068.94 --> 1069.48]  Ja, precies.
[1070.16 --> 1071.30]  En dit is er een.
[1071.36 --> 1073.06]  Die doen we gewoon zero shot.
[1073.26 --> 1074.76]  Lekker snel antwoord.
[1075.00 --> 1077.98]  Want, en dit kan ook allemaal met duimpjes omhoog en naar beneden.
[1078.06 --> 1078.92]  Want dat lijkt heel suf.
[1078.98 --> 1079.90]  Maar dat is echt genoeg.
[1079.98 --> 1082.04]  Als je miljoenen hebt, mensen hebt die daar per dag op klikken.
[1082.04 --> 1085.44]  Om erachter te komen, wanneer hadden we gelijk om de zware in te schakelen.
[1085.52 --> 1087.36]  En wanneer hadden we gelijk om de lichten in te schakelen.
[1087.54 --> 1087.62]  Ja.
[1088.48 --> 1088.66]  Ja.
[1089.26 --> 1089.42]  Ja.
[1089.88 --> 1090.08]  Nou.
[1090.36 --> 1090.76]  Oké.
[1090.86 --> 1091.20]  Voorwaard.
[1091.26 --> 1091.96]  Pingel dan maar hè.
[1091.96 --> 1097.40]  Hier ben ik even helemaal ingedoken.
[1097.52 --> 1100.68]  Er is een revolutie in het geneesmiddelenonderzoek.
[1100.88 --> 1103.52]  Google DeepMind die heeft hier aan gewerkt.
[1103.70 --> 1109.70]  Dat is de AI-tak die allemaal onderzoek doet naar hoe we AI nog breder zouden kunnen toepassen in de samenleving.
[1110.04 --> 1113.42]  Zo ook in het geneesmiddelenvlak, in de medische wereld.
[1113.42 --> 1115.70]  En dit heet AlphaProteo.
[1116.02 --> 1121.22]  Dat is een slim computersysteem dat eiwitten kan bedenken en eiwitten kan ontwerpen.
[1121.42 --> 1121.94]  Eiwitten kan bedenken.
[1122.06 --> 1122.74]  Eiwitten, ja.
[1122.82 --> 1125.38]  En daarmee kunnen we nieuwe geneesmiddelen maken.
[1125.62 --> 1129.54]  Nou zal ik je even een kleine opfrisser geven over wat nou precies zo'n eiwit is.
[1129.54 --> 1130.86]  Jezus, je bent er echt ingedoken.
[1130.88 --> 1131.12]  Ja.
[1131.36 --> 1134.32]  En het is best ingewikkeld voor iemand die geen medische achtergrond heeft.
[1134.38 --> 1136.36]  Dat is ook meteen de disclaimer voor de rest van dit verhaal.
[1136.52 --> 1137.34]  Wat is een eiwit?
[1137.38 --> 1142.44]  Eiwitten zijn als het ware de arbeidertjes in je lichaam.
[1142.44 --> 1143.68]  Ze regelen van alles.
[1143.82 --> 1147.50]  Van celgroei tot koude en warmte.
[1147.64 --> 1149.70]  Tot afweer tegen ziektes.
[1150.18 --> 1150.88]  En eiwitten.
[1150.88 --> 1151.14]  Bieren.
[1151.14 --> 1152.08]  Spieren.
[1152.64 --> 1153.20]  Spieren wat?
[1153.42 --> 1154.92]  Nou ja, daar moet je ook eiwitten voor nemen.
[1155.12 --> 1155.52]  Daar ken ik het van.
[1155.52 --> 1156.66]  Ja, daar eet je het voor.
[1156.78 --> 1157.84]  Je eet het voor.
[1157.98 --> 1158.16]  Maar ook.
[1158.16 --> 1158.98]  Zo'n grote emmer.
[1159.16 --> 1160.48]  Ja, grote emmers heb ik thuis.
[1160.60 --> 1162.06]  Maar je bedoelt, het is ook nodig.
[1162.06 --> 1162.68]  In je bloed.
[1162.68 --> 1162.70]  In je bloed.
[1162.70 --> 1163.46]  Ja, oké.
[1163.46 --> 1165.66]  Het zijn een soort van, ze kunnen boodschappen, dingen.
[1165.74 --> 1167.28]  Ze regelen gewoon allemaal dingen.
[1167.40 --> 1167.66]  Juist.
[1167.66 --> 1170.10]  En ze hebben allemaal verschillende vormpjes.
[1170.10 --> 1170.22]  Ja.
[1170.22 --> 1173.26]  En het werkt dan als een soort van, ja, moleculair lego.
[1173.26 --> 1176.76]  Ze hechten zich aan een ander eiwit in ons lichaam.
[1177.72 --> 1181.98]  En zo kunnen ze op verschillende plekjes komen, maar kunnen ze ook dingen activeren in cellen.
[1181.98 --> 1185.90]  Of, nou ja, ze hebben allemaal boodschappen en functies.
[1186.38 --> 1187.14]  Ja, precies.
[1187.28 --> 1194.72]  Dus die binding, de binding van eiwitten, die is weer heel belangrijk voor allerlei processen op celniveau in ons lichaam.
[1194.78 --> 1194.94]  Ja.
[1195.34 --> 1197.46]  Ja, en wat kun je daar dan mee?
[1197.60 --> 1203.52]  Nou, het kan dus helpen in het genezen van kanker, diabetes en het kan helpen virussen te bestrijden.
[1203.52 --> 1204.10]  Ah, oké.
[1204.14 --> 1205.22]  Dus dit is wat ze altijd zeggen.
[1205.22 --> 1211.64]  Een soort van, gaat AI helpen met het oplossen van ziektes die we nu niet kunnen bestrijden, zoals kanker?
[1211.64 --> 1212.08]  Ja.
[1212.18 --> 1216.00]  Dan gaan ze dus proteïne ontwerpen om dat te kunnen doen.
[1216.50 --> 1216.84]  Klopt.
[1217.04 --> 1218.56]  Ja, dat is eigenlijk precies wat er gebeurt.
[1218.66 --> 1220.12]  En dan is bijvoorbeeld dus kanker.
[1220.24 --> 1222.80]  Je hebt een tumor bijvoorbeeld ergens in je lichaam.
[1222.88 --> 1227.98]  Dat zijn lichaamseigen cellen, maar die zijn dan wel weer net iets ander dan de gewone cellen in een lichaam.
[1227.98 --> 1234.80]  En je kan dus een eiwit ontwerpen dat precies op een eiwit kan hechten.
[1234.84 --> 1234.98]  Ja.
[1235.08 --> 1236.28]  Dat bij die tumor hoort.
[1236.28 --> 1239.98]  En dan bijvoorbeeld de zelfmoordmodus van die cel.
[1240.20 --> 1241.18]  Ja, ja, aanzetten.
[1241.36 --> 1241.72]  Aanzetten.
[1241.90 --> 1242.78]  Dat is een vinkje.
[1242.86 --> 1242.96]  Ja.
[1243.14 --> 1244.84]  Die staat uit, maar die kun je aanzetten.
[1244.92 --> 1250.68]  En je kunt je voorstellen wat dat dus betekent, want dan kun je heel gericht en heel precies een tumor gewoon uitschakelen.
[1251.14 --> 1252.88]  Misschien wel dat er geen operatie meer nodig is.
[1252.96 --> 1256.10]  In ieder geval geen chemotherapie, want het is echt iets anders.
[1256.10 --> 1261.54]  En je kunt je voorstellen als dit lukt, dan heeft Google DeepMind natuurlijk echt goud in handen.
[1261.64 --> 1264.14]  Want iedereen wil zo'n oplossing.
[1264.46 --> 1266.42]  En wat is hier nu precies nieuw aan?
[1266.94 --> 1269.10]  Nou, ze konden het al bedenken.
[1269.90 --> 1274.10]  Dus van wat voor soorten verschillende...
[1274.10 --> 1275.88]  Want het is echt heel veel verschillende soorten vormpjes.
[1276.00 --> 1277.04]  Echt honderd, miljoenen.
[1277.04 --> 1279.68]  Met ze bedoel je een groep wetenschappers als mensen?
[1279.98 --> 1280.76]  Ja, alfavolt.
[1281.06 --> 1281.74]  Dat is een soort voorgang.
[1281.74 --> 1283.70]  Ook al met een algoritme.
[1283.92 --> 1284.16]  Ja.
[1284.52 --> 1286.20]  Je hebt het wel eens een heel klein beetje over gehad.
[1286.30 --> 1287.94]  En toen vonden we het ook erg complex.
[1288.18 --> 1290.74]  Ja, het is ook wel complex.
[1290.94 --> 1294.26]  Maar het is ook begrijpelijk, als ik het kan begrijpen enigszins.
[1294.86 --> 1301.16]  Dus alfaproteo is getraind op hele grote hoeveelheden eiwitdata van de Protein Data Bank.
[1301.90 --> 1305.26]  En het kon dus al voorspellen hoe dan een eiwit eruit moest komen te zien.
[1305.26 --> 1307.22]  Maar het kan het nu ook daadwerkelijk ontwerpen.
[1307.48 --> 1309.68]  Veel beter dan we tot nu toe eerst konden.
[1309.68 --> 1315.90]  En het kan wel tot 300 keer betere binding opleveren dan wat we eerst al konden.
[1316.32 --> 1319.12]  En het gaat veel sneller dan wat mensen het kunnen doen.
[1319.24 --> 1322.02]  Het is de eerste keer dat een AI-tool hier ook in is geslaagd.
[1322.16 --> 1326.06]  Dus het is echt wel een belangrijke stap in de medische wereld.
[1326.16 --> 1327.84]  Het is nog niet natuurlijk het gouden ei.
[1328.06 --> 1330.82]  Er moet nog heel veel doorontwikkeld worden en heel bedacht.
[1330.82 --> 1338.28]  Maar het is wel een heel mooi voorbeeld denk ik van hoe AI ons gewoon echt hele grote problemen.
[1338.34 --> 1339.08]  Zoals kanker.
[1339.20 --> 1341.12]  Want ja, heel veel mensen krijgen kanker.
[1341.96 --> 1342.98]  Dat hoef ik niet met wat uit te leggen.
[1343.10 --> 1345.10]  Ja, het is weer wat anders dan een tekst samenvatten.
[1345.32 --> 1345.44]  Ja.
[1345.70 --> 1346.98]  Dit gebeurt ondertussen ook.
[1347.08 --> 1350.94]  En het insane idee dat dit een taalmodel is waar dit op draait.
[1351.14 --> 1351.30]  Ja.
[1351.30 --> 1352.26]  Nou, dat is de vraag.
[1352.36 --> 1353.58]  Ik denk wel dat het transformers zijn.
[1353.70 --> 1355.62]  Maar daar zou ik dan iets dieper in moeten duiken.
[1355.72 --> 1365.14]  Maar ik kan me voorstellen dat de doorbraken in wat dan vroeger machine learning heette en nu AI genoemd wordt en vaak over taalmodellen gaat, ook in deze zit.
[1365.24 --> 1368.92]  Of is het expliciet genoemd dat het een taalmodel gebaseerde technologie is?
[1369.06 --> 1372.90]  Het is getraind op de grote hoeveelheden eiwitdata van de Protein Databank.
[1373.38 --> 1375.46]  Ik ga er altijd van uit dat dit taalmodellen zijn.
[1375.46 --> 1378.96]  Ja, het punt is dat taalmodellen natuurlijk weer op fundamentele technologie gebouwd zijn.
[1378.96 --> 1383.32]  En die ook gebruikt kan worden om andere voorspellingen te maken dan het volgende woord.
[1383.44 --> 1383.66]  Precies.
[1383.78 --> 1386.36]  Dus ik denk dat de intuïtie daar waarschijnlijk wel klopt.
[1386.44 --> 1388.32]  Maar daar kunnen we nog wel eens een keer dieper induiken.
[1388.48 --> 1389.10]  Want dit is wel...
[1389.10 --> 1389.38]  Een expert.
[1389.74 --> 1395.40]  Ja, en ik denk dat het ook voor ons wel tof is om voorbij die taalmodellen te kunnen uitleggen aan de luisteraar.
[1395.48 --> 1401.04]  Van joh, dat hele wat doet AI voor de wereld en hoe kom je van chat GPT tot een medicijn.
[1401.38 --> 1402.84]  Dat de mensen daar wel nieuwsgierig naar zijn.
[1402.84 --> 1403.08]  Ja.
[1403.88 --> 1404.72]  Nou, wordt vervolgd.
[1404.78 --> 1407.10]  Nou ja, nog even de implicatie.
[1407.10 --> 1412.66]  Het kan zorgen voor veel gerichtere behandelingen, veel minder bijwerkingen, potentiële genezingen.
[1413.26 --> 1421.36]  En echt het versnelt die processen van geneesmiddelen onderzoek van maanden of jaren naar dagen en zelfs naar uren.
[1421.54 --> 1423.64]  Dus dat is wel een groot stap voorwaarts.
[1423.64 --> 1427.52]  En misschien dan als trick erom vind ik het interessant dat...
[1427.52 --> 1428.66]  Wat is Google?
[1429.50 --> 1429.80]  Ja.
[1429.80 --> 1430.44]  Ik bedoel, ja toch?
[1430.52 --> 1431.18]  Of alfabet.
[1431.42 --> 1437.08]  Want blijkbaar, dat wisten we al, want ze zijn op allerlei vlakken, waren ze weer ballonnen aan het oplaten met computers eronder.
[1437.30 --> 1438.12]  Dus ze waren al bijz' breed.
[1438.12 --> 1438.72]  Ze deden wel rare shit.
[1438.78 --> 1438.86]  Ja.
[1439.42 --> 1443.68]  Maar dit is wel, ik bedoel, dit is wel de breedste interpretatie van het woordzoekmachine ooit.
[1443.68 --> 1444.08]  Ja.
[1444.34 --> 1444.48]  Ja.
[1448.48 --> 1450.82]  Ja, nog heel even kort, even lokaal.
[1451.30 --> 1458.22]  Omroep RTV Drenthe, die heeft deze zomer geëxperimenteerd met een AI-stem die het nieuws zou lezen.
[1458.26 --> 1462.70]  Want ja, in de zomertijd zijn er iets minder mensen op de redactie, want iedereen is met vakantie, hoop ik.
[1463.52 --> 1465.46]  En nou ja, ze hebben dat even zitten uitproberen.
[1465.56 --> 1468.58]  En ze hebben hem ook in juni, was dat al, hebben ze hem als volgt voorgesteld.
[1468.76 --> 1470.46]  Vertel eens, wat ga je eigenlijk doen?
[1470.46 --> 1474.26]  Vanaf 6 juli ga ik in het weekend het nieuws lezen op Radio Drenthe.
[1474.74 --> 1478.32]  Zaterdag en zondag om 9 uur ochtends en rond lunchtijd kun je mij horen.
[1478.78 --> 1480.50]  En waarom ga jij het nieuws voorlezen?
[1480.86 --> 1483.58]  Nou, ook bij RTV Drenthe willen we met onze tijd meegaan.
[1483.92 --> 1488.60]  In de zomer zijn er vaak wat minder mensen aan het werk, dus is dat een mooi moment voor een experiment met AI.
[1489.54 --> 1496.90]  Wat wel even belangrijk is om te zeggen, ik vervang geen personen en de nieuwsberichten die ik lees, worden nog altijd door een mens geschreven.
[1496.90 --> 1498.90]  Ja, soms klinkt het echt heel erg...
[1498.90 --> 1500.50]  Ik wou zeggen, het is best oké.
[1500.50 --> 1500.70]  Ja.
[1500.70 --> 1502.80]  En dan af en toe heb je van die momentjes dat je denkt...
[1502.80 --> 1503.60]  Precies, dat einde.
[1503.60 --> 1506.60]  Mijn Eleven Labs Spidey Sense gaat af.
[1506.80 --> 1507.68]  Ja, dit is Eleven Labs?
[1507.80 --> 1508.30]  Dat voel ik.
[1508.36 --> 1508.44]  Jij herkent dat.
[1508.44 --> 1511.50]  Nee, maar ik bedoel, dit is een soort van rinkeltje in of zo ergens.
[1511.50 --> 1511.84]  Ja, oké.
[1512.00 --> 1513.52]  En het wordt van een melodietje.
[1513.78 --> 1513.90]  Maar...
[1513.90 --> 1517.42]  Het is ook het meest voor de hand licht dat het Eleven Labs is, want wie kan je anders gebruiken?
[1517.50 --> 1517.58]  Ja.
[1517.58 --> 1519.94]  Tegelijkertijd, drie jaar geleden klok het nog.
[1520.04 --> 1520.28]  Hallo.
[1520.28 --> 1521.06]  Nee, zeker.
[1521.06 --> 1521.34]  Ja.
[1521.34 --> 1521.46]  Ja.
[1524.18 --> 1527.78]  We zijn een soort van AI snobs geworden.
[1528.10 --> 1529.56]  Er wordt nog wel een rinkeltje daar.
[1529.76 --> 1534.92]  Nou ja, kijk, ik denk dat die eindredacteur Marten van der Veen, die kent jou niet.
[1534.92 --> 1540.44]  Want hij zegt, heel af en toe hoor je dat Bart, want zo hebben ze hem genoemd, Bart AI is.
[1540.72 --> 1541.88]  Bijvoorbeeld met klemtonen.
[1542.14 --> 1543.54]  Maar over het algemeen zijn we tevreden.
[1543.56 --> 1544.24]  Ik snap het wel.
[1544.34 --> 1544.88]  Ik snap het wel.
[1544.98 --> 1547.42]  Ik vind het eigenlijk heel erg best oké klinken.
[1547.42 --> 1547.64]  Ik ook.
[1547.90 --> 1552.06]  Ja, ook de rest van het gesprek, wat te lang was om te laten horen, maar er zat echt wel
[1552.06 --> 1554.10]  gevoel en emotie en leven in.
[1554.52 --> 1556.54]  Alleen dit stukje was best wel een beetje belabberd net.
[1556.70 --> 1557.02]  Maar ja, ik moest het voorbij.
[1557.02 --> 1559.78]  Het gaat wel ver, want hier doen ze nu net alsof ze een gesprek voeren.
[1560.10 --> 1561.22]  En daar hebben ze natuurlijk gezet te editen.
[1561.22 --> 1561.90]  Ja, dat is ook een inch.
[1561.90 --> 1564.72]  Want dit is allemaal niet real time.
[1564.72 --> 1566.30]  Dat kan helemaal niet met Eleven Labs.
[1566.92 --> 1568.20]  Dus daar hebben ze in zitten editen.
[1568.40 --> 1573.52]  En het feit dat je een nieuwslezer faked is tot daaraan toe.
[1573.60 --> 1578.02]  Maar dat je een fake gesprek voert met een nieuwslezer om het punt te maken dat de tekst
[1578.02 --> 1579.52]  heus wel door een mens geschreven is.
[1579.62 --> 1584.76]  Ik weet niet, er ploft bij de studie voor journalistiek wel iets in elkaar.
[1585.00 --> 1589.80]  Maar even, want ik heb ook voor de radio gewerkt en daar worden soms draaiboeken gemaakt.
[1589.80 --> 1591.76]  Daar wordt de vraag van de interviewer ingezet.
[1591.78 --> 1592.54]  Ja, precies.
[1592.54 --> 1593.86]  Ja, ik heb een tweetje met de sidekick.
[1593.86 --> 1594.12]  Ja.
[1594.12 --> 1595.40]  En dat is ook helemaal gescript.
[1595.40 --> 1595.46]  Ja.
[1595.46 --> 1598.98]  En eigenlijk verschilt het niet eens van wat je hier klinkt.
[1598.98 --> 1599.14]  Nee.
[1599.48 --> 1606.04]  En even nog, van wie het wel weten, de zogeheten text-to-speech technologie is gebruikt om twee
[1606.04 --> 1611.22]  stemmen te combineren zodat ze toch een beetje een echt unieke stem wel zelf hebben gecreëerd.
[1611.22 --> 1611.46]  Huh?
[1611.54 --> 1613.12]  Een soort merge bedoel je van meer stemmen?
[1613.12 --> 1614.96]  Ja, zo begrijp ik het wel.
[1615.22 --> 1615.40]  Huh?
[1615.40 --> 1618.22]  Maar dan is het dan, meestal dan wel Eleven Labs.
[1618.50 --> 1621.24]  Zouden ze gewoon meerdere stemmen in Eleven Labs gegooid hebben?
[1621.32 --> 1623.36]  Ik denk dat je daar meerdere samples in kan gooien.
[1623.48 --> 1626.24]  Maar ik bedoel, ik bluff nu die Eleven Labs.
[1626.42 --> 1627.66]  We kunnen het ze vragen.
[1628.02 --> 1628.58]  Ik ben benieuwd.
[1628.66 --> 1629.78]  Hij klonk zo.
[1630.46 --> 1631.46]  Ik vond hem heel erg zo klinken.
[1631.54 --> 1634.28]  Ik ken hem heel goed omdat ik dagelijks daar dingen in maak.
[1634.50 --> 1634.64]  Ja.
[1634.64 --> 1639.56]  Maar goed, jij had vorige week Gemini Live hier.
[1639.82 --> 1639.98]  Ja.
[1640.12 --> 1641.32]  Die vond ik ook Eleven Labs.
[1641.46 --> 1644.98]  Dus misschien is het meer gewoon wat erachter zit aan technologie die ervoor zorgt dat je
[1644.98 --> 1649.16]  dat rinkelen soms krijgt waardoor er een soort van computergeluidje bij komt ergens.
[1649.76 --> 1651.14]  Moet makkelijk te fixen zijn denk ik.
[1651.22 --> 1653.64]  Nou toch vind ik het in ieder geval leuk dat Drenthe hiermee echt voorop loopt.
[1654.58 --> 1655.92]  Dat was je punt hè Milou.
[1655.92 --> 1656.10]  Ja.
[1657.10 --> 1661.32]  En zitten wij nu soort van kritisch erop te zijn omdat we er eigenlijk zenuwachtig van worden?
[1661.66 --> 1661.94]  Iets van?
[1662.24 --> 1663.44]  Nee, helemaal niet.
[1663.62 --> 1664.16]  Nee, nee, nee.
[1664.16 --> 1668.14]  En ik denk ook de hele tijd wat wordt het moment dat wij zelf overbodig worden?
[1668.38 --> 1672.46]  Straks hoor je de tip van de week en dan komt het echt heel dichtbij hoor dat wij overbodig
[1672.46 --> 1672.76]  worden.
[1672.96 --> 1674.70]  Want ik heb namelijk een stem van Google.
[1674.96 --> 1678.08]  Het gesprek wat jij net voerde Milou over wat is het?
[1678.16 --> 1678.82]  Protein fold?
[1679.00 --> 1679.32]  Hoe heet het?
[1679.88 --> 1681.08]  Alpha proteo.
[1681.08 --> 1685.30]  Heb ik door twee Google stemmen laten doen in podcast vorm.
[1685.48 --> 1687.82]  En dan denk ik wel hoe lang zitten wij hier nog?
[1687.98 --> 1688.34]  Hoe dan ook.
[1688.34 --> 1690.44]  Eerst de reclame.
[1690.44 --> 1696.58]  Want droom jij van werken bij topbedrijven als KLM, Rabobank of PostNL?
[1696.96 --> 1698.74]  Met Calco wordt die droom misschien wel werkelijkheid.
[1698.74 --> 1700.12]  Hun IT-training ship.
[1700.20 --> 1703.44]  Blijt je de kans om te leren en te werken bij de beste bedrijven van Nederland.
[1703.78 --> 1707.84]  Bij Calco krijg je niet alleen technische trainingen, maar leggen ze ook grote nadruk op het ontwikkelen
[1707.84 --> 1708.58]  van je soft skills.
[1708.80 --> 1712.16]  Zo word je optimaal voorbereid op het bedrijfsleven in je nieuwe vakgebied.
[1712.16 --> 1716.86]  Of je nu een businessrol of een technische functie wil, Calco maakt je klaar voor de toekomst.
[1716.94 --> 1718.20]  Je krijgt direct een vast contract.
[1718.78 --> 1719.30]  Dat is chill.
[1719.64 --> 1722.90]  Je kan op elk moment solliciteren, want er staat elke maand een nieuwe groep.
[1723.10 --> 1725.50]  Wil je zo na de zomer je grenzen verleggen?
[1725.58 --> 1727.00]  Bezoek dan calco.nl.
[1727.10 --> 1729.32]  Dat is C-A-L-C-O.nl.
[1729.78 --> 1732.48]  En begin je avontuur in de dynamische wereld van IT.
[1733.38 --> 1733.62]  Dat.
[1734.26 --> 1735.02]  Gezegd hebbende.
[1735.68 --> 1738.30]  Ik had het dus over een nieuw ding van Google.
[1738.60 --> 1740.70]  Een notebook LM voor de tip van de week.
[1741.64 --> 1744.70]  Notebook LM is sowieso een geweldige tool van Google.
[1744.90 --> 1749.28]  Google daarna zou ik zeggen als je wilt praten met je eigen documenten.
[1749.58 --> 1755.74]  Dat is een super handige functie als je bijvoorbeeld een stuk aan het schrijven bent en je wil je baseren op meerdere bronnen.
[1756.58 --> 1759.78]  Bijvoorbeeld je bent een stuk aan het schrijven, daarvoor heb je rapporten gelezen.
[1759.78 --> 1762.86]  Of je bent een werkstuk aan het schrijven, of een scriptie aan het schrijven.
[1762.94 --> 1766.80]  Of je probeert een ingewikkeld onderwerp te doorgronden.
[1767.42 --> 1774.84]  Het verschil tussen dan met chat GPT praten en dit ding, notebook LM praten, is dat je bij notebook LM zelf bronnen toevoegt.
[1775.40 --> 1779.84]  Dat kan natuurlijk in theorie bij chat GPT, maar dan gaat hij ook de hele tijd andere meuk erbij halen.
[1779.96 --> 1787.88]  En notebook LM baseert zich in die chat die volgt alleen maar op die teksten die je hebt geüpload.
[1788.00 --> 1788.88]  Alleen op de bron.
[1788.88 --> 1791.58]  Alleen op de bronnen die jij zelf hebt toegevoegd.
[1791.66 --> 1794.96]  Dus die kun je erin slepen, of van je drive halen, of gewoon erin plakken.
[1795.52 --> 1799.72]  En dan kun je daarmee chatten en dan houdt hij zich ook echt alleen maar tot die teksten.
[1799.84 --> 1801.84]  Ja, maar bij chat GPT weet je vaak niet waar het vandaan komt.
[1801.84 --> 1802.06]  Precies.
[1802.12 --> 1802.66]  Moet je dus naar vragen.
[1802.68 --> 1806.04]  En met notebook LM is hij heel precies waar het vandaan komt.
[1806.12 --> 1810.26]  Je kan dan vragen waar staat het en dan kan hij precies de alinea aangeven waar het staat.
[1810.38 --> 1811.74]  En chat GPT heeft daar vaak nog wel moeite mee.
[1811.74 --> 1814.00]  Nou, dit is op zich niet nieuw.
[1814.12 --> 1816.28]  Ondanks dat sommige mensen er nog niet van gehoord hebben.
[1816.38 --> 1817.72]  Dan zou ik zeggen probeer dat.
[1818.16 --> 1821.20]  Maar ze hebben sinds gisteren een nieuwe functie.
[1821.46 --> 1826.60]  Namelijk een functie waarbij je als je die bronnen hebt toegevoegd of één bron hebt toegevoegd.
[1826.60 --> 1830.08]  Kun je er, hou je vast, een podcast van maken.
[1830.08 --> 1835.30]  Dus je kan een artikel toevoegen in welke taal dan ook, inclusief het Nederlands.
[1835.86 --> 1839.98]  En dan maakt dat ding daar een podcast van tussen twee AI stemmen.
[1840.16 --> 1844.72]  Nou, ik heb dat gedaan met een artikel uit de Volkskrant van gisteren over die landelijke NS-staking.
[1844.86 --> 1847.22]  Ik weet niet of je met de trein bent geweest deze week.
[1847.22 --> 1848.42]  Nee, dat ging dus niet.
[1848.56 --> 1849.24]  Maar het is echt een teringramp.
[1849.72 --> 1851.40]  Omdat er dus ochtends gestaakt wordt.
[1851.60 --> 1853.20]  Volkskrant heeft daar een repo over geschreven.
[1853.58 --> 1855.58]  Ik dacht, het is gewoon een random voorbeeld hoor.
[1855.58 --> 1861.54]  Maar ik dacht, hoe klinkt dat als Amerikanen een podcast zouden maken over dit typisch Nederlandse gebeuren.
[1861.62 --> 1862.24]  Leuk, ja.
[1862.26 --> 1863.24]  Met de blaadjes op het spoor.
[1863.74 --> 1866.78]  Dit is dan hoe dat klinkt nadat ik het Volkskrant-artikel geplakt heb.
[1867.06 --> 1869.68]  Oké, dus imagine je bent in Amsterdam, right?
[1869.86 --> 1870.14]  Ja.
[1870.28 --> 1872.14]  Early morning, je bent op je weg naar werk.
[1872.42 --> 1873.62]  Just, zoals altijd.
[1873.78 --> 1874.78]  Ja, de usuale commuut.
[1875.00 --> 1875.26]  Exact.
[1875.98 --> 1878.00]  Maar dan is er iets anders.
[1878.96 --> 1882.40]  Inzij de usuale deurden op de treinstaking, er is gewoon deze weerde energie.
[1882.40 --> 1884.70]  Een beetje confusie, misschien even een panik.
[1885.58 --> 1885.98]  Want...
[1885.98 --> 1887.16]  No trains.
[1887.34 --> 1888.66]  Ah, a strike.
[1888.76 --> 1889.76]  Exactly, a train strike.
[1890.28 --> 1894.06]  See, news about a strike, it's almost background noise these days, right?
[1894.14 --> 1895.10]  Happens all the time, yeah.
[1895.40 --> 1901.80]  But what we're doing today is hitting pause on that headline and using it to understand something bigger.
[1902.70 --> 1904.48]  We found this article, it's in Dutch, by the way.
[1904.82 --> 1909.30]  And it dives into the real-life impact of this strike on everyday people.
[1909.46 --> 1911.62]  Like, down to the individual commuters.
[1911.74 --> 1913.22]  The human side of things.
[1913.22 --> 1913.58]  Exactly.
[1913.76 --> 1915.20]  And get this, the article points out.
[1915.20 --> 1915.70]  Yeah.
[1916.20 --> 1916.60]  I know.
[1916.60 --> 1917.62]  Wie zit ook met zijn handen voor het mond.
[1917.62 --> 1920.12]  Ja, je hangt toch aan de lippen van deze AI's, hè?
[1920.12 --> 1922.62]  Sowieso, Amerikanen zijn natuurlijk, ze kunnen dit gewoon.
[1922.70 --> 1922.86]  Ja.
[1923.02 --> 1927.08]  Blijkbaar kunnen Amerikanse AI's dan ook heel spannend dingen daarvoor te zeggen.
[1927.26 --> 1928.74]  Zoals stakingen in Nederland.
[1928.98 --> 1929.10]  Ja.
[1929.82 --> 1931.04]  Maar holy fuck.
[1931.26 --> 1932.66]  Ik heb bijna zin om door te luisteren.
[1932.74 --> 1933.54]  Zo'n klein stukje.
[1933.74 --> 1933.92]  Ja.
[1934.04 --> 1934.52]  Klein stukje.
[1934.52 --> 1936.14]  Dus wat was de reason in dit case?
[1936.14 --> 1936.30]  Ja.
[1936.30 --> 1939.04]  Well, the article mentions it was about retirement benefits.
[1939.56 --> 1942.82]  And something about the strain of being a train worker.
[1943.72 --> 1945.80]  But it doesn't really go into a lot of detail beyond that.
[1945.80 --> 1947.38]  And that's what I find fascinating.
[1947.50 --> 1949.12]  This idea of strain of the work.
[1949.32 --> 1951.42]  That phrase pops up again and again in the article.
[1951.58 --> 1951.80]  Yeah.
[1952.04 --> 1952.52]  How so?
[1952.90 --> 1957.78]  The train workers, they were asking for earlier retirement because their jobs were so demanding,
[1957.94 --> 1958.10]  right?
[1958.40 --> 1959.92]  Heavy work, they called it.
[1959.92 --> 1961.88]  But then you see that same phrase.
[1961.88 --> 1962.32]  This is about five minutes.
[1962.32 --> 1962.88]  Heavy work.
[1963.54 --> 1964.94]  And that is het hele artikel.
[1965.12 --> 1967.78]  Dus hij pakt alle feiten die in het artikel staan.
[1967.92 --> 1969.18]  En dus ook dit soort citaatjes.
[1969.24 --> 1970.06]  Dit is echt gezegd.
[1970.20 --> 1970.52]  Wow.
[1970.72 --> 1973.34]  Wat hij net soort van in een gesprek verwerkte.
[1973.52 --> 1973.72]  Ja.
[1974.32 --> 1978.24]  En als je dit luistert, heb je het hele artikel geluisterd.
[1978.32 --> 1979.78]  Maar dan in een veel chillere vorm.
[1979.90 --> 1981.00]  Nou, maar ik ga gewoon twee stemmen niet met elkaar.
[1981.08 --> 1983.84]  Ik vind dit super fijn om naar te luisteren, eerlijk gezegd.
[1983.86 --> 1985.44]  Je denkt echt, er gaat echt iets komen.
[1985.62 --> 1987.56]  Je hangt echt aan die lippen.
[1987.56 --> 1990.62]  Omdat hij vertelt het met zoveel spanning erin.
[1991.08 --> 1992.70]  Alsof hij precies weet hoe je...
[1992.70 --> 1993.72]  Ja, dat weet hij waarschijnlijk ook.
[1993.78 --> 1993.80]  Amerikanen.
[1993.80 --> 1994.86]  Zoals Amerikanen dat doen.
[1995.02 --> 1996.48]  Die kunnen dat ook veel beter dan wij.
[1996.60 --> 2001.56]  Ik moet zeggen, want ik heb het ook wel eens als ik aan Claude vraag om iets een beetje
[2001.56 --> 2002.68]  levendiger te vertellen.
[2002.88 --> 2004.72]  Hoe zou jij dat doen, vraag ik dan aan Claude.
[2005.18 --> 2007.76]  En dan komt hij wel ook altijd met, stel je voor.
[2008.38 --> 2008.86]  Ja, ja, ja.
[2009.40 --> 2010.70]  Nou, het is echt waar.
[2010.88 --> 2011.30]  Ja, ja, ja.
[2011.46 --> 2013.26]  Dus dan vind ik het altijd een beetje irritant.
[2013.26 --> 2013.40]  Ja.
[2013.76 --> 2015.80]  Maar in een podcast werkt het wel beter.
[2015.80 --> 2016.48]  Juist heel goed.
[2016.60 --> 2016.70]  Ja.
[2016.70 --> 2023.58]  Dus wat het ding kan, is notities omzetten in een, Google noemt het een levigende discussie
[2023.58 --> 2024.92]  tussen twee AI-stemmen.
[2025.20 --> 2027.18]  Ik zou zeggen, daar is geen woord van gelogen.
[2027.36 --> 2028.82]  Je kan dus ook alles erin gooien.
[2028.94 --> 2033.10]  Niet alleen een artikel van de Volkskrant in dit geval, maar ook thai rapporten.
[2033.28 --> 2035.02]  Dus bijvoorbeeld een paper.
[2035.52 --> 2038.48]  Nou, ik doorgrond normaal gesproken heel moeilijk papers.
[2038.76 --> 2041.56]  Ik vind dat echt verschrikkelijk om wetenschappelijke papers te lezen.
[2041.56 --> 2044.34]  Na twee zinnen haak ik af, omdat ik het te ingewikkeld vind.
[2045.36 --> 2047.36]  Of, weet ik veel, een stuk uit Economist.
[2047.48 --> 2049.66]  Of een ander ding waar je echt je aandacht bij moet houden.
[2049.74 --> 2050.44]  Als je dan opeens stem...
[2050.44 --> 2052.96]  Het duurt wat langer dan het normaal lezen.
[2053.14 --> 2055.54]  Maar dan heb je wel gewoon chille stemmen die...
[2055.54 --> 2057.16]  Ja, precies wat jij wil weten.
[2057.68 --> 2060.34]  En dan ook nog eens, gaat vertellen wat erin staat.
[2060.46 --> 2063.40]  Ja, want als je onderweg bent, dan kun je het niet zitten lezen.
[2063.50 --> 2065.50]  Dan is het ideaal om die podcast te kunnen luisteren in de auto.
[2065.70 --> 2067.68]  Nou, ja, en dan had je kunnen zeggen...
[2067.68 --> 2070.00]  Dan kun je naar de audioversie van een artikel luisteren.
[2070.08 --> 2070.96]  Ja, maar dan moet je...
[2070.96 --> 2072.02]  Dan selecteert hij het voor je.
[2072.02 --> 2073.56]  Daar hou ik ook niet mijn aandacht bij.
[2073.56 --> 2074.26]  Nee, ik ook niet.
[2074.92 --> 2080.68]  Het is zoveel chiller om naar twee mensen te luisteren die over iets praten versus een audio.
[2080.68 --> 2084.04]  Dus ik vind audioversies van artikelen vaak...
[2084.04 --> 2088.48]  Ik luister misschien de eerste minuut en daarna heb ik niet eens meer door...
[2088.48 --> 2089.86]  Dat ik het eigenlijk niet meer luister.
[2089.96 --> 2091.58]  Ik hoor het nog wel, maar ik luister helemaal niet.
[2092.06 --> 2095.02]  En het is veel makkelijker om hier je aandacht bij vast te houden.
[2095.20 --> 2096.16]  Dus wat mij betreft...
[2096.16 --> 2099.36]  Ik ga ervan uit dat dit is hoe...
[2099.36 --> 2102.44]  Als je artikelen gaat luisteren, wat niet heel veel mensen doen nu...
[2102.44 --> 2104.14]  Of audiobooken. Nou, audiobooken weet ik niet.
[2104.24 --> 2105.94]  Maar artikelen luisteren...
[2105.94 --> 2110.24]  Ik ga ervan uit dat het zo gaat zijn in plaats van voorgelezen artikelen.
[2110.24 --> 2113.20]  Want voorgelezen artikelen zijn gemaakt om te lezen.
[2113.20 --> 2114.92]  Ze zijn niet gemaakt om te luisteren.
[2115.14 --> 2118.36]  En hoeveel podcasts zijn er ook waar je maar één iemand hoort?
[2118.98 --> 2119.18]  Precies.
[2119.18 --> 2120.28]  Want dat is altijd...
[2120.28 --> 2121.72]  Inderdaad, je moet gewoon meerdere mensen...
[2121.72 --> 2122.12]  Precies.
[2122.12 --> 2123.04]  Dat luistert veel fijner.
[2123.08 --> 2123.88]  Dat luistert veel fijner.
[2123.92 --> 2124.20]  Dat is geen enkele podcast.
[2124.42 --> 2124.74]  Evident.
[2125.00 --> 2126.82]  Nee, wie doet dat? Een podcast is er eentje.
[2127.02 --> 2129.28]  Ja, Rosanne Hersberg heeft een tijdje een podcast in een eentje gehad.
[2129.46 --> 2131.04]  Oké, dat kan je wel blijkbaar verbrengen.
[2131.74 --> 2133.78]  Maar goed, je kunt dit dus zelf doen.
[2133.96 --> 2135.94]  Dan kun je naar notebook LM gaan.
[2136.12 --> 2136.88]  Google daarnaar.
[2136.96 --> 2140.82]  Dan kun je een gesprek genereren op basis van die bronnen.
[2140.82 --> 2141.48]  Dat is een knopje.
[2141.60 --> 2143.38]  Hij noemt het een verdiepingsgesprek.
[2143.60 --> 2145.46]  En dan gaat hij een paar minuutjes rekenen.
[2145.56 --> 2146.74]  En dan krijg je dus die mp3.
[2146.88 --> 2148.82]  En die kun je dan via je telefoon luisteren.
[2148.90 --> 2150.46]  Hij is alleen maar in het Engels.
[2151.02 --> 2152.50]  Dat is belangrijk om erbij te zeggen.
[2152.74 --> 2154.66]  Hij genereert dus nog niet Nederlandse gesprekken.
[2154.76 --> 2157.10]  Dus vooralsnog hoeven we ons geen zorgen te maken, jongens.
[2157.24 --> 2158.96]  Maar hoe lang nog?
[2159.04 --> 2159.52]  Dat is de vraag.
[2159.72 --> 2163.20]  Ik heb dus gevraagd aan de hand van de AI-report van vandaag.
[2163.34 --> 2164.04]  Onze nieuwsbrief.
[2164.04 --> 2169.98]  Waar ook een stuk in staat over dit Google medicijnen ding waar je het net over had.
[2170.02 --> 2170.70]  Alpha Patea.
[2170.74 --> 2171.68]  Ja, toen dacht ik dus.
[2172.12 --> 2172.84]  Wat is beter?
[2173.16 --> 2174.20]  Wij die hierover praten?
[2174.44 --> 2175.72]  Of Google AI?
[2176.14 --> 2177.02]  Waarschijnlijk het laatste.
[2177.10 --> 2177.74]  Luister even mee.
[2177.74 --> 2179.66]  Het lijkt me dat AI ervaring is deze dagen.
[2180.20 --> 2182.16]  Als ik eruit schuim, is er een nieuwe hoofdlijn.
[2182.42 --> 2185.10]  Het is geweldig hoe snel dingen vermoeden vermoeden.
[2185.34 --> 2185.70]  Toeens.
[2186.04 --> 2190.28]  En vandaag wilde ik echt een paar verkeerden van Google DeepMind.
[2190.38 --> 2192.66]  Dat echt mijn zin verloopt.
[2192.76 --> 2193.84]  Oké, ik ben enthousiëerd.
[2193.84 --> 2194.76]  Wat caught je eye?
[2194.90 --> 2196.58]  Well, one's revolutionizing medicine.
[2197.14 --> 2202.60]  And the other, get this, could totally change how we program, like, forever.
[2202.86 --> 2203.20]  Wow.
[2203.36 --> 2203.70]  Oké.
[2203.74 --> 2205.20]  Now those are some big claims.
[2205.24 --> 2205.42]  Right.
[2205.68 --> 2207.06]  Let's start with the medicine side.
[2207.10 --> 2207.66]  Sounds good.
[2207.76 --> 2209.26]  So this one's called Alpha Proteo.
[2209.38 --> 2210.40]  Alpha Proteo.
[2210.60 --> 2210.84]  Right.
[2210.94 --> 2212.30]  I've been hearing a lot about this one.
[2212.34 --> 2214.96]  It's doing some really revolutionary work with proteins.
[2215.38 --> 2221.84]  Oké, so I know proteins are, like, the building blocks of, well, everything in our bodies, basically.
[2221.98 --> 2222.42]  Pretty much.
[2222.42 --> 2224.70]  They're the little machines that make our biology work.
[2224.94 --> 2228.36]  But to be honest, my understanding of them is pretty surface level.
[2228.36 --> 2237.38]  Well, to really grasp Alpha Proteo's impact, it helps to picture proteins as these, like, intricate machines with super specific shapes.
[2237.52 --> 2239.46]  And these shapes are everything.
[2239.46 --> 2244.36]  Oké, so how do they figure out, or what's this Alpha Proteo thing have to do with these shapes?
[2244.76 --> 2245.82]  So, get this.
[2246.26 --> 2249.66]  It actually designs completely new proteins from scratch.
[2249.72 --> 2250.46]  It designs though.
[2250.46 --> 2252.62]  It's not just figuring out what these shapes are.
[2252.66 --> 2252.70]  It's not just figuring out what these shapes are.
[2252.70 --> 2252.74]  It's not just figuring out what these shapes are.
[2252.74 --> 2253.10]  It's actually going to be a little bit.
[2253.10 --> 2255.38]  It's not just figuring out what these shapes are.
[2255.38 --> 2256.38]  Yeah.
[2256.38 --> 2258.24]  I have 100 things to say, but that's too much.
[2258.34 --> 2258.90]  Because we want to talk about it.
[2258.90 --> 2260.00]  We want to talk about it.
[2260.00 --> 2261.44]  But I want to talk about it.
[2261.44 --> 2263.34]  I think it's fascinating.
[2263.34 --> 2266.44]  I thought, jeetje, what's there a lot of mansplaining in?
[2266.44 --> 2269.22]  Why is it now a woman who says, oh, oh, oh, oh, oh.
[2269.22 --> 2270.16]  And then the man who says, oh, oh.
[2270.16 --> 2270.66]  Oh, that's a switch.
[2270.66 --> 2272.38]  Nee, maar dat is niet zo, want hij is de switcher.
[2272.56 --> 2273.00]  Ja, ze switcher.
[2273.00 --> 2274.28]  Hij gaat degenen van alles uitleggen.
[2274.40 --> 2274.70]  Dus dat vind ik.
[2274.70 --> 2275.82]  Dat doen Amerikanen, hè.
[2275.82 --> 2277.76]  Die kunnen dan heel goed onnozel spelen.
[2277.98 --> 2279.60]  En dan opeens blijken ze de expert zijn.
[2279.80 --> 2282.68]  Ik vind dat dus geniaal, want dat doet er natuurlijk ook niet toe.
[2282.76 --> 2285.08]  Je bedoel, je kan letterlijk waarschijnlijk in de prompt zeggen, joh, verdelen het een
[2285.08 --> 2285.84]  beetje leuk over de.
[2286.08 --> 2288.04]  En misschien kan iemand ook ineens een expert worden.
[2288.16 --> 2289.38]  Bam, waarom niet?
[2289.58 --> 2291.58]  Ik bedoel, dus dat vond ik fascinerend.
[2292.08 --> 2296.72]  Ik zat ook te denken, er zijn hele theorieën over dat wij eigenlijk verbale wezens zijn die per
[2296.72 --> 2300.02]  ongeluk tijdelijk in de geschiedenis eventjes zijn gaan schrijven en lezen wat we allemaal
[2300.02 --> 2300.98]  eigenlijk helemaal niet wilden.
[2301.24 --> 2302.50]  En nu kunnen we weer lekker praten.
[2302.98 --> 2304.48]  Dus ik vind dat idee van.
[2304.70 --> 2307.04]  Ik voel me nu echt even een AI stem door die mm.
[2307.38 --> 2307.78]  Sorry.
[2308.98 --> 2311.42]  Ik zit hier dus met Alexander, denk ik.
[2312.48 --> 2313.16]  So interesting.
[2313.54 --> 2314.22]  Ja, echt.
[2314.36 --> 2314.64]  Dankjewel.
[2315.72 --> 2317.22]  Wil jij nu de expert zijn trouwens?
[2317.32 --> 2318.08]  Want dan swappen we even.
[2318.52 --> 2319.38]  Maar goed, Milou, wil jij even?
[2320.12 --> 2322.62]  Maar, het is zo weird.
[2323.36 --> 2326.22]  Maar ik had ook net toen, want ik was heel lang stil.
[2326.72 --> 2329.28]  Omdat ik toen, toen open en hij...
[2329.28 --> 2329.92]  Ik ging even stukken jouw houd.
[2329.92 --> 2330.34]  Ja, echt.
[2330.78 --> 2334.44]  Toen open en hij Sora uitkwam met hun video dingen, toen ben ik letterlijk, toen ik die
[2334.44 --> 2337.02]  video's keek, naar buiten gelopen op mijn bankje gaan zitten bij de Rijn.
[2337.34 --> 2339.88]  En ik heb als een soort apathisch tien minuten lang voor me uitgesproken.
[2340.14 --> 2341.86]  Toen ben ik daar met mijn vriendin een rondje gaan lopen.
[2341.96 --> 2342.60]  Toen zei ze, wat is er?
[2342.64 --> 2344.12]  Ik zeg, aliens zijn geland.
[2344.28 --> 2345.18]  Niemand heeft het nog door.
[2345.30 --> 2345.78]  What the fuck?
[2346.10 --> 2347.76]  Goed, ik was echt, ik kon bijna niet meer lopen.
[2347.78 --> 2348.76]  Vond je dat een beetje overtrokken achteraf?
[2348.86 --> 2349.06]  Of niet?
[2349.18 --> 2349.40]  Nee.
[2349.68 --> 2349.88]  Nee?
[2350.10 --> 2352.08]  Nee, maar dat is omdat ik, wat wij eerder al zeiden...
[2352.08 --> 2352.86]  Dabbeldine al in.
[2352.90 --> 2358.10]  Maar ja, wat Alexander heel goed inmiddels doorheeft over mij is, reageer je nu op wat er vandaag
[2358.10 --> 2359.92]  verteld wordt of wat jij extrapoleert?
[2360.48 --> 2361.66]  Ja, jij doet het laatste.
[2361.74 --> 2364.86]  Ja, ik zit hier op mijn fantasie te reageren en niet op wat ik nu hoor alleen.
[2365.40 --> 2367.62]  En dus wat, als Alexander zegt, wanneer wordt het Nederlands?
[2367.78 --> 2368.36]  Zes maanden?
[2368.50 --> 2369.56]  Twaalf, maar niet tien jaar.
[2370.20 --> 2372.64]  En wanneer wordt het interessant genoeg voor mij om naar te luisteren?
[2372.74 --> 2374.14]  Al heel snel, denk ik.
[2374.32 --> 2374.46]  Ja.
[2374.46 --> 2376.74]  En dus om nog even terug te komen op dat verbalen.
[2377.28 --> 2380.54]  Ik zat in Istanbul in een taxi twee weken geleden.
[2380.86 --> 2384.14]  En die meneer in die taxi had een hele grote witte knop op zijn telefoon.
[2384.32 --> 2385.38]  Gigantische witte cirkel.
[2385.72 --> 2387.48]  Ik heb niet kunnen achterhalen welke app dit is.
[2387.56 --> 2389.66]  Als iemand dit weet, stuur het naar me op, please.
[2390.12 --> 2392.58]  En die drukte de hele tijd op die witte knop om dingen in te spreken.
[2392.78 --> 2394.68]  En dan kwam er sporadisch wat terug.
[2394.80 --> 2397.80]  Dus je was eigenlijk voice notes heen en weer aan het sturen met iemand.
[2397.92 --> 2399.18]  Al rijdend, niet helemaal de bedoeling.
[2399.56 --> 2399.76]  Maar goed.
[2400.14 --> 2402.70]  Zo'n push to talk, chat app.
[2402.70 --> 2405.94]  Ja, het was een soort telefoon, tussen een telefoongesprek en voice messages.
[2406.34 --> 2406.74]  Walkie talkie.
[2406.80 --> 2407.32]  Ja, precies.
[2407.52 --> 2410.00]  En dan soms reageerde die persoon na één minuut.
[2410.08 --> 2411.00]  En soms na één seconde.
[2411.06 --> 2411.96]  En soms door elkaar heen.
[2412.04 --> 2414.92]  Waardoor ik dacht, wat is dit voor hybrid telefoongesprek slash voice message?
[2415.30 --> 2418.68]  Maar goed, daarmee, ik heb dat in andere culturen ook gezien in India.
[2418.82 --> 2421.62]  Mensen sturen elkaar de hele dag op WhatsApp alleen maar berichtjes met audio.
[2421.86 --> 2426.24]  En wij zitten hier als een soort van stone age people op elkaar testen te sturen.
[2426.24 --> 2428.90]  Op een soort virtueel keyboard in beeld.
[2429.00 --> 2431.24]  Zo voel ik dan een soort van haak, kwerty.
[2431.24 --> 2431.78]  Maar goed.
[2433.06 --> 2434.40]  En nu met deze tool.
[2434.72 --> 2440.98]  Want ik had de reacties op een beetje de podcastification van media op deze manier.
[2441.16 --> 2443.54]  Of dat alles nu een podcast kan worden.
[2443.98 --> 2448.34]  Was vrij negatief bij heel veel mensen die wat meer wetenschappelijk willen lezen.
[2448.44 --> 2453.86]  Die echt zeggen, vorm, moet dan overal suiker en snoep en chocola en vla bij voordat mensen het willen eten.
[2453.86 --> 2456.16]  Als een soort van podcast zijn oppervlakkig.
[2456.16 --> 2460.40]  Terwijl ik denk, bullshit, je kan een negen uur lange aflevering laten genereren.
[2460.60 --> 2464.44]  Die tien andere artikelen erbij haalt om het nog veel meer uit te diepen.
[2464.70 --> 2468.52]  Dus ik vind het een zeer enthousiaste test van de hypothese.
[2468.94 --> 2473.06]  Mensen zijn verbale wezens die per ongeluk tijdelijk in een textuele wereld beland zijn.
[2473.34 --> 2477.12]  Want als dit meer en meer en meer groeit en we minder en minder en minder gaan lezen.
[2477.38 --> 2478.48]  Dan wordt dat een beetje waar.
[2478.64 --> 2480.34]  En ik geloof er intuïtief een beetje in.
[2480.34 --> 2482.12]  En wij zitten midden in die hypothese.
[2482.20 --> 2484.64]  Want wij nemen een podcast op en schrijven geen artikelen de hele dag.
[2485.12 --> 2485.88]  Wel een nieuwsbrief.
[2486.16 --> 2488.90]  Maar we mensen aangeven dat ze het vaak prettig vinden om hem juist te lezen.
[2489.06 --> 2490.64]  En niet naar de podcast te luisteren.
[2490.80 --> 2493.96]  Ja, blijkbaar bestaat die behoefte nog steeds.
[2494.44 --> 2499.12]  En ik vind het gewoon heel interessant dat wij hier met z'n drieën zitten.
[2499.40 --> 2501.54]  Ik ga nu even meta beschrijven wat de luisteraar voelt.
[2501.78 --> 2502.08]  Sorry.
[2503.38 --> 2506.46]  Ja, dat wij gewoon geconfronteerd worden met het einde van de podcast.
[2507.24 --> 2509.18]  En daar hebben we het al langer geleden over gehad.
[2509.18 --> 2511.66]  Ja, het is wel weer een fundamentele stap dichterbij.
[2511.76 --> 2512.06]  Man.
[2512.36 --> 2514.78]  Maar toch, wat zit hier nog tussen wat jullie betreft?
[2515.16 --> 2518.54]  Voordat je dit gaat luisteren in plaats van een podcast gaat luisteren.
[2518.60 --> 2523.18]  Want dat dit moment op een gegeven moment op de horizon ligt, ben ik wel van overtuigd.
[2524.24 --> 2531.22]  Dat we met evenveel plezier naar een AI-stem gaan luisteren dan naar zoals wij hier bij elkaar zitten.
[2531.40 --> 2531.80]  Nog even.
[2532.72 --> 2534.96]  Maar waarom is dit nog niet goed genoeg?
[2535.28 --> 2536.14]  Er missen grapjes.
[2536.14 --> 2539.92]  Nou, weet ik niet. Ik heb natuurlijk maar een klein beetje gehoord.
[2540.06 --> 2542.06]  Eigenlijk vind ik het juist wel...
[2542.06 --> 2543.14]  Er zit weinig van mij tussen.
[2543.84 --> 2547.36]  Want je kan het dus zelf eenmaal voeden met wat je wil horen.
[2547.60 --> 2547.68]  Ja.
[2547.86 --> 2549.78]  Het is super persoonlijk daardoor.
[2549.86 --> 2552.20]  Het is echt iets wat ik zelf eigenlijk bijna gemaakt heb.
[2552.32 --> 2552.50]  Ja.
[2552.78 --> 2553.76]  Wat ik dan alleen nog maar...
[2553.76 --> 2557.74]  Ja, precies. Het is een beetje als een IKEA-meubel. Je bent er trotser op omdat je het zelf in elkaar gezet hebt.
[2558.08 --> 2559.28]  Dat is een heel mooi verwoord.
[2559.32 --> 2563.38]  Ja, maar dat geeft meer liefde voor wat je gaat consumeren vervolgens.
[2563.66 --> 2566.56]  Ja, ik vind het uniek op een bepaalde manier.
[2566.64 --> 2566.86]  Zeker.
[2567.60 --> 2568.64]  Veel minder generiek.
[2568.64 --> 2572.16]  Het is super informatief.
[2572.28 --> 2573.20]  Dus ik vind er eigenlijk al...
[2573.20 --> 2574.40]  Er zit voor mij niet zoveel tussen.
[2574.52 --> 2575.78]  Maar goed, ik ben ook snel tevreden.
[2575.86 --> 2578.34]  Er zit niet zoveel tussen tussen dat jij het gaat gebruiken bedoel ik.
[2578.36 --> 2578.46]  Ja.
[2579.98 --> 2581.60]  Behalve dat ik het zelf moet maken dan misschien.
[2581.60 --> 2583.90]  Nou, een groot ding is natuurlijk...
[2583.90 --> 2585.24]  Het bronmateriaal moet goed zijn.
[2585.36 --> 2585.44]  Ja.
[2585.44 --> 2588.94]  Dus er is alsnog een journalist die die reportage heeft geschreven.
[2589.10 --> 2589.40]  Laten we dat ook weten.
[2589.40 --> 2592.62]  En hoe gaan die betaald krijgen als het zonder banners gewoon jouw oor in gaat?
[2592.76 --> 2593.00]  Maar goed.
[2593.24 --> 2593.74]  Ja, maar goed.
[2593.80 --> 2596.48]  Ik ga er dan vanuit dat dit dan onderdeel wordt van de Volkskrant-app.
[2596.98 --> 2597.42]  Ja, zeker.
[2597.42 --> 2600.88]  Maar met jouw blendelgedachte, Ben, ik bedoel, dit is...
[2600.88 --> 2602.98]  Nee, maar dit is dus wel waar mijn frustratie...
[2602.98 --> 2607.24]  Blendel is ooit ontstaan omdat ik gewoon boos was over de traagheid van de voortbeweging.
[2607.40 --> 2611.12]  Als je nu met DPG-mensen praat, die hebben echt het idee dat de Volkskrant-app wel af is.
[2611.42 --> 2615.12]  Dus die kwamen uit een soort pdf-tijd waarin je de krant in pdf...
[2615.44 --> 2618.10]  Weet je, de papieren versie doorscrollen op je telefoon.
[2618.54 --> 2623.14]  Toen kwamen ze op een gegeven moment tot het briljante inzicht dat dat misschien niet zo chill is op een klein schermpje.
[2623.36 --> 2627.92]  Nu hebben we daar, nou ja, gewoon een feed met wat knopjes onderin.
[2628.88 --> 2629.92]  Dan vinden ze het wel weer af.
[2630.64 --> 2632.82]  En dan zetten ze dan misschien een audioknopje erbij.
[2633.06 --> 2635.18]  En dan denken ze, ja, dan zetten we onze podcast erin.
[2635.26 --> 2637.10]  Dan hebben we dat deel ook alweer gecoverd.
[2637.22 --> 2643.08]  Terwijl ik denk, hier begint het nieuwe begin voor hoe je app...
[2643.08 --> 2645.92]  Ja, dit is hoe je totaal andere app moet gaan maken.
[2646.62 --> 2649.74]  Want ik wil gewoon op play kunnen rammen en naar de Volkskrant van vandaag luisteren.
[2650.36 --> 2652.36]  Dus misschien moeten ze even Foundermode aanzetten.
[2653.66 --> 2655.74]  Maar wat ik nog wilde zeggen is dat ik...
[2655.74 --> 2659.94]  En dat doet ook een beetje foreshadowing naar het onderwerp waar we straks nog wat dieper op ingaan.
[2660.34 --> 2663.24]  Is dat ik me voor kan stellen, zoals we vorige week bespraken.
[2663.56 --> 2665.66]  Je hebt een general purpose robot in huis lopen.
[2665.94 --> 2669.18]  En dan zeg je, joh, kan je ook even mondhygiënisten voor me spelen?
[2669.54 --> 2672.34]  En dan wordt er een tas thuis gebracht met wat tools erin.
[2672.40 --> 2674.70]  En dan ga jij liggen en dan gaat die robot jouw nachtmerrimateriaal.
[2674.78 --> 2676.32]  Maar die gaat jouw tanden dan schoonmaken.
[2676.72 --> 2678.28]  En daarna gaat hij weer de vaat doen.
[2678.28 --> 2683.66]  Dat ik me ook kan voorstellen dat je op een gegeven moment een ensemble of een groepje aan AI-entities hebt.
[2683.72 --> 2685.62]  Met prettige stemmen, wat vrienden van jou zijn.
[2685.96 --> 2690.84]  En dat je dan zegt, Alexander en Milou, speel even een podcast voor mij.
[2691.22 --> 2694.26]  Dus daar heb je al een band mee met identiteiten al maanden of jaren.
[2694.70 --> 2695.90]  En dan zeg, kunnen jullie twee...
[2695.90 --> 2699.38]  Zoals onze stemmen, de luisteraar misschien ook een band mee ontwikkelen.
[2699.40 --> 2701.90]  Zo'n parasociale relatie als de luisteraars met ons hebben.
[2702.22 --> 2705.60]  Dat ik me voor kan stellen, Milou, dat jij zegt, joh, ik heb hier een complex artikel.
[2705.60 --> 2708.12]  Ik wil wat voorbereiden voor een podcast die ik eigenlijk niet meer hoef te maken.
[2708.50 --> 2711.82]  En ik ga twee van mijn AI-vrienden, waar ik al een hele band mee heb.
[2712.00 --> 2714.76]  Eén gebaseerd op de stem van een oude buurman die super tof was.
[2715.06 --> 2717.14]  Of Bart, hoe heet die, Bart Nijsman.
[2718.64 --> 2720.12]  Omroep Dren doet dit allang.
[2720.68 --> 2722.76]  Ik weet niet wat je nou stoer aan het doen bent.
[2722.92 --> 2725.34]  Ik heb een sterke parasociale relatie met Bart.
[2725.36 --> 2729.78]  Goed, we gaan straks het hebben over dat deze podcast een boek gaat uitbrengen.
[2730.32 --> 2730.92]  Dat straks.
[2731.08 --> 2732.88]  Maar nu eerst het hoofdonderwerp.
[2734.88 --> 2736.24]  Stel je voor...
[2736.24 --> 2739.54]  Ik dacht, ik doe hem even op de podcast manier.
[2739.72 --> 2740.28]  Stel je voor.
[2740.40 --> 2741.14]  Stel je voor.
[2741.34 --> 2742.66]  Oh ja, dit is...
[2742.66 --> 2744.72]  Ja, dat ben je natuurlijk in één keer klaar mee.
[2744.76 --> 2745.06]  Nee, ja.
[2745.12 --> 2746.08]  Wij gaan gewoon...
[2746.08 --> 2746.88]  Ik kan dat ook niet.
[2746.98 --> 2747.56]  Ja, right.
[2747.82 --> 2748.00]  Oh, wow.
[2748.00 --> 2749.38]  I always wondered.
[2749.72 --> 2752.58]  I have only superficial knowledge of this.
[2752.72 --> 2753.52]  Ja, maar ze zeggen...
[2753.52 --> 2754.42]  Ze doen wel iets meer.
[2754.58 --> 2755.92]  Ze participeren meer.
[2756.06 --> 2759.82]  En aangezien jullie hier nog waarschijnlijk niks van weten, omdat ik dit helemaal weer zelf heb zitten onderzoeken.
[2761.30 --> 2763.30]  Moeten jullie toch even doen met mijn monoloog.
[2763.96 --> 2764.86]  Nou, kom maar.
[2765.08 --> 2765.42]  Ja, kijk.
[2765.60 --> 2766.54]  Wat er dus is gebeurd.
[2766.98 --> 2771.70]  De allereerste autonome AI-beschavingen zijn ontstaan.
[2771.74 --> 2773.02]  Wat is een AI-beschaving?
[2773.02 --> 2775.00]  Dat kun je je inderdaad afvragen.
[2775.36 --> 2776.08]  Ja, dat vraag ik me af.
[2776.08 --> 2777.70]  En een onbenomen AI-beschaving.
[2777.84 --> 2778.58]  Nou, het is...
[2778.58 --> 2781.28]  We moeten het even naar de wereld van Minecraft halen.
[2781.36 --> 2783.04]  Dat is die game met die blokken.
[2783.28 --> 2785.30]  Kinderen vinden het leuk, maar blijkbaar volwassenen ook.
[2785.88 --> 2786.74]  En het is van Microsoft.
[2786.92 --> 2788.66]  En als Microsoft het koopt, dan is het belangrijk.
[2789.12 --> 2790.62]  Nou, en daar hebben ze...
[2790.62 --> 2793.72]  Ze is in dit geval een bedrijf dat heet Altera.
[2793.94 --> 2794.10]  Ja.
[2794.32 --> 2800.36]  Heeft daar heel veel verschillende autonome AI-agenten in gezet.
[2800.44 --> 2800.60]  Ja.
[2800.68 --> 2802.84]  Dus het zijn allemaal AI-spelertjes.
[2802.84 --> 2803.08]  Ja.
[2803.46 --> 2806.78]  Die zelf kunnen denken en doen en redeneren en...
[2806.78 --> 2809.00]  Nou ja, een Minecraft eruit kunnen hangen.
[2809.02 --> 2812.30]  En zijn die tussen de gewone Minecrafters zich door die wereld aan het begeven?
[2812.44 --> 2813.92]  Of doen die AI-spelertjes met elkaar?
[2813.94 --> 2814.64]  Nee, doen het allemaal met elkaar.
[2814.94 --> 2815.56]  Oh, met elkaar.
[2815.86 --> 2817.28]  Ja, ze doen het allemaal met elkaar.
[2817.86 --> 2817.98]  Ja.
[2818.68 --> 2822.36]  En is dit dan iets wat wij als mensen kunnen aanschouwen?
[2822.56 --> 2823.62]  Of is dit meer iets voor...
[2823.62 --> 2824.62]  Maar kan ik het vinden, Milou?
[2824.80 --> 2826.94]  Jullie zijn heel goed jullie werk aan het doen.
[2827.82 --> 2830.58]  Je kon al een potje Minecraften met een AI-agent.
[2830.66 --> 2831.42]  Dat kan nog niet heel lang.
[2831.42 --> 2832.54]  Maar je kan bijvoorbeeld...
[2832.54 --> 2836.40]  Als ik in de multiplayer mode ga, dan kan ik spelen samen met een AI-agent.
[2836.52 --> 2836.80]  Oké.
[2836.84 --> 2838.94]  Die huppelt dan ook echt een beetje achter je aan.
[2839.06 --> 2841.00]  En die staat je een beetje glazig aan te kijken.
[2841.10 --> 2843.26]  Hij kan ook niet anders, want hij heeft een vierkant hoofd natuurlijk.
[2843.44 --> 2845.44]  Maar het voelt nog net een beetje...
[2845.44 --> 2845.96]  Het is een NPC.
[2846.34 --> 2847.38]  Ja, wat?
[2847.58 --> 2848.36]  Het is zo'n NPC.
[2848.70 --> 2848.82]  Het is zo'n NPC.
[2848.82 --> 2849.90]  Het is zo'n...
[2849.90 --> 2851.40]  Want dit in games loopt...
[2851.40 --> 2854.94]  Sinds mensenheugenis lopen er al soort van andere karakters achter mij aan.
[2855.04 --> 2856.26]  Als ik door een wereld aan het rennen ben.
[2856.56 --> 2858.00]  Maar dat is dan geen AI.
[2858.18 --> 2859.70]  Dat is gewoon ingeprogrammeerd.
[2859.82 --> 2860.66]  Hoe die moet reageren.
[2860.66 --> 2862.82]  Die reageert wel op echt wat jij bedenkt.
[2862.92 --> 2864.28]  En hij zegt zelf ook wel eens dingen.
[2864.52 --> 2866.62]  Maar over het algemeen zat er niet heel veel initiatief in.
[2866.92 --> 2868.32]  Maar wat ze nu dus hebben gedaan...
[2868.32 --> 2869.64]  En daarvan was ik wel echt...
[2869.64 --> 2870.18]  Dat ik dacht...
[2870.18 --> 2871.94]  Wow, dat dat gebeurt...
[2871.94 --> 2873.44]  Is dat er dus een beschaving ontstaat.
[2873.54 --> 2876.14]  En een beschaving is niet gewoon alleen maar mensen bij elkaar zetten.
[2876.58 --> 2876.66]  En...
[2876.66 --> 2877.94]  Ja.
[2878.20 --> 2880.12]  Dus een beetje een dingetje laten doen in Minecraft.
[2880.30 --> 2881.66]  Maar ze gaan met elkaar interacteren.
[2882.66 --> 2883.66]  En er...
[2883.66 --> 2884.98]  Gaan shit maken.
[2885.14 --> 2885.78]  Ze gaan shit maken.
[2885.82 --> 2887.10]  Maar ze maken dus ook...
[2887.10 --> 2889.28]  Overheidsinstituties bijvoorbeeld.
[2890.24 --> 2891.48]  En ze spreken met elkaar...
[2891.48 --> 2892.50]  Gelijk bureaucratie.
[2892.82 --> 2892.98]  Ja.
[2893.18 --> 2894.36]  Dit is de toekomst van AI.
[2894.50 --> 2896.26]  Ze gaan eerst hun eigen bureaucratie maken.
[2896.38 --> 2896.52]  Ja.
[2896.54 --> 2897.82]  Ik vind het een belangrijke waarschuwing.
[2897.88 --> 2900.02]  Nou, ik zal je even over de democratie gesproken.
[2900.34 --> 2901.92]  Er is een kleine demo.
[2902.04 --> 2903.50]  Want er is nog niet heel veel over te vinden.
[2903.98 --> 2907.16]  Maar in één video die Altea hierover heeft gemaakt.
[2907.54 --> 2908.52]  Hoor je...
[2908.52 --> 2910.00]  Nou ja, het volgende.
[2910.00 --> 2912.78]  We simuleren parallelen werelds onder Trump en Kamala.
[2912.78 --> 2916.38]  Voor elk een hebben de lidstaten een shared constitution in Google Docs.
[2916.48 --> 2917.84]  Dat ze kunnen vervangen om te vervangen.
[2918.12 --> 2920.16]  Unter Trump, de simulatie passen nieuwe lezen.
[2920.28 --> 2921.76]  Dat het deur vervangen van de polissen in de wereld.
[2922.24 --> 2925.06]  Unter Kamala, ze focussen zeer op de kriminele justitereform.
[2925.20 --> 2926.00]  En de vervangen van de doodschap.
[2926.74 --> 2929.06]  Omdat onze agents zijn sociale en over tijd groeien.
[2929.18 --> 2930.92]  Ze zijn impact door grouwdynamics.
[2931.06 --> 2933.96]  Maar ze gebruiken ook hun individueel kracht om het systeem te vervangen.
[2934.54 --> 2935.32]  Ik weet het lukkig.
[2935.64 --> 2940.12]  Maar dit is de eerste keer dat we een agents kunnen vervangen en zichzelf vervangen.
[2940.12 --> 2948.58]  Maar zijn er dan heel regelmatig ter dood stellingen in Minecraft, in de Trump-wereld...
[2948.58 --> 2951.30]  ...dat Kamala dat kan verbieden?
[2951.34 --> 2955.16]  Nou, wat mij dus heel leuk lijkt is als hier een soort van 24 uur livestream van komt.
[2955.30 --> 2957.26]  Dat je gewoon de hele tijd kan meekijken wat daar gebeurt.
[2957.38 --> 2958.60]  Maar dat is er volgens mij nog niet.
[2958.68 --> 2959.64]  Ik heb het nog niet gevonden.
[2959.64 --> 2960.96]  Maar dit gebeurt dus.
[2961.02 --> 2964.64]  Er ontstaan vormen van bestuur, van cultuur, economie.
[2964.78 --> 2971.08]  Want er is ook een samenleving waarin er op een gegeven moment een soort markt wordt gecreëerd.
[2971.18 --> 2973.58]  En dan gaan ze ook met elkaar afspreken van nou, we gaan hiermee betalen.
[2973.72 --> 2976.02]  Dit wordt ons betaalmiddel, ons gem.
[2976.18 --> 2977.28]  Ja, ik ben geen minecrafter.
[2977.82 --> 2978.94]  Zo diep zit ik daar niet in.
[2978.94 --> 2981.12]  Je hebt coins nodig, waarschijnlijk om dingen te krijgen.
[2981.20 --> 2982.38]  Maar dat spreken ze dus met elkaar af.
[2982.56 --> 2983.82]  En niet echt gems.
[2984.28 --> 2985.94]  Dus hier gaan we, dit is onze currency.
[2985.94 --> 2991.92]  En vervolgens komt er dan een priester en die koopt iedereen om om zich bij zijn religie aan te sluiten.
[2992.10 --> 2993.34]  Dat gebeurt ook.
[2993.46 --> 2996.58]  Dus er is ook religieuze aspecten bestaander.
[2996.66 --> 2999.90]  Dus eigenlijk niets menselijks is deze AI-agentering.
[2999.90 --> 3003.58]  Een soort QAnon-shamaan die dan in de wereld van Trump komt.
[3003.76 --> 3010.04]  Ja, en toen dacht ik echt, ik vond het behoorlijk gewoon eerie eigenlijk.
[3010.18 --> 3012.64]  Gewoon een beetje dat dit gebeurt.
[3012.78 --> 3013.04]  Waarom?
[3013.04 --> 3017.28]  Nou, dat is het cliché van de Westworld-samenleving enzo.
[3017.34 --> 3022.32]  Maar als zij zelf gaan interacteren met elkaar, en dat is wel in Minecraft, dan kunnen ze...
[3022.32 --> 3027.02]  Het lijkt bijna alsof ze allemaal een eigen denkende mind hebben.
[3027.90 --> 3030.44]  Nee, wat snap ik. Maar waar komt de eerie vandaan?
[3030.64 --> 3037.14]  Nou, als ze dat hier al doen, stel dat we straks robots hier rond hebben lopen die ook allemaal met elkaar gaan interacteren.
[3037.14 --> 3044.06]  En vormen van bestuur bedenken. En ja, dan is in mijn hoofd de vorm van bestuur naar de vorm van overrulen van alle mensen.
[3044.56 --> 3048.32]  Dat ze uit die beeldschermen kruipen als het ware en de wereld inkomen met hun.
[3048.96 --> 3051.66]  En dat ze ook al jaren getraind hebben op samenlevingen vormen.
[3051.68 --> 3053.42]  Ja, maar er zijn toch al robots in onze wereld?
[3053.42 --> 3057.36]  Nee, ik bedoel, ik zeg het niet cynisch. Ik denk dat ik je wel hoor. Ik zat even te zoeken naar die eeriness.
[3058.04 --> 3060.60]  Ja, maar misschien is dat eens overdreven. Dat weet ik niet.
[3061.02 --> 3067.28]  Nou, ik denk dat je je intuïtie best mag volgen. Maar goed, ik zit hier vaak aan tafel met een spannende fantasie.
[3067.92 --> 3074.04]  Ik moest denken dat in Hitchhiker's Guide to the Galaxy, spoiler voor de mensen die de film of boek niet hebben gelezen, ga dat doen.
[3074.04 --> 3078.56]  En daarin is de aarde een supercomputer gebouwd door muizen. En die muizen lopen hier dus alleen maar rond.
[3079.14 --> 3083.78]  Dat zijn wij nu, zeg maar. Muizen hier om dat computerproject in de gaten te houden.
[3084.22 --> 3091.52]  En wij zijn daar ook onderdeel van. En die hele computer, de aarde is een computer en de mensen zijn transistoren om hele moeilijke rekensommen te doen.
[3091.52 --> 3100.34]  In dat opzicht zou een samenlevingtje bouwen, wat dus nu veel onderzoekers aan het doen zijn, waarbij je AI-entiteiten in groepen met elkaar laat nadenken,
[3100.34 --> 3109.80]  best een handige manier van compute kunnen zijn, oftewel tot oplossingen komen, om inderdaad politieke systemen te kunnen testen die normaal niet te testen zijn.
[3109.88 --> 3114.36]  Want dan moet je ze testen in onze samenleving. Je wil eigenlijk een simulatie hebben waarbinnen je dat kan testen.
[3114.36 --> 3115.00]  Ja, ja, ja.
[3115.00 --> 3122.52]  En dat is dan nog een heel concreet voorbeeld van, we willen dingen weten over mensen, wat in menselijke samenlevingen zou gebeuren.
[3122.52 --> 3131.90]  Maar wat nu veel uit onderzoek blijkt, is van die groepjes agenten maken, agents, waarin AI's verschillende wensen hebben, verschillende waarden hebben,
[3132.26 --> 3142.78]  en die in een grote groep samen tot conclusies laten komen en consensus, dat het best een hele interessante manier is om tot oplossingen te komen in heel veel verschillende domeinen,
[3142.90 --> 3144.66]  niet alleen maar politieke vraagstukken.
[3144.66 --> 3152.42]  En daarom dat in computer games agentjes maken, ik bedoel, het leeft voor ons veel meer, omdat je letterlijk kan meekijken in Minecraft.
[3152.72 --> 3152.82]  Ja.
[3152.94 --> 3154.02]  Je kan er ook in.
[3154.36 --> 3154.62]  Ja.
[3154.72 --> 3155.12]  Dan kom je net.
[3155.12 --> 3155.90]  Je kan meedoen.
[3155.96 --> 3159.04]  Je kan meedoen in een computer eigenlijk.
[3159.26 --> 3161.14]  En dat is wel fascinerend.
[3161.14 --> 3162.22]  Het is een testruimte.
[3162.78 --> 3162.94]  Ja.
[3163.56 --> 3165.22]  Dus het lijkt me dat je 3-4 opmaakt.
[3165.22 --> 3174.48]  Als ik het goed begrijp, misschien om het concreet te maken, wat zou dan een voorbeeld zijn van iets wat je in een maatschappij wil onderzoeken,
[3174.48 --> 3177.70]  maar wat nu onhaalbaar of onethisch is of...
[3177.70 --> 3179.28]  Een universeel basisinkomen.
[3179.44 --> 3180.18]  Ja, precies.
[3180.56 --> 3185.14]  Dan bedoel, nu nemen we wel veel stappen hoor, want alle parameters die daarmee...
[3185.14 --> 3188.14]  Dan moet je eigenlijk echt een digital twin, zoals dat noemt.
[3188.14 --> 3191.28]  Dat is een digital twin maken van onze aarde.
[3191.88 --> 3193.98]  En daar moet je wel wat meer compute voor meenemen.
[3193.98 --> 3206.90]  Maar ik kan me voorstellen dat het 10 minuten tot 10 uur tot 10 dagen vooruit voorspellen van bepaalde dynamieken waarin je mensachtige meeneemt.
[3207.00 --> 3209.36]  Dus je gaat weersystemen simuleren bijvoorbeeld.
[3209.56 --> 3212.44]  Maar je wil eigenlijk weersystemen plus het effect op de samenleving doen.
[3212.96 --> 3217.38]  Maar goed, nu springen we echt ver vooruit denk ik voor wat we nu kunnen aan compute.
[3217.38 --> 3220.24]  Als we het even naar vandaag willen halen of dichterbij vandaag.
[3221.18 --> 3227.38]  Wat ik nu een beetje zie gebeuren in veel software is, omdat die AI nog met ons moet interacteren.
[3228.02 --> 3229.02]  Dus wij zijn nog in de loop.
[3229.20 --> 3229.84]  Wij doen nog mee.
[3230.34 --> 3232.50]  Dus ChatGPT praat in een taal die wij kennen.
[3233.02 --> 3234.12]  Nu ook met audio.
[3234.62 --> 3237.16]  Waardoor Chat die computer eigenlijk meegaat doen in het gesprek.
[3237.28 --> 3238.46]  Die gaat podcast maken.
[3238.80 --> 3242.64]  Dat moet voor die modellen zelf echt mega saai zijn.
[3242.64 --> 3249.84]  Moeten we nou super traag in het Engels een soort van zo Hallo mensen.
[3251.18 --> 3254.58]  Haha, we moeten wel lachen want anders zijn we jullie aandacht kwijt.
[3255.14 --> 3255.82]  Leem.
[3256.08 --> 3259.70]  Weet je al, dat die AI's onderling kunnen met 20 petabyte gewoon.
[3261.94 --> 3263.28]  Onderling klinken zij meer zo.
[3264.24 --> 3265.34]  Nou ja, noem maar eens.
[3266.44 --> 3268.00]  Ik volg u niet zo.
[3268.32 --> 3268.54]  Ja, volg.
[3269.72 --> 3270.24]  Bedankt.
[3270.24 --> 3271.40]  Maar.
[3272.04 --> 3273.04]  Kun je iets langzamer praten?
[3273.18 --> 3273.32]  Ja.
[3275.62 --> 3276.38]  In ieder geval.
[3277.50 --> 3284.34]  Wat gebleken is dat wij zijn mensen die ook, wij willen dus graag interacteren met iets wat een taal spreekt die wij begrijpen.
[3284.86 --> 3286.62]  Die emotie toont in stem.
[3286.94 --> 3289.24]  Zodat we daar aandacht voor hebben en ook meer kunnen voelen.
[3289.62 --> 3291.76]  We vinden het leuk als AI muziek voor ons gaat maken.
[3291.90 --> 3295.20]  Het zijn allemaal vormen van informatie uitwisselen als je het heel plat zegt.
[3295.28 --> 3296.76]  Maar in ieder geval we communiceren met elkaar.
[3296.76 --> 3301.74]  Nu is het ook zo dat wij over het algemeen niet communiceren met één persoon in ons dagelijk leven.
[3301.84 --> 3303.70]  Maar met meerdere mensen in meerdere rollen.
[3304.16 --> 3307.60]  En dat wat nu blijkt is om die AI ook allerlei gezichten te gaan geven.
[3308.12 --> 3310.46]  En die gezichten in groepen samen te trekken.
[3310.86 --> 3311.60]  Waardoor dus iets.
[3311.60 --> 3312.84]  Zo'n proze types?
[3312.84 --> 3319.94]  Nou dat jij, het is voor jou waarschijnlijk interessant als jij zegt, ik wil wel even een artikel schrijven, een column schrijven.
[3320.26 --> 3322.44]  Daar zet ik een klein teampje van AI's op.
[3322.54 --> 3324.14]  Ik doe er één die doet desk research.
[3324.28 --> 3326.18]  Ik doe er één die helpt me mee met het schrijven.
[3326.54 --> 3329.42]  Eén gaat alleen maar kritiek leveren met superdroge grappen eromheen.
[3329.50 --> 3330.60]  Zodat ik het een beetje kan handelen.
[3331.04 --> 3335.56]  We hebben een, en je zou kunnen zeggen, dit is ook het commentaar op Bipedal Robots.
[3335.56 --> 3339.68]  Er wordt heel vaak gezegd, waarom zijn we robots aan het maken in de vorm van de mens?
[3340.46 --> 3342.56]  Die kunnen in hele andere vormen gemaakt worden.
[3342.72 --> 3347.48]  Wat zijn we toch, wat mis we toch creativiteit om alleen maar kopieën van onszelf te willen maken?
[3347.58 --> 3351.04]  Je zou ook een robot met tien armen of die kan een raketmotor heeft of noem maar wat op.
[3351.66 --> 3353.88]  Maar dat is omdat het moet passen bij de menselijke maat.
[3354.42 --> 3359.60]  Dus je zou kunnen zeggen, ga je dan een heel redactieteam met alle rollen zoals we die nu kennen.
[3359.74 --> 3362.22]  Met hoofdredacteur en desk research en noem maar op.
[3362.60 --> 3363.88]  Gaan we, en journalistiek.
[3363.88 --> 3371.24]  Gaan we AI echt beperken tot die rare menselijke rollen die misschien heel inefficiënt zijn?
[3371.56 --> 3377.24]  Het voelt logischer om die AI helemaal vrij te laten als een soort van spacevaardigheden.
[3377.48 --> 3383.76]  Of hebben wij daar als mensen, voordat AI er was, structuren uitgevonden die ook effectief zijn.
[3384.02 --> 3384.56]  Evolutionair.
[3384.68 --> 3387.14]  Ja, die ook passen binnen een AI.
[3387.50 --> 3392.88]  Dus dat het inderdaad blijkt dat zeggen, zullen we een redactieteam naboodsen in AI met alle rollen erin.
[3392.88 --> 3395.44]  Wat heel absurd klinkt, eigenlijk hartstikke goed werkt.
[3395.52 --> 3400.82]  Oké, en als je dat dan terugbrengt naar dat Altera, dat bedrijf die die Minecraft AI's heeft gemaakt.
[3401.06 --> 3403.54]  Waarom, zeg maar, wat heeft dat met elkaar te maken?
[3403.76 --> 3407.60]  Nou, dat die bedrijven, en er zijn er nog een aantal, en er is ook heel veel onderzoek wordt gedaan naar dit.
[3408.28 --> 3410.50]  NPC's maken, AI's binnen spelwerelden.
[3410.50 --> 3414.84]  Ten eerste, die spelwerelden hebben we al, dus dat krijg je eigenlijk gratis.
[3415.00 --> 3419.88]  Dus je kan gewoon Minecraft, de hele engine en het 3D en hoe mensen Minecraft gebruiken, heb je gratis erbij.
[3420.28 --> 3426.58]  Dan ga je daar binnen die AI-agenten neerzetten en dan een uur of twee uur, ligt aan of het realtime is.
[3426.68 --> 3431.06]  Dus is het mensentijd, of druk je op een knop, spoel je dat ding even 10 miljoen jaar vooruit in 10 minuten.
[3431.28 --> 3432.48]  Dan ga je dan terugkijken.
[3432.54 --> 3433.32]  Dat kan dus ook dan.
[3433.32 --> 3433.96]  Nou, daarom.
[3434.16 --> 3435.76]  Of spoel je er een miljoen tegelijkertijd.
[3435.96 --> 3443.88]  Dat bedoelde ik met, zeg maar, dan krijg je waarschijnlijk een podcast samenvatting van hoe de laatste 10 miljoen jaar geschiedenis is geweest in jouw simulatie die je in één minuut hebt berekend.
[3444.24 --> 3447.98]  En dan kan je door dat vooruit spoelen tot inzicht te komen.
[3448.66 --> 3453.80]  En dus als jij vraagt, Alexander, hoe heeft dat te maken met die spelwerelden?
[3454.30 --> 3460.06]  Dat zijn fijne zandbakken, zandbakjes, om die ideeën in te testen.
[3460.06 --> 3461.92]  Want uiteindelijk moet het empirisch worden.
[3461.92 --> 3479.42]  Je moet zeggen, als we een chatbot hebben die één entiteit heeft in één rol en die geven een taak om een column te schrijven versus een redactieteam van misschien wel 800, 80, 8000, 10 redactieteams die met elkaar gaan concurreren en de beste wint.
[3479.72 --> 3484.18]  En komt daar dan een beter artikel uit, een beter artikel als in volgens de norm die we hebben gesteld.
[3484.18 --> 3487.64]  Maar ik snap dat en dat is mooi, maar ik zit even door te denken.
[3487.86 --> 3493.00]  Kijk bijvoorbeeld met, als je kijkt naar samenlevingen, wat er met corona is gebeurd.
[3493.12 --> 3497.64]  Dan hebben we natuurlijk ook een tijd gehad waar we in scenario's moesten denken, wat is we dit doen, wat is we dat doen?
[3498.14 --> 3506.40]  Nu kun je dus heel precies kijken van, ik ga zeggen van, nou de wetenschap die gaat nu alles bepalen en alle mensen moeten hun mond houden.
[3506.40 --> 3509.74]  Dus stel dat we de wetenschappelijke lijn gaan volgen, wat gebeurt er dan?
[3510.08 --> 3515.72]  Nou dan gaat zo'n, doe je dat in de Minecraft samenleving invoeren en dan kijken wat er uitkomt.
[3516.12 --> 3518.06]  Ja, is dat het idee van zo'n digital twin?
[3518.52 --> 3520.94]  Dat je dit soort simulaties kan draaien?
[3520.94 --> 3527.96]  Ik denk dus dat wat Nvidia aankondigde destijds met hun weermodel, dus een digital twin of Earth, deels hype, deels waar.
[3528.20 --> 3533.16]  De chips zijn zo krachtig dat je nu de aarde tot op bepaalde hoogte, want het blijft een model.
[3533.24 --> 3534.38]  Maar de aarde, wat betekent dat?
[3534.46 --> 3538.22]  De hele planeet, het hele weersysteem kan simuleren zonder mensen.
[3538.78 --> 3539.30]  Zonder mensen.
[3539.76 --> 3545.24]  Dus als je inzoomt op die aarde, want het is gewoon een soort 3D computer game, dan lopen daar geen dieren of mensen.
[3545.36 --> 3545.42]  Nee.
[3545.58 --> 3546.24]  Die wel invoeren hebben.
[3546.24 --> 3546.92]  Maar wel bergen.
[3547.16 --> 3549.82]  Ja, zeker, want daar gaat de jetstream dan een beetje tegenaan en zo.
[3549.82 --> 3551.16]  Exact. Dus Riojef is er wel.
[3551.46 --> 3553.38]  En dan ga je zoeken hoeveel kunnen we weghouden.
[3554.50 --> 3556.24]  En dan kan je hem backcasten.
[3556.28 --> 3560.56]  Dus dan kan je zeggen, als we deze hadden gehad een maand geleden, hoe komt het dan overheen met het weer?
[3560.64 --> 3562.10]  Dan kan je gewoon testen en dan blijkt hij best goed.
[3562.22 --> 3563.52]  En dan forecasten ga je vooruit.
[3563.60 --> 3566.92]  Dat is wat we eigenlijk willen, want we willen natuurlijk weten wanneer komt er een storm, et cetera.
[3567.22 --> 3572.20]  Dan blijkt wel dat, en dat weet iedere meteoroog die nu luistert, er zit een soort van, ja maar, ja maar, ja maar.
[3573.14 --> 3577.44]  Ieder uurtje dat je toevoegt aan die forecast, komt er enorm veel noise bij.
[3577.44 --> 3582.26]  En kan dat ding helemaal, want die orkaan gaat dan niet naar links, maar naar rechts, naar Amerika heen.
[3582.36 --> 3583.88]  Wat totaal andere gevolgen heeft.
[3584.16 --> 3585.16]  Ja, butterfly effect ook nog.
[3585.16 --> 3586.52]  Ja, enorm, nou ja, precies dat.
[3586.94 --> 3591.52]  En als je dan ook nog de dieren, de mensen en al dat ding, dat past dan wat minder goed in die digital twin.
[3591.52 --> 3599.02]  En het zorgt er gewoon voor dat wij zulke, wij zijn zulke irrationele wezens als mensen, wij zijn heel moeilijk te volgen als een rekenmachine.
[3599.50 --> 3603.44]  Wij maken de meest wauze beslissingen op basis van wat een simulatie nooit zou kunnen doen.
[3604.14 --> 3608.16]  Dat wij heel moeilijk te voorspellen zijn, dus iedere minuut die je erbij doet is moeilijker.
[3608.16 --> 3622.82]  Maar, om antwoord te geven op je vraag, deze taalmodellen, deze mensachtige algoritme in een computer game, daardoor zou je wel de sociale dimensie die dus nu onzichtbaar is en niet in die modellen zit, aan de digital twin kunnen toevoegen.
[3622.82 --> 3632.22]  Persoonlijk denk ik dat het interessanter is om eerst eens te kijken, joh, we willen een interventie doen op een team mensen die bij een bepaalde organisatie werken.
[3632.54 --> 3641.30]  Kunnen we een digital twin maken van hun situatie en daar in een week of een maand vooruit op basis van veel minder parameters dan de hele aarde.
[3641.56 --> 3649.26]  Ik wil een beetje terugduwen op, ik denk dat jullie intuïtie van, wow, gaat dit van computer game naar de hele aarde simuleren?
[3649.26 --> 3656.42]  Dat klopt, maar ik denk dat dat op korte termijn nog niet mogelijk is en nooit helemaal, maar daar kunnen we het in een andere aflevering nog wel eens over hebben.
[3657.34 --> 3668.50]  Voor vandaag is het al superboeiend dat waar we net over die luchtverkeersleider hadden die dan mogelijk voor ChatGPT geplaatst wordt om het snelle of het trage of het dure of het goedkope model te kiezen.
[3668.80 --> 3676.90]  Dat je daar dus al de eerste twee entiteiten in krijgt, namelijk een rolverdeler en dingen met rollen erachter of eigenlijk meer dan één entiteit al, tien als je wil.
[3676.90 --> 3683.86]  En dat dat idee van groepen laten samenwerken, groepjes van entiteiten, hier heel mooi mee getest kan worden.
[3684.52 --> 3693.96]  Daarom die bedrijven, want ik dacht ook eerst, is dit echt sec onderzoek uit nieuwsgierigheid of zit hier echt potentiële commerciële waarde achter?
[3694.04 --> 3703.76]  Nou ja, ze zeggen, Althra, ze zegt inderdaad zelf, stel je het potentieel voor van dit soort modellen op grote schaal, waarbij oplossingen voor maatschappelijke problemen in de echte wereld worden gesimuleerd en getest.
[3703.76 --> 3711.36]  En daarom wou ik nog heel even terugkomen op het corona voorbeeld, want dan kun je dus kijken van oké, nou dit gebeurt er in dat scenario en dat gebeurt er in dat scenario.
[3711.48 --> 3713.38]  Je kan het helemaal zo bekijken, wat komt eruit?
[3713.84 --> 3724.40]  Maar dat betekent dan dus ook dat je als beleidsmaker een keuze moet gaan maken waarvan je weet, oké, hier vallen er misschien zoveel doden en ontstaat er zoveel onrust.
[3724.40 --> 3731.54]  En daar ontvallen ons zoveel mensen, maar dat is dan meer die groep of zo. Dus laten we dan voor dat scenario kiezen.
[3732.08 --> 3740.14]  Je gaat hele belangrijke, zware beslissingen misschien wel moeten nemen, juist omdat je zo helder kan kiezen uit verschillende scenario's.
[3740.30 --> 3741.02]  Snap je wat ik bedoel?
[3741.02 --> 3741.30]  Ja.
[3741.68 --> 3746.62]  Dus het wordt nog meer, je kan nog meer meten, je weet nog dingen misschien nog preciezer, dat is dus even de vraag.
[3746.88 --> 3747.08]  Ja.
[3747.92 --> 3750.06]  Maar dat is ook wel, dat heeft ook iets engs eigenlijk.
[3750.28 --> 3750.50]  Ja.
[3750.50 --> 3759.54]  Ja, jij stelt je dan zo voor dat dat OMT bij elkaar kwam of wie dan ook de beslissingen nam, die alles over zien.
[3759.88 --> 3760.02]  Ja.
[3760.18 --> 3769.30]  Dat zij veel scherper, zes schermen naast elkaar hadden met scenario 1 waar ook nog de scholen openbleven en wat dat voor effect had gehad.
[3769.40 --> 3774.02]  En dan op lange termijn duur voor mentale gezondheid van kinderen en wat dat voor kosten oplevert.
[3774.02 --> 3778.56]  En overvraging van de GGZ en weet ik van allemaal de problemen die we nu hebben hierdoor.
[3778.60 --> 3780.10]  Ja, dit is het meest efficiënt.
[3780.24 --> 3780.78]  Ja, sorry.
[3781.74 --> 3785.48]  Sorry voor de morele bezwaren op dit moment misschien.
[3785.68 --> 3785.84]  Ja.
[3786.18 --> 3787.92]  Maar hierdoor leven het meeste mensen.
[3788.16 --> 3789.02]  Dus het wordt...
[3789.02 --> 3793.02]  Maar jij denkt dat dat scherper was geweest, ook voor de buitenwereld.
[3793.76 --> 3799.30]  En dat het duidelijker was geweest wat de consequenties zouden zijn van het handelen van de overheid.
[3799.44 --> 3803.28]  Ja, ik kan me voorstellen dat zoiets als dit, als zij zeggen van het kan daarvoor worden,
[3803.28 --> 3805.06]  je kan het simuleren, je kan het testen.
[3805.50 --> 3805.56]  Ja.
[3805.58 --> 3809.30]  Als dat de belofte is, dan is dat ook denk ik de logische uitkomst.
[3809.68 --> 3812.04]  En dan moet je wel je afvragen, wil je dat?
[3812.34 --> 3816.50]  Ja, en moeten we natuurlijk ook aannemen dat dat kan technisch.
[3816.62 --> 3819.86]  Dat je die, dat je het helemaal kan doorrekenen die scenario's.
[3819.92 --> 3819.98]  Ja.
[3820.14 --> 3822.92]  En überhaupt hoeveel scenario's ga je dan presenteren.
[3823.02 --> 3827.22]  Want je kan er dan wel, je kan dan wel 2000 scenario's presenteren, maar dan heeft het eigenlijk nog steeds geen waarde.
[3827.96 --> 3833.18]  Dus er is een, nou er zitten wat caveats in, om het maar zacht te zeggen.
[3833.28 --> 3840.28]  Maar stel dit kan, wordt onze maatschappij of onze democratie er dan rechtvaardiger of bestendiger van?
[3841.14 --> 3847.24]  Als je de bevolking transparantie kan geven over hoe de toekomst eruit ziet.
[3847.40 --> 3847.52]  Ja.
[3848.18 --> 3848.80]  Ja, dat is een heel instante vraag.
[3848.80 --> 3850.56]  Eigenlijk kunnen we een beetje in de toekomst kijken.
[3850.68 --> 3850.84]  Ja.
[3850.84 --> 3853.96]  Ja, al is dat een goed ding dat we in de toekomst kunnen krijgen.
[3853.96 --> 3854.42]  Dat is het.
[3854.78 --> 3859.56]  Ja, en ik denk dat dat zit ook weer een beetje in die soort risico-averse samenleving waarin we leven.
[3859.70 --> 3861.92]  Want alles gaat dan om risico-vermijden, risico-vermijden.
[3862.84 --> 3863.98]  Hoe ver willen we daarin gaan?
[3864.00 --> 3864.64]  Niet alleen toch?
[3864.76 --> 3868.32]  Het is ook de positieve kanten of de dingen die juist gestimuleerd worden.
[3868.32 --> 3870.20]  Risico-vermijden hoeft niet negatief te zijn.
[3870.34 --> 3873.08]  Ik bedoel, als jij een helm op doet bij een kind, dan zie ik dat niet als negatief.
[3873.22 --> 3874.72]  Dus ik bedoel dat niet eens cynisch.
[3875.32 --> 3881.70]  Ik denk dat we tot bepaalde hoogte er allemaal naar kunnen streven om te proberen om een nucleaire
[3881.70 --> 3882.32]  installatie...
[3882.98 --> 3883.10]  Ja.
[3883.32 --> 3885.14]  Die moeten we niet in een woonwijk zetten.
[3885.38 --> 3886.62]  En een vuurwerkfabriek ook niet.
[3886.76 --> 3889.42]  Dus er zijn wel bepaalde hele logische dingen moeten we gewoon niet doen.
[3889.42 --> 3892.42]  Maar ik denk wel dat...
[3892.96 --> 3894.18]  Wat ik interessant vind is dat ik denk...
[3894.18 --> 3897.42]  Zij praten over een soort level 10 in hun marketing.
[3898.42 --> 3900.46]  Terwijl ik denk level 1 is al super waardevol.
[3900.58 --> 3902.72]  Dus ik begrijp hun overpromise niet helemaal.
[3902.86 --> 3904.84]  Want dat is wat mij betreft nog heel ver weg.
[3904.92 --> 3907.78]  En mogelijk zelfs bijna onmogelijk om zo ver te simuleren.
[3908.32 --> 3913.76]  Er zitten in de eerste negen stappen daarvoor al heel interessante andere toepassingen van die technologie.
[3914.50 --> 3917.24]  En ik ben benieuwd dat die scenario's...
[3917.24 --> 3921.42]  Want daar is best wel wat voordat we scenario's gingen maken met taalmodellen in Minecraft.
[3922.58 --> 3924.12]  Je hebt best wel veel...
[3924.12 --> 3925.16]  Ik weet daar toevallig veel van.
[3925.22 --> 3927.08]  Want ik heb ooit een studie gedaan die daar precies over ging.
[3927.20 --> 3930.12]  Dat heet Future Studies, Anticipatory Systems.
[3930.34 --> 3930.94]  Het is allemaal...
[3930.94 --> 3932.54]  Mensen maken zich hier erg druk om.
[3932.98 --> 3934.76]  En dat is plausibele scenario's maken.
[3934.84 --> 3935.88]  Wat is de waarde van een scenario?
[3935.98 --> 3937.42]  Maar vooral...
[3937.42 --> 3939.60]  Wanneer gaan we scenario's leven die we hebben bedacht?
[3939.70 --> 3942.30]  Dus we krijgen een soort self-fulfilling waarde.
[3942.56 --> 3943.50]  Dus je kan...
[3943.50 --> 3945.30]  Als jij in samenleving een aantal scenario's voorstelt...
[3945.30 --> 3949.00]  Dan gaan we bijna dat scenario lopen als een soort playbook.
[3949.22 --> 3950.44]  Omdat we hem al hebben gezien.
[3950.58 --> 3952.66]  Dus die scenario's hebben ook een effect op ons.
[3953.00 --> 3955.80]  Door dat scenario we zo gaan leven.
[3955.94 --> 3956.96]  Dus dat is ook heel boeiend.
[3957.12 --> 3958.30]  Hoe dus deze simulaties...
[3959.02 --> 3960.30]  Eigenlijk ook al zijn het fantasietjes...
[3961.46 --> 3962.30]  Ons gaan...
[3963.30 --> 3963.98]  Ja...
[3963.98 --> 3965.20]  We gaan ons er naar gedragen.
[3965.42 --> 3966.54]  Ja, exact. Mooi gezegd.
[3966.54 --> 3969.64]  Omdat het dan onderdeel wordt van de cultuur?
[3969.86 --> 3970.70]  Of zeg maar...
[3970.70 --> 3973.72]  Dan omdat films erover gaan dan over de simulaties?
[3973.86 --> 3974.40]  Of wat is...
[3974.40 --> 3977.04]  Nou, ik kan me voorstellen dat op het moment dat je...
[3977.04 --> 3979.40]  Wat een beetje science fiction nu natuurlijk al doet...
[3979.40 --> 3981.54]  In de vorm van boeken, tv, series, films...
[3982.20 --> 3984.78]  Is mogelijke imaginary...
[3984.78 --> 3985.60]  Zoals dat duur heet.
[3985.90 --> 3988.04]  Mogelijke toekomsten van de samenleving.
[3988.04 --> 3988.60]  Black Mirror.
[3988.74 --> 3989.60]  Dat het een bepaalde manier is.
[3989.80 --> 3992.74]  De politici gebruiken Black Mirror als een schrikbeeld.
[3993.06 --> 3994.28]  Als ik met politici praat...
[3994.28 --> 3996.72]  Hebben ze allemaal die serie gezien.
[3996.86 --> 3998.30]  En ze handelen ook daarop.
[3998.68 --> 3999.04]  In de...
[3999.04 --> 4000.00]  In soort van...
[4000.00 --> 4001.62]  Waar ze kiezen beleid over te maken.
[4001.96 --> 4003.44]  Dus je zou kunnen zeggen...
[4003.44 --> 4004.36]  Dat is dat.
[4004.78 --> 4008.04]  Ja, en hoe Sam Altman helemaal gefascineerd is door...
[4008.60 --> 4009.42]  De film Her.
[4009.78 --> 4011.94]  En dan Skye Scarlet gaat maken.
[4012.14 --> 4013.14]  Die dopjes en zo.
[4013.38 --> 4015.06]  Ik bedoel, op bepaalde manier is het...
[4015.06 --> 4016.44]  We gaan science fiction...
[4016.44 --> 4017.58]  Science fiction is als het woord...
[4017.58 --> 4018.56]  De roadmap geworden.
[4018.66 --> 4018.88]  Ja.
[4018.94 --> 4020.04]  De samenleving.
[4020.52 --> 4022.58]  En dat is niet per se erg.
[4023.38 --> 4024.58]  Maar als je het bewustzijn hebt...
[4025.36 --> 4027.18]  Dat die fantasieën die je maakt...
[4027.18 --> 4028.76]  In allerlei vormen...
[4028.76 --> 4031.02]  Een roadmap-achtige effect kunnen hebben...
[4031.02 --> 4031.58]  Op wat we doen.
[4031.72 --> 4032.94]  Dan heb je wel een verantwoordelijkheid...
[4032.94 --> 4034.64]  Om dat zo breed mogelijk neer te zetten.
[4034.76 --> 4035.58]  Met zoveel mogelijk afslagen.
[4036.24 --> 4038.08]  Want anders beperken we onszelf...
[4038.08 --> 4040.58]  Tot onze eigen AI-simulatie fantasieën.
[4041.66 --> 4041.98]  Ja.
[4042.54 --> 4043.48]  Zod ik het al niet bekeken.
[4044.10 --> 4044.58]  Maar ik vind het...
[4044.58 --> 4044.98]  Ik ben het...
[4044.98 --> 4046.38]  Want jij zei net van...
[4046.38 --> 4047.56]  Het is voor die negativiteit...
[4047.58 --> 4049.58]  Ik heb een soort...
[4049.58 --> 4051.58]  Op kleine schaal...
[4052.10 --> 4054.42]  Een brandoefening in een groot gebouw.
[4054.50 --> 4055.80]  Je gaat een nieuw gebouw ontwerpen...
[4055.80 --> 4057.26]  En dan wil je eigenlijk een oefening doen.
[4057.60 --> 4059.56]  Hoe gaan alle kinderen hun school uit bij brand?
[4059.94 --> 4060.10]  Ja.
[4060.10 --> 4061.28]  Wat heb ik daarop tegen?
[4061.48 --> 4061.92]  Fantastisch.
[4062.04 --> 4062.96]  Laten we het simuleren.
[4063.44 --> 4064.58]  Het scheelt namelijk een hoop brandoefeningen.
[4065.64 --> 4066.58]  Maar toch zal je zien dat die kinderen...
[4066.58 --> 4068.58]  Die kinderen ineens hele andere...
[4068.58 --> 4069.86]  Irrationele beslissingen gaan maken.
[4070.30 --> 4073.50]  Want hun smartphone lag nog ergens.
[4073.50 --> 4075.30]  De favoriete trui hing nog in de kast.
[4075.42 --> 4076.50]  En dat favoriete trui ding.
[4076.82 --> 4077.80]  Maar die AI's...
[4077.80 --> 4078.58]  Zullen mogelijk wel...
[4079.10 --> 4081.58]  Een favoriete trui-achtig ding kunnen simuleren.
[4081.64 --> 4083.06]  Omdat het een soort mensachtige zijn.
[4083.14 --> 4084.50]  Dus dat is wel iets anders...
[4084.50 --> 4085.54]  Van een soort 3D-modelletje...
[4085.54 --> 4086.82]  Die X en Y kan lopen.
[4087.16 --> 4088.20]  Iets met wensen.
[4088.32 --> 4090.34]  Het heeft iets met waarschijnlijkheid...
[4090.34 --> 4091.94]  Ook te maken natuurlijk.
[4092.24 --> 4094.48]  En hoe zullen de meeste kinderen zich gedragen.
[4094.92 --> 4096.74]  En hoe kunnen we zo veel mogelijk kinderen...
[4096.74 --> 4097.48]  Redden uit de brand.
[4098.44 --> 4099.24]  Dat is toch belangrijk.
[4099.60 --> 4099.70]  Ja.
[4100.20 --> 4101.48]  Nou, Witse...
[4101.48 --> 4102.80]  Mijn hoofd is weer ontploft.
[4103.04 --> 4104.30]  Met al die kennis die je deelt.
[4104.46 --> 4106.40]  En daar gaat toevallig onze volgende reclame over.
[4109.28 --> 4111.54]  Wat heb jij ook kennis die je wilt delen met de wereld?
[4111.62 --> 4113.14]  Met Squarespace kun je eenvoudig...
[4113.14 --> 4114.88]  Je eigen online cursus creëren...
[4114.88 --> 4116.20]  En verkopen op je eigen website.
[4116.92 --> 4118.94]  Squarespace biedt alle tools die je nodig hebt...
[4118.94 --> 4120.22]  Om je expertise om te zetten...
[4120.22 --> 4121.76]  In een professionele leeromgeving.
[4121.86 --> 4123.56]  Dan kies je een layout die past bij je merk.
[4123.92 --> 4124.80]  Upload wat video's.
[4124.88 --> 4125.84]  En bewerk alles naar wens...
[4125.84 --> 4127.36]  Met hun gebruiksvriendelijke interface.
[4127.80 --> 4130.16]  En dan kon je kooklessen, yoga cursussen...
[4130.16 --> 4133.18]  Of zelfs techniek tutorials aanbieden via Squarespace.
[4133.42 --> 4134.38]  Want die maken dat makkelijk.
[4134.54 --> 4135.46]  Stel een prijs in.
[4136.02 --> 4138.48]  Kies tussen eenmalige betaling of een abonnement.
[4138.48 --> 4141.02]  En laat Squarespace alle transacties veilig afhandelen.
[4141.32 --> 4141.90]  En dat is niet alles.
[4141.90 --> 4143.64]  Met de ingebouwde SEO tools...
[4143.64 --> 4146.22]  Worden je cursussen ook snel gevonden in zoekmachines.
[4146.62 --> 4148.02]  En dankzij de krachtige analytics...
[4148.02 --> 4150.66]  Kun je het succes van jouw cursussen meten en verbeteren.
[4150.90 --> 4152.72]  Dus wil jij zelf ervaren...
[4152.72 --> 4155.64]  Hoe je met Squarespace jouw kennis kan omzetten...
[4155.64 --> 4157.06]  In succesvolle online cursussen.
[4157.24 --> 4158.92]  Ga dan nu naar squarespace.com.
[4158.92 --> 4162.36]  Slash pokey10 voor een gratis proefperiode van 14 dagen.
[4162.58 --> 4164.04]  Als je klaar bent om live te gaan...
[4164.04 --> 4167.90]  Gebruik dan dus die promocode pokey10 voor 10% korting.
[4168.02 --> 4169.46]  Op je eerste website of domeinnaam.
[4170.04 --> 4170.56]  Nog een keer.
[4171.16 --> 4173.04]  Squarespace.com slash pokey10.
[4173.04 --> 4176.90]  Omdat jouw kennis het verdient om gedeeld te worden.
[4177.50 --> 4178.76]  Bouwen ze ook een website voor je?
[4179.50 --> 4180.60]  Dat kun je dus daarmee doen.
[4180.78 --> 4182.32]  Dat kun je dus daarmee doen.
[4182.34 --> 4182.86]  De website ook.
[4182.90 --> 4183.94]  Nee, maar dit ging over de cursussen.
[4184.06 --> 4184.78]  Ja, maar dat is allemaal...
[4185.78 --> 4187.14]  Dit hoort nog bij de reclame.
[4187.42 --> 4188.62]  Het zit allemaal in Squarespace.
[4188.62 --> 4193.50]  En als je denkt waarom heel soms zit er een stiele stotter als Alexander dit doet.
[4193.70 --> 4194.68]  Dat doet hij expres.
[4195.28 --> 4196.72]  Want dat voegt authenticiteit toe.
[4196.74 --> 4196.98]  Yes.
[4197.60 --> 4197.90]  Goed.
[4198.00 --> 4199.40]  Waarom is het een professional, Wietse?
[4200.18 --> 4202.36]  Dank jullie wel voor deze...
[4202.36 --> 4203.46]  Hé, en er was nog iets.
[4203.60 --> 4204.64]  Want je hebt een boek zitten lezen.
[4204.82 --> 4205.74]  Hoe moet ik dit zien?
[4205.78 --> 4206.94]  Ik heb belangrijk nieuws voor jullie.
[4207.10 --> 4207.24]  Ja?
[4207.44 --> 4210.64]  Deze podcast gaat een boek uitbrengen.
[4211.26 --> 4211.70]  Ja.
[4212.20 --> 4213.38]  Je moet niet zo verbaasd doen.
[4213.56 --> 4214.66]  Het is een papieren boek.
[4214.86 --> 4217.24]  En de ironie daarover ontgaat mij niet.
[4217.24 --> 4221.48]  Maar wij dachten het is tijd om voorbij de hype te kijken.
[4221.68 --> 4225.22]  En het is tijd om de balans op te maken over al die afleveringen van Poki.
[4225.56 --> 4228.70]  Om te kijken wat het nou echt gaat betekenen voor ons werk en ons leven.
[4229.30 --> 4231.06]  Ik heb veel boeken gelezen over AI.
[4231.54 --> 4237.06]  En ik denk dat we het beste boek wat internationaal is uitgekomen hebben aangekocht.
[4237.16 --> 4238.66]  En hebben vertaald naar het Nederlands.
[4238.74 --> 4240.34]  Omdat dat toch wel chill is.
[4240.80 --> 4242.24]  En dat boek heet Co-intelligence.
[4243.08 --> 4244.48]  Een boek van Ethan Mollick.
[4244.48 --> 4247.30]  Dat komt dus in het Nederlands uit als Co-intelligentie.
[4247.92 --> 4250.02]  En hij is een Wharton professor.
[4250.26 --> 4251.16]  Een business professor.
[4251.58 --> 4254.44]  Die al heel lang heel veel publiceert over AI.
[4254.56 --> 4255.48]  Ik voel hem al jaren.
[4256.06 --> 4258.50]  En hij maakt het heel praktisch en toegepast.
[4258.68 --> 4260.14]  Hij heeft enorme prompt libraries.
[4260.66 --> 4263.16]  Maar is daarnaast ook iemand die heel erg denkt in concepten.
[4263.16 --> 4268.38]  Waardoor ik veel meer ben gaan begrijpen van alles wat AI voor ons leven gaat betekenen.
[4268.46 --> 4272.18]  Omdat hij het eigenlijk gewoon door simpele...
[4272.18 --> 4273.44]  Hij maakt een soort van lenzen.
[4273.60 --> 4274.70]  Waardoor je naar de wereld kan kijken.
[4274.80 --> 4277.10]  En die lenzen, die overkoepelende lenzen, die concepten.
[4278.00 --> 4279.24]  Die deelt hij in dat boek.
[4279.36 --> 4283.48]  Dus het is een combinatie van een soort van manier om naar de wereld te kijken.
[4283.58 --> 4288.54]  En aan de andere kant hele praktische dingen over hoe je het dan daadwerkelijk kan toepassen.
[4289.12 --> 4291.50]  Dat boek komt 8 oktober uit.
[4291.98 --> 4293.70]  En dat kan je alvast voorbestellen.
[4293.86 --> 4297.54]  En de grap is dat je als je dat nu doet, het voorbestellen...
[4298.56 --> 4301.54]  dat je niet alleen het papierenboek, maar ook het e-boek krijgt.
[4302.34 --> 4305.06]  Plus drie maanden gratis AI report.
[4305.46 --> 4307.86]  Dus het betaalde deel van onze nieuwsbrief.
[4307.86 --> 4313.56]  Waar we iedere of twee keer per week allemaal praktische tips delen over hoe je AI in je werk kan toepassen.
[4314.44 --> 4316.04]  Krijg je er ook nog eens gratis bij.
[4316.14 --> 4318.44]  Maar dat is dus alleen maar als je het voor 8 oktober bestelt.
[4318.84 --> 4321.26]  En ben je al abonnee van AI report, dat kan natuurlijk ook.
[4321.32 --> 4325.52]  Dan krijg je dat e-boek gratis wanneer het uitkomt in je e-mail.
[4325.72 --> 4326.58]  Dat is wel zo eerlijk.
[4326.62 --> 4328.84]  Wat is het instapniveau als luisteraar?
[4329.40 --> 4332.34]  Kunnen mensen die deze podcast kunnen volgen dit boek lezen?
[4332.36 --> 4333.00]  Zeer zeker.
[4333.70 --> 4335.38]  En als je de podcast niet kan volgen?
[4335.92 --> 4342.72]  Het is een boek wat je eerst introduceert en dan vervolgens de diepte ingaat.
[4342.90 --> 4345.48]  Dus het is voor allebei denk ik heel erg geschikt.
[4345.62 --> 4347.60]  Zelfs voor mensen die al veel met AI bezig zijn.
[4348.54 --> 4351.44]  Het is fijn dat mensen alles even op een rijtje zetten.
[4351.54 --> 4357.00]  Maar hij gaat toch op een manier de diepte in die in ieder geval voor mij heel verrassend was.
[4358.40 --> 4360.42]  Nou ja, we hebben allemaal aanbevelingen.
[4360.54 --> 4363.58]  Pieter Zwart van Coolblue zegt verplicht huiswerk voor iedere ondernemer.
[4363.58 --> 4366.34]  Bas van den Veld van AFAS.
[4366.68 --> 4372.80]  Die zegt, Molleck maakt het praktisch en overzichtelijk hoe je een team van mensen en robots aanstuurt zonder dat ze in opstand komen.
[4372.90 --> 4379.16]  Nou, als je het boek wil voorbestellen dan kan je naar co-intelligentie.nl gaan of co-intelligentie.nl.
[4379.48 --> 4380.42]  Dan kun je het boek bestellen.
[4380.68 --> 4382.04]  Nou, tot zover de reclame jongens.
[4382.04 --> 4385.04]  Het was weer een boeiende aflevering.
[4385.94 --> 4391.24]  Ja, ik krijg nachtmerries over AI, Minecraft, beschavingen denk ik.
[4391.50 --> 4391.64]  Ja.
[4392.00 --> 4395.72]  Maar ik heb er ook zin in, want het is ook weer een mooi inkijkje in de toekomst.
[4395.72 --> 4398.32]  Kijk, Hitchhikers Guide to the Galaxy, een luchtige kijk op dit allemaal.
[4398.32 --> 4398.52]  Ja.
[4398.66 --> 4400.54]  Ik ga gewoon vaker naar Omroep Drenthe luisteren.
[4400.60 --> 4401.76]  Dat is mijn manier om ermee om te gaan.
[4402.36 --> 4403.44]  Zo doen we allemaal ons ding.
[4404.12 --> 4405.12]  Ja, dat was het.
[4405.46 --> 4408.30]  Leuk dat jullie hier met mij over hebben willen praten, jongens.
[4408.82 --> 4410.86]  Ik zie even niet meer wat ik moet zeggen aan Lein Noe daar.
[4411.64 --> 4412.00]  Oké.
[4412.22 --> 4412.62]  Draaiboek.
[4412.80 --> 4412.94]  Ja.
[4413.94 --> 4417.56]  We moeten steeds minder gaan knippen, want dit is dus precies wat het zo goed maakt.
[4417.66 --> 4418.40]  Zielig versam.
[4418.58 --> 4420.04]  Nee, dat maakt die podcast zo goed.
[4420.04 --> 4421.30]  Oh ja, je moet dit gewoon erin laten.
[4421.50 --> 4421.78]  Snap je?
[4422.10 --> 4423.78]  We moeten steeds meer van dit soort dingen erin laten.
[4423.78 --> 4424.42]  Ja, ik vind het ook.
[4424.80 --> 4425.66]  Own het gewoon, Mido.
[4426.10 --> 4427.30]  Wat moet ik ook weer zeggen?
[4427.54 --> 4428.26]  Wat moet ik ook weer zeggen?
[4428.32 --> 4429.66]  Wacht, ik kan het wel.
[4429.76 --> 4430.36]  Dat klipt niet wel goed.
[4430.44 --> 4431.04]  Wat moet ik ook weer zeggen?
[4431.10 --> 4431.66]  Oh ja, daar staat het.
[4431.78 --> 4431.84]  Ja.
[4432.34 --> 4434.22]  Dank Sam Hengeveld voor de edit.
[4434.86 --> 4435.80]  Het is altijd een hele klus.
[4435.90 --> 4436.34]  We weten het.
[4436.58 --> 4440.42]  En als je een lezing wil over AI van Wietse of van Alexander, dan kan dat.
[4440.54 --> 4442.24]  Mail ons op lezing.pokie.show.
[4442.76 --> 4445.46]  En vergeet je natuurlijk niet te abonneren op die nieuwsbrief.
[4445.62 --> 4447.00]  Je krijgt dus dat gratis e-book.
[4447.38 --> 4448.00]  Helemaal super.
[4448.18 --> 4450.44]  Kijk op AI-report.email.
[4451.00 --> 4451.36]  Jezus.
[4451.48 --> 4454.10]  Van een hoop reclame toch in deze podcast tegenwoordig, hè?
[4454.26 --> 4454.92]  You're welcome.
[4455.04 --> 4456.92]  Maar dit is deze tijd.
[4457.30 --> 4458.28]  Moet geld verdiend worden.
[4458.32 --> 4461.04]  Ja, maar ook een soort van hoezeer dit onderwerp leeft.
[4461.16 --> 4461.72]  Dat blijkt maar weer.
[4461.74 --> 4462.86]  Het is toch ook het belangrijkste onderwerp?
[4462.86 --> 4463.34]  Nee, zeker.
[4463.66 --> 4465.32]  Ik own het ook gewoon.
[4465.94 --> 4467.56]  Ik was gewoon veel reclame aan het voorlezen.
[4468.36 --> 4468.76]  Squarespace.
[4469.20 --> 4469.60]  Kalko.
[4470.28 --> 4470.60]  Kalko.
[4470.76 --> 4472.02]  Dat komt allemaal voorbij.
[4472.16 --> 4472.32]  Goed.
[4472.62 --> 4473.16]  Zullen we het afronden?
[4473.44 --> 4474.44]  Tot volgende week.
[4474.48 --> 4474.60]  Dag.
[4474.80 --> 4475.04]  Dag.
[4475.04 --> 4505.02]  TV Gelderland 2021
